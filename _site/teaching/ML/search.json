[
  {
    "objectID": "06_boosting.html",
    "href": "06_boosting.html",
    "title": "6  Boosting",
    "section": "",
    "text": "6.1 Introduction\nIn this exercise session, we will consider multiple advanced machine learning models. Our base model will not be a penalized logistic regression as in Session 05 rather than a random forest. The models we are considering subsequently, are also widely used in application as their performance on classification tasks is superb! However, similar to random forests, their explainability is still subpar compared to a simple logistic regression or classification tree. Before we learn how to train and finetune these models, we will discuss some theoretical aspects.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Boosting</span>"
    ]
  },
  {
    "objectID": "06_boosting.html#introduction",
    "href": "06_boosting.html#introduction",
    "title": "6  Boosting",
    "section": "",
    "text": "6.1.1 Confusion matrices in R\nSince we will be working with on a classification task in the exercises, being able to construct a confusion matrix is crucial.\nConsider the following example data set which is part of the {yardstick} library:\n\nlibrary(tidyverse)\nlibrary(yardstick)\n\ntwo_class_example %&gt;% glimpse()\n\nRows: 500\nColumns: 4\n$ truth     &lt;fct&gt; Class2, Class1, Class2, Class1, Class2, Class1, Class1, Clas…\n$ Class1    &lt;dbl&gt; 0.0035892426, 0.6786210540, 0.1108935221, 0.7351617031, 0.01…\n$ Class2    &lt;dbl&gt; 9.964108e-01, 3.213789e-01, 8.891065e-01, 2.648383e-01, 9.83…\n$ predicted &lt;fct&gt; Class2, Class1, Class2, Class1, Class2, Class1, Class1, Clas…\n\n\nSay, we want to label change the label Class1 to Positive and Class2 to Negative. Then, we can simply apply the mutate() function:\n\ntwo_class_example &lt;- two_class_example %&gt;%\n  mutate(\n    truth = fct_relabel(truth,\n                        ~if_else(.==\"Class1\",\"Positive\",\"Negative\")\n                        ),\n    predicted = fct_relabel(predicted,\n                          ~if_else(.==\"Class1\",\"Positive\",\"Negative\")\n                          )\n  )\n\nTo create a simple confusion matrix, we can use the conf_mat function that is also part of the {yardstick} library:\n\n(cm_example &lt;- two_class_example %&gt;% conf_mat(truth,predicted))\n\n          Truth\nPrediction Positive Negative\n  Positive      227       50\n  Negative       31      192\n\n\nWe can also use the ggplot function to create a visually more appealing version of this matrix. To do so, we first have to convert the confusion matrix into a proper data frame and set the levels of the Predictions and Truth.\n\ncm_tib&lt;- as_tibble(cm_example$table)%&gt;% \n  mutate(\n    Prediction = factor(Prediction,\n                        levels = rev(levels(factor(Prediction)))),\n    Truth = factor(Truth)\n)\n\nIn the snippet above we had to reverse the levels of the variable Prediction so that we can place the TP values in the top left, and TN values in the bottom right of the confusion matrix.\nOnce the confusion matrix has been converted to a data frame, we can pass it into the ggplot function with the argument fill set to n. The geom_tile() function places a tile at each coordinate provided by the data frame (Note: Coordinates are discrete and given by Negative and Positive). The argument colour = \"gray50\" adds a gray border to each tile. By adding the geom_text() function where the aesthetics are provided by the label argument, we can add the number of samples falling into each class (TP, TN, FP, and FN) to each tile. The scale_fill_gradient() function, allows to change the colors of the tiles with respect to the value of n. Here, a low value of n indicates the the tile will be \"white\" and a high value of n indicates that the color of the tile will be light green (with HEX Code \"#9AEBA3\"). Setting the theme to minimal, and removing the legend yields a cleaner representation of the confusion matrix.\n\ncm_tib %&gt;% ggplot(aes(x = Prediction, y = Truth,fill = n)) +\n    geom_tile( colour = \"gray50\")+\n    geom_text(aes(label = n))+\n    scale_fill_gradient(low = \"white\", high = \"#9AEBA3\")+\n    theme_minimal()+\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n6.1.2 Tuning an XGBoost model\nSince we have intensively covered random forests in previous exercises, we only consider an XGBoost model in this introduction. AdaBoost is rarely used in practice anymore, which is why we will directly move towards training an XGBoost model. The approach is similar to training and tuning every other model but compared to previous exercises we will not perform cross validation, rather than a simple training/validation/test split to save some time.\nWe can create an XGBoost model by using the boost_tree function. Looking at the documentation, you will notice that there are quite a few parameters for us to consider:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\ntrees\nNumber of trees contained in the ensamble\n\n\ntree_depth\nInteger for the maximum depth of the trees\n\n\nmin_n\nMinimum number of data points in a node required for a split\n\n\nmtry\nNumber of randomly selected at each split\n\n\n\nThe parameters above are not new to us. In fact, they are the exact same parameters we use for training a random forest model. That is why we will not go into detail with respect to the ones above.\nThere are, however, a few new parameters that are worth an explanation:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nloss_reduction\nNumber for the reduction in loss that is required to split further \\(\\in [0,\\infty]\\)\n\n\nsample_size\nSubsample ratio of the training instances \\(\\in (0,1].\\)\n\n\nlearn_rate\nRate at which the algorithm adapts from iteration to iteration \\(\\in [0,1]\\).\n\n\n\nThe three parameters above have only been referenced in the lectures so far, so let’s quickly describe them in a bit more detail.\n\n6.1.2.0.1 loss_reduction :\nIn Exercise 6.2, we will derive the optimal expansion coefficient \\(\\alpha\\) (similar to the coefficients in linear regression) which solves the minimization problem\n\\[\n(\\alpha_b,h_b) = \\arg \\min_{\\alpha &gt;0, h\\in\\mathcal{H}}\\sum_{n=1}^N L(y_n,\\hat{f}^{(b-1)}(x_i)+\\alpha h(x_i))\n\\]\nHere, \\(L\\) denotes a loss function that we aim to minimize with respect to \\(\\alpha\\) and an additional (potentially weak) learner \\(h\\) that we add to the previous estimator.\nIf the term\n\\[\n\\left| \\sum_{n=1}^N L(y_n,\\hat{f}^{(b-1)}(x_i)+\\alpha h(x_i)) - \\sum_{n=1}^N L(y_n,\\hat{f}^{(b)}(x_i)+\\alpha h(x_i)) \\right|,\n\\]\ni.e., the loss reduction between step \\(b\\) and \\(b+1\\) is smaller than the parameter loss_reduction, the algorithm stops.\n\n\n6.1.2.0.2 sample_size :\nLet \\(q\\in(0,1]\\) denote the sample_size parameter and \\(N\\) the number of samples in our training data. Then, XGBoost selects \\(q\\cdot N\\) samples prior to growing trees. This subsampling occurs once in every boosting iteration.\n\n\n6.1.2.0.3 learn_rate :\nIn simple terms, the learning rate specifies how quickly the model adapts to the training data. An analogy can be drawn to gradient based models that use gradient descent on a loss function. Here, the goal is to minimize the loss function by stepping towards its minimum. To illustrate the learning rate in a gradient descent context, consider the following examples where we can imagine the polynomial of degree four to be a loss function that we try to minimize.\n\nChoosing a learning rate that is too high, might result in missing an optimal model because it is being stepped over, while a learning rate chosen too small might result in the objective never being reached at all.\nSimilar to choosing a learning rate that is too high, we could also choose a learning rate that is too low, resulting in the global minimum never being reached at all.\n\nThe learning rate in the XGBoost algorithm describes a factor \\(\\gamma\\) that scales the output of the most recently fit tree that is added to the model. In simple terms, the learning rate in the XGBoost algorithm describes a shrinkage parameter.\nIn the following example, we will try to predict the base rent prices in Munich using an XGBoost model. The data Apartment rental offers in Germany is the same as in Exercise 04.\n\nlibrary(tidymodels)\n\ndata_muc &lt;- read.csv(\"data/rent_muc.csv\")\n\nInstead of using a cross validation approach, we will use a simple training/validation/test split to reduce computing time.\nBy using the validation split function on the training data, we split the training data into a training and validation subset. The data_val object can then be passed into the tune_grid function in the same fashion as we did with a cross validation object.\n\nset.seed(24)\nsplit_rent &lt;- initial_split(data_muc)\ndata_train &lt;- training(split_rent)\ndata_val &lt;- validation_split(data_train)\ndata_test &lt;- testing(split_rent)\n\nPreprocessing of the data is handled by the following recipe.\n\nrec_rent &lt;- recipe(\n    formula = baseRent ~., \n    data = data_train\n  ) %&gt;%\n  update_role(scoutId, new_role = \"ID\") %&gt;%\n  step_select(!c(\"serviceCharge\",\"heatingType\",\"picturecount\",\n                 \"totalRent\",   \"firingTypes\",\"typeOfFlat\",\n                 \"noRoomsRange\", \"petsAllowed\",\n                 \"livingSpaceRange\",\"regio3\",\"heatingCosts\",\n                 \"floor\",\"date\", \"pricetrend\")) %&gt;%\n  step_mutate(\n    interiorQual = factor(\n      interiorQual,\n      levels = c(\"simple\", \"normal\", \"sophisticated\", \"luxury\"),\n      ordered = TRUE\n      ),\n    condition = factor(condition,\n      levels = c(\"need_of_renovation\", \"negotiable\",\"well_kept\",\n                 \"refurbished\",\"first_time_use_after_refurbishment\",\n                 \"modernized\",\"fully_renovated\", \"mint_condition\",\n                 \"first_time_use\"),\n      ordered = TRUE),\n    geo_plz = factor(geo_plz),\n              across(where(is.logical),~as.numeric(.))) %&gt;%\n  step_string2factor(all_nominal_predictors(),\n                     all_logical_predictors()) %&gt;%\n  step_ordinalscore(all_ordered_predictors())%&gt;%\n  step_novel(all_factor_predictors())%&gt;%\n  step_unknown(all_factor_predictors()) %&gt;%\n  step_dummy(geo_plz)%&gt;%\n  step_impute_knn(all_predictors()) %&gt;%\n  step_filter(baseRent &lt;= 4000, livingSpace &lt;= 200)\n\nAfter preprocessing the data, we can create a workflow object and specify our XGBoost model.\n\nwf_rent &lt;- workflow() %&gt;%\n  add_recipe(rec_rent)\n\nWe want to tune every parameter except for trees. Since we are using a regression model, we need to use the mode \"regression\".\n\nxgb_model &lt;- boost_tree(\n  trees = 1000,\n  tree_depth = tune(),\n  min_n = tune(),\n  mtry = tune(),         \n  loss_reduction = tune(),                     \n  learn_rate = tune()                          \n) %&gt;%\n  set_mode(\"regression\")\n\nwf_rent &lt;- wf_rent %&gt;% add_model(xgb_model)\n\nTo tune the model and select the best model based on the performance on the validation data, we use the tune_grid function.\n\nmulti_metrics &lt;- metric_set(rmse,rsq,mae)\n\nxgb_tune_res &lt;- wf_rent %&gt;%\n  tune_grid(\n    resamples = data_val,\n    metrics = multi_metrics,\n    grid = 20,\n  )\n\nAfter tuning the model parameters, we use the optimal candidate hyperparameters to train a final model on all the training data and evaluate it on the test data.\n\nxgb_best_parm &lt;- xgb_tune_res %&gt;% select_best(metric = \"rmse\")\n\nlast_xgb_fit &lt;- wf_rent %&gt;%\n  finalize_workflow(xgb_best_parm) %&gt;%\n  last_fit(split_rent)\n\nlast_xgb_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     539.    Preprocessor1_Model1\n2 rsq     standard       0.724 Preprocessor1_Model1\n\n\nIn Exercise 04, where we performed the same regression task with a decision tree, the OOS performance was substantially worse:\n\n\n\nMetric\nEstimate\n\n\n\n\nRMSE\n622\n\n\n\\(R^2\\)\n0.616\n\n\n\nWe can, therefore, conclude that the XGBoost model is more suitable for estimating the base rent for rental apartments in Munich.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Boosting</span>"
    ]
  },
  {
    "objectID": "06_boosting.html#exercises",
    "href": "06_boosting.html#exercises",
    "title": "6  Boosting",
    "section": "6.2 Exercises",
    "text": "6.2 Exercises\n\n6.2.1 Theoretical exercises\n\nExercise 6.1 Explain in your own words, the difference between Boosting (Trees), Bagging (Trees), and Random Forests.\n\n\nExercise 6.2 On slide 89 in the lecture notes, the AdaBoost algorithm stated as follows.\n\n\n\n\n\n\nFigure 6.1: AdaBoost Algorithm\n\n\n\nIn the third line of Figure 6.1 , the scaling coefficients \\(\\alpha_b\\) at step \\(b\\in\\{1,..,B\\}\\) are set to\n\\[\\begin{equation}\n  \\alpha_b = \\frac{1}{2}\\log\\left(\\frac{1-\\mathrm{err}_b}{\\mathrm{err}_b}\\right).\n\\end{equation}\\]\nThe goal of this exercise is to figure out, why the scaling coefficients are defined that way. The essence of this derivation lies in the more general idea of boosting, where the minimization problem at step \\(b\\in\\{1,...,B\\}\\) is given by (cf. Slide 91)\n\\[\n  (\\alpha_b,h_b) = \\underset{\\alpha&gt;0,h\\in\\mathcal{H}}{\\mathrm{argmin}} \\sum_{i=1}^{n}L(y_i,\\hat{f}^{(b-1)}(x_i)+\\alpha h(x_i))\n\\tag{6.1}\\]\nFor AdaBoost, the loss function \\(L\\) is defined by\n\\[\n  L(y,\\hat{f}(x)) = \\exp(-y\\hat{f}(x))\n\\tag{6.2}\\]\nBy minimizing Equation 6.1 with respect to \\(\\alpha\\), we obtain the desired coefficient.\n\nShow that \\[\n  \\underset{\\alpha&gt;0,h\\in\\mathcal{H}}{\\mathrm{argmin}} \\sum_{i=1}^{n}L(y_i,\\hat{f}^{(b-1)}(x_i)+\\alpha h(x_i)) = \\underset{\\alpha&gt;0,h\\in\\mathcal{H}}{\\mathrm{argmin}} \\sum_{i=1}^{n} w_b(i) \\exp(-\\alpha y_i h(x_i))\n\\tag{6.3}\\]\nShow that the objective function of the right hand side of Equation 6.3 can be expressed as\n\\[\n  e^{-\\alpha}\\sum_{y_i = h(x_i)} w_b(i) + e^{\\alpha}\\sum_{y_i \\neq h(x_i)} w_b(i)\n\\tag{6.4}\\]\nShow that Equation 6.4 is equal to \\[\n(e^{\\alpha}-e^{-\\alpha})\\sum_{i = 1}^n w_b(i)I(y_i\\neq h(x_i)) + e^{-\\alpha} \\sum_{i=1}^n w_b(i)\n\\tag{6.5}\\]\nArgue by using Equation 6.5 that for any \\(\\alpha &gt;0\\) the solution to Equation 6.1 for \\(h\\) is given by\n\\[\n  h_b = \\underset{h}{\\mathrm{argmin}}\\sum_{i=1}^{n} w_b(i) I(y_i\\neq h(x_i)).\n\\tag{6.6}\\]\nFinally, plug the objective function Equation 6.5 into Equation 6.3 and show that minimizing the loss function for \\(\\alpha\\) yields\n\\[\\begin{equation}\n  \\alpha_b =      \\frac{1}{2}\\log\\left(\\frac{1-\\mathrm{err}_b}{\\mathrm{err}_b}\\right),\n\\end{equation}\\]\nwhere \\[\\begin{equation}\n  \\mathrm{err}_b =  \\frac{\\sum_{i=1}^nw_b(i)I(y_i\\neq h_b(x_i))}{\\sum_{i=1}^nw_b(i)}.\n\\end{equation}\\]\nHint: You can assume that the candidate for \\(\\alpha\\) is indeed a minimizer.\n\n\n\n\n6.2.2 Programming Exercises\nThe following exercise is similar to Exercise 5.3.2. However, instead of fitting penalized logistic regression and classification tree, we fit a XGBoost and LightGBM model on the credit card data.\n\nlibrary(\"vip\")\nlibrary(\"finetune\")\nlibrary(\"bonsai\")\nlibrary(\"patchwork\")\nlibrary(\"ggtext\")\n\nThe dataset we will consider in this exercise will be the Credit Card Customers data set that we already used in previous exercises. You can either download it again using the provided link or the button below.\n\nDownload BankChurners\n\nRecall that the data set consists of 10,127 entries that represent individual customers of a bank including but not limited to their age, salary, credit card limit, and credit card category.\nThe goal is to find out whether a customer will stay or leave the bank given the above features.\nThe following training, validation and test split should be used for training the models in of the subsequent exercises.\n\ncredit_info &lt;- read.csv(\"data/BankChurners.csv\")\n\nset.seed(121)\nsplit &lt;- initial_split(credit_info, strata = Attrition_Flag)\ndata_train_ci &lt;- training(split)\ndata_val_ci &lt;- validation_split(data_train_ci)\ndata_test_ci &lt;- testing(split)\n\nPreprocessing of the data is handled by the following recipe.\n\nlevels_income &lt;- c(\"Less than $40K\",\"$40K - $60K\",\n                   \"$60K - $80K\",\"$80K - $120K\",\"$120K +\")\n\nlevels_education &lt;- c(\"Uneducated\", \"High School\",\"College\",\n                      \"Graduate\",  \"Post-Graduate\", \"Doctorate\")\n\nrec_ci &lt;- recipe(Attrition_Flag ~., data = data_train_ci) %&gt;%\n  update_role(CLIENTNUM, new_role = \"ID\") %&gt;%\n  step_mutate_at(all_nominal_predictors(),\n               fn = ~if_else(.%in% c(\"Unknown\",\"unknown\"),NA,.)\n  ) %&gt;%\n  step_mutate(Attrition_Flag = factor(\n                 Attrition_Flag,\n                 labels = c(\"Positive\", \"Negative\")\n               )\n               ) %&gt;%\n  step_string2factor(Income_Category,\n                     levels = levels_income,\n                     ordered = TRUE) %&gt;%\n  step_string2factor(Education_Level,\n                     levels = levels_education,\n                     ordered = TRUE) %&gt;%\n  step_ordinalscore(all_ordered_predictors()) %&gt;%\n  step_unknown(all_factor_predictors()) %&gt;%\n  step_impute_knn(all_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_corr(all_predictors())\n\nci_wf &lt;- workflow() %&gt;%\n  add_recipe(rec_ci) \n\nmulti_metrics &lt;- metric_set(roc_auc,pr_auc,accuracy,recall)\n\nNote, that we encoded the target variable Attrition_Flag with new labels, namely Positive and Negative. Positive corresponds to a customer leaving the bank, while Negative corresponds to a customer staying with the bank.\n\nExercise 6.3 Create and train a random forest model using the provided recipe with \\(1000\\) trees and tune the parameters mtry and min_n.\nTune the model on a grid of size 20 using the tune_grid function on the validation split generated with the training data.\nFind the best model by evaluating the tuning results with respect to the models’ accuracy.\nBased on these parameters train a model on the whole training data.\n\n\nExercise 6.4 Create two tibbles containing the data necessary to plot a ROC- and PR curve. When creating the tibbles, add a column containing the model name \"Random forest\", so that we can correctly identify the models later during model evaluation.\n\n\nExercise 6.5 Tune a XGBoost model in the same fashion as the random forest. Set the number of trees to 1000, and every other parameter, except for sample_size, to tune().\nAfter tuning and refitting the best model on the whole training data, repeat Exercise 6.4 for this XGBoost model on the test data.\n\n\n\n\n\n\n\nNote\n\n\n\nThe following model is not relevant for the exam. However, it is extremely relevant in today’s ML landscape, so I encourage you to solve the following exercises as well.\n\n\n\nExercise 6.6 (Bonus Exercise) The last model we want to train is called LightGBM. It was developed by Microsoft and is, as well as XGBoost, a gradient-based ensemble learner. An advantage compared to XGBoost is the focus on performance and scalability, meaning that it is designed to work well on CPUs while trying to at least match the performance of XGBoost.\nThe steps for training a LightGBM model are exactly the same as for training an XGBoost model, except for the model specification. Here we set the engine to \"lightgbm\" instead of \"xgboost\". Every other parameter stays the same, thanks to the {tidymodels} framework.\nRepeat Exercise 6.5 for a LightGBM model.\n\n\n\n\n\n\n\nTip\n\n\n\nIf you get stuck recreating the following plots, revisit the solutions to Exercise Sheet 05, where we created the same plot for a penalized logistic regression, a classification tree, and a random forest.\n\n\n\nExercise 6.7 Create a plot showing the ROC- and PR-curve for each of the models we trained in the previous exercises (Random Forest, XGBoost, LightGBM). Compare the performances visually and decide which model performed the best. For reference, you can find what such a plot could look like below.\n\n\nExercise 6.8 For each of the previously trained models (Random Forest, XGBoost, LightGBM), create a confusion matrix based on the test sets to evaluate which model performed best on unseen data.\n\n\nExercise 6.9 For the confusion matrices above, find out which model has the overall best out-of-sample performance. For this best model, calculate the following metrics:\n\nSensitivity\nPrecision\nAccuracy",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Boosting</span>"
    ]
  },
  {
    "objectID": "06_boosting.html#solutions",
    "href": "06_boosting.html#solutions",
    "title": "6  Boosting",
    "section": "6.3 Solutions",
    "text": "6.3 Solutions\n\nSolution 6.1 (Exercise 6.1). \n\nBagging (bootstrap aggregation) is a special case of random forests. Here, we also create a predetermined number of trees. However, the main difference is that in Bagging the full set of features is considered when creating a split for a node. In a random forest, only a subset of all features is randomly considered when creating a split for a new node.\nBoosting (Trees) combines many weak learners, e.g., tree stumps, to make a prediction. Compared to Bagging and Random forests, those weak learners are weighted, e.g., one tree stump has more say than another when making a final decision. Furthermore, weak learners are not created independently because each weak learner is built by considering the previous learners’ mistakes.\n\n\n\nSolution 6.2 (Exercise 6.2). \n\nPlugging Equation 6.2 into Equation 6.1 yields\n\\[\\begin{align*}\n  \\underset{\\alpha&gt;0,h\\in\\mathcal{H}}{\\mathrm{argmin}}\\sum_{i=1}^{n}L(y_i,\\hat{f}^{(b-1)}(x_i)+\\alpha h(x_i)) &=\\underset{\\alpha&gt;0,h\\in\\mathcal{H}}{\\mathrm{argmin}}\\sum_{i=1}^{n}\\exp(-y_i(\\hat{f}^{(b-1)}(x_i)+\\alpha h(x_i)))\\\\\n  &= \\underset{\\alpha&gt;0,h\\in\\mathcal{H}}{\\mathrm{argmin}}\\sum_{i=1}^{n} \\underbrace{\\exp(-y_i\\hat{f}^{(b-1)}(x_i))}_{\\coloneqq w_b(i)}\\exp(-\\alpha y_i h(x_i)) \\\\\n  &= \\underset{\\alpha&gt;0,h\\in\\mathcal{H}}{\\mathrm{argmin}}\\sum_{i=1}^{n} w_b(i)\\exp(-\\alpha y_i h(x_i)).\n\\end{align*}\\]\nSince \\(y_i\\in \\{1,-1\\}\\) and \\(h(x_i)\\in \\{-1,1\\}\\) as well, either \\(y_i\\cdot h(x_i) = 1\\) if \\(y_i = h(x_i)\\), or \\(y_i\\cdot h(x_i) = -1\\) if \\(y_i \\neq h(x_i)\\) (since one of the two terms is equal to \\(-1\\) and the other equal to \\(1\\)). The condition above can be formalized as\n\\[\\begin{equation}\n    y_i\\cdot h(x_i) = \\begin{cases}1 &\\text{ if } y_i=h(x_i)\\\\\n                                   -1 &\\text{ if } y_i\\neq h(x_i)\n                      \\end{cases},\n\\end{equation}\\] and rewriting the right hand side of Equation 6.3 using these conditions yields \\[\\begin{align*}\n  \\sum_{i=1}^n w_b(i)\\exp(-\\alpha y_i h(x_i)) &= \\sum_{i=1}^n (I(y_i = h(x_i))+I(y_i \\neq h(x_i)) w_b(i)\\exp(-\\alpha y_i h(x_i))\\\\\n  &=\\sum_{i=1}^n I(y_i = h(x_i)) w_b(i)\\exp(-\\alpha y_i h(x_i)) \\\\\n  &\\qquad +\\sum_{i=1}^n I(y_i \\neq h(x_i))w_b(i)\\exp(-\\alpha y_i h(x_i))\\\\\n  &= \\sum_{y_i = h(x_i)} w_b(i)\\exp(-\\alpha\\cdot 1) + \\sum_{y_i \\neq h(x_i)} w_b(i)\\exp(-\\alpha\\cdot -1)\\\\\n  &= e^{-\\alpha} \\sum_{y_i = h(x_i)} w_b(i)+ e^{\\alpha}\\sum_{y_i \\neq h(x_i)} w_b(i)\\\\\n\\end{align*}\\]\nBy expanding and rearranging Equation 6.5, we obtain \\[\\begin{align}\n    &e^\\alpha \\sum_{i=1}^n w_b(i)I(y_i\\neq h(x_i)) +\n      e^{-\\alpha}\\sum_{i = 1}^n w_b(x_i)-w_b(i)I(y_i\\neq h(x_i))\\\\\n    &\\quad =e^\\alpha \\sum_{y_i\\neq h(x_i)}^n w_b(i) +\n      e^{-\\alpha}\\sum_{I(y_i= h(x_i))}^n w_b(x_i).\n\\end{align}\\]\nUsing the results of sub tasks 1)-3), we can rewrite the minimization problem of Equation 6.1 as follows:\n\\[\n\\underset{\\alpha&gt;0,h\\in\\mathcal{H}}{\\mathrm{argmin}}\\left\\{ (e^{\\alpha}-e^{-\\alpha})\\sum_{i = 1}^n w_b(i)I(y_i\\neq h(x_i)) + e^{-\\alpha} \\sum_{i=1}^n w_b(i)\\right\\}\n\\tag{6.7}\\]\nEquation 6.7 only contains one term that depends \\(h\\), i.e.\n\\[\\begin{equation}\n    (e^{\\alpha}-e^{-\\alpha})\\sum_{i = 1}^n w_b(i)I(y_i\\neq h(x_i)).\n\\end{equation}\\]\nTherefore, any function \\(h\\) that minimizes Equation 6.6 also minimizes Equation 6.7.\nTo minimize Equation 6.5 with respect to \\(\\alpha\\), we have to set the derivative of Equation 6.5 with respect to \\(\\alpha\\) to 0. Define \\(t\\coloneqq \\sum_{i = 1}^n w_b(i)I(y_i\\neq h(x_i))\\) and \\(s\\coloneqq \\sum_{i = 1}^n w_b(i)\\). Then, \\[\\begin{equation*}\n\\frac{\\partial}{\\partial \\alpha} (e^\\alpha - e^{-\\alpha})t+e^{-\\alpha}s) = te^\\alpha-(s-t)e^{-\\alpha}.\n\\end{equation*}\\] Now, \\[\\begin{align*}\n\\frac{\\partial}{\\partial \\alpha} ((e^\\alpha - e^{-\\alpha})t+e^{-\\alpha}s) = te^\\alpha-(s-t)e^{-\\alpha} &\\overset{!}{=} 0 \\\\\n&\\iff te^\\alpha = (s-t)e^{-\\alpha} \\\\\n&\\iff e^{2\\alpha} = \\frac{(s-t)}{t} \\\\\n&\\iff 2\\alpha = \\log\\left(\\frac{(s-t)}{t}\\right)\\\\\n&\\iff \\alpha = \\frac{1}{2}\\log\\left(\\frac{(s-t)}{t}\\right).\n\\end{align*}\\] Defining \\[\\begin{equation}\n\\mathrm{err}_b \\coloneqq \\frac{\\sum_{i = 1}^n w_b(i)I(y_i\\neq h(x_i))}{\\sum_{i = 1}^n w_b(i)}\n\\end{equation}\\] yields \\[\\begin{equation}\n\\frac{1-\\mathrm{err}_b}{\\mathrm{err}_b} = \\frac{1}{\\mathrm{err}_b}-1 = \\frac{\\sum_{i = 1}^n w_b(i)}{\\sum_{i = 1}^n w_b(i)I(y_i\\neq h(x_i))}-1 = \\frac{s}{t}-1 = \\frac{s-t}{t}.\n\\end{equation}\\] Finally, re-substituting \\(s\\) and \\(t\\) in \\(\\frac{1}{2}\\log\\left(\\frac{(s-t)}{t}\\right)\\) yields \\[\\begin{equation}\n  \\alpha = \\frac{1}{2}\\log\\left(\\frac{1-\\mathrm{err}_b}{\\mathrm{err}_b}\\right).\n\\end{equation}\\]\n\n\n\nSolution 6.3 (Exercise 6.3). \n\ncores &lt;- parallel::detectCores()\n\nrf_model &lt;- rand_forest(\n  mode = \"classification\",\n  mtry = tune(),\n  min_n = tune(),\n  trees = 1000\n) %&gt;%\n  set_engine(\"ranger\",\n             num.threads = cores\n  )\n\nci_wf &lt;- ci_wf %&gt;% add_model(rf_model)\n\nrf_tune_res &lt;- ci_wf %&gt;% \n    tune_grid(grid = 20,\n              resamples = data_val_ci,\n              metrics = multi_metrics\n    )\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\nrf_best_parm &lt;- rf_tune_res %&gt;%\n  select_best(metric = \"accuracy\")\n\nlast_rf_fit &lt;- ci_wf %&gt;%\n  finalize_workflow(rf_best_parm) %&gt;%\n  last_fit(split)\n\n\n\nSolution 6.4 (Exercise 6.4). \n\nrf_roc &lt;- last_rf_fit %&gt;% \n  collect_predictions() %&gt;% \n  roc_curve(Attrition_Flag, .pred_Positive) %&gt;% \n  mutate(model = \"Random Forest\")\n\nrf_pr &lt;- last_rf_fit %&gt;% \n  collect_predictions() %&gt;% \n  pr_curve(Attrition_Flag, .pred_Positive) %&gt;% \n  mutate(model = \"Random Forest\")\n\n\n\nSolution 6.5 (Exercise 6.5). \n\nxgb_model &lt;- boost_tree(\n  trees = 1000,\n  tree_depth = tune(),\n  min_n = tune(),\n  mtry = tune(),         \n  loss_reduction = tune(),                     \n  learn_rate = tune()                          \n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\nci_wf &lt;- ci_wf %&gt;% update_model(xgb_model)\n\ndoParallel::registerDoParallel()\n\nxgb_tune_res &lt;- tune_grid(\n  ci_wf,\n  resamples = data_val_ci,\n  grid = 20,\n  metrics = multi_metrics\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\nxgb_best_parm &lt;- xgb_tune_res %&gt;% select_best(metric = \"accuracy\")\n\nlast_xgb_fit &lt;- ci_wf %&gt;%\n  finalize_workflow(xgb_best_parm) %&gt;%\n  last_fit(split)\n\nxgb_roc &lt;- \n  last_xgb_fit %&gt;% \n  collect_predictions() %&gt;% \n  roc_curve(Attrition_Flag, .pred_Positive) %&gt;% \n  mutate(model = \"XGBoost\")\n\nxgb_pr &lt;- \n  last_xgb_fit %&gt;% \n  collect_predictions() %&gt;% \n  pr_curve(Attrition_Flag, .pred_Positive) %&gt;% \n  mutate(model = \"XGBoost\")\n\n\n\nSolution 6.6 (Exercise 6.6). \n\nlightgbm_model &lt;- boost_tree(\n  trees = 1000,\n  tree_depth = tune(),\n  min_n = tune(),\n  loss_reduction = tune(),                     \n  mtry = tune(),         \n  learn_rate = tune()                          \n) %&gt;%\n  set_engine(\"lightgbm\") %&gt;%\n  set_mode(\"classification\")\n\nci_wf &lt;- ci_wf %&gt;% update_model(lightgbm_model)\n\nlightgbm_res &lt;- tune_grid(\n  ci_wf,\n  resamples = data_val_ci,\n  grid = 20,\n  metrics = multi_metrics\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\nlightgbm_res_best &lt;- lightgbm_res %&gt;% select_best(metric = \"accuracy\")\n\nlast_lightgbm_fit &lt;- ci_wf %&gt;%\n  finalize_workflow(lightgbm_res_best) %&gt;%\n  last_fit(split)\n\nlightgbm_roc &lt;- last_lightgbm_fit %&gt;% \n  collect_predictions() %&gt;% \n  roc_curve(Attrition_Flag, .pred_Positive) %&gt;% \n  mutate(model = \"lightGBM\")\n\nlightgbm_pr &lt;- last_lightgbm_fit %&gt;% \n  collect_predictions() %&gt;% \n  pr_curve(Attrition_Flag, .pred_Positive) %&gt;% \n  mutate(model = \"lightGBM\")\n\n\n\nSolution 6.7 (Exercise 6.7). \n\ncols &lt;- c(\"#80003A\",\"#506432\",\"#FFC500\")\nnames(cols) &lt;- c(\"lgbm\", \"rf\", \"xgb\")\nplot_title &lt;- glue::glue(\"ROC- and PR-Curve for a &lt;span style='color:{cols['rf']};'&gt;Random Forest&lt;/span&gt;,&lt;br&gt;\n                         &lt;span style='color:{cols['xgb']};'&gt;XGBoost model&lt;/span&gt;,\n                         and &lt;span style='color:{cols['lgbm']};'&gt;LightGBM model&lt;/span&gt;\")\np1 &lt;- bind_rows(rf_roc, xgb_roc, lightgbm_roc) %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + \n  geom_path(lwd = 1.5, alpha = 0.8) +\n  geom_abline(lty = 3) + \n  coord_equal() + \n  scale_color_manual(values = unname(cols))+\n  theme(legend.position = \"none\")\n\np2 &lt;- bind_rows(rf_pr, xgb_pr, lightgbm_pr) %&gt;% \n  ggplot(aes(x = recall, y = precision, col = model)) + \n  geom_path(lwd = 1.5, alpha = 0.8) +\n  geom_abline(lty = 3, slope = -1, intercept = 1) + \n  coord_equal() + \n  scale_color_manual(values = unname(cols))+\n  theme(legend.position = \"none\")\n\n(p1|p2) +\n  plot_annotation(\n  title = plot_title,\n  theme = theme(plot.title = element_markdown()))\n\n\n\n\n\n\n\n\n\n\nSolution 6.8 (Exercise 6.8). \n\ncols &lt;- c(\"#80003A\",\"#506432\",\"#FFC500\")\nnames(cols) &lt;- c(\"lgbm\", \"rf\", \"xgb\")\n\ntitle_tib &lt;- tibble(\n  x=0,\n  y=1,\n  label = glue::glue(\"&lt;p&gt;&lt;b&gt;Confusion matrices for a &lt;span style='color:{cols['rf']};'&gt;random forest&lt;/span&gt;, &lt;br/&gt; \n                         &lt;span style='color:{cols['xgb']};'&gt;XGBoost model&lt;/span&gt;,\n                         and &lt;span style='color:{cols['lgbm']};'&gt;LightGBM model&lt;/span&gt;.&lt;/b&gt;&lt;/p&gt;\n                     &lt;p&gt; Looking at the number of &lt;b&gt;True Positives &lt;/b&gt;(top left panel) &lt;br/&gt;\n                      and &lt;b&gt;True Negatives&lt;/b&gt; (bottom right panel), it becomes &lt;br /&gt;\n                      clear that the &lt;span style='color:{cols['lgbm']};'&gt;LightGBM model&lt;/span&gt; performs best.&lt;br /&gt;\n                      Additionally, the &lt;b&gt;True Positive rate&lt;/b&gt; (ratio of customers &lt;br /&gt;\n                      that have been correctly identified to truely leave the bank)&lt;br /&gt;\n                      is the highest, and the number of &lt;b&gt;False Positives&lt;/b&gt; &lt;br /&gt;\n                      (top right panel) is the lowest for the &lt;span style='color:{cols['lgbm']};'&gt;LightGBM model&lt;/span&gt;.&lt;/p&gt;\")\n)\n\ncm_plot &lt;- function(last_fit_model,high){ \n  cm &lt;- last_fit_model %&gt;%\n    collect_predictions() %&gt;%\n    conf_mat(Attrition_Flag, .pred_class)\n  \n  cm_tib &lt;- as_tibble(cm$table)%&gt;% mutate(\n    Prediction = factor(Prediction),\n    Truth = factor(Truth),\n    Prediction = factor(Prediction, \n                        levels = rev(levels(Prediction)))\n  )\n  \n  cm_tib %&gt;% ggplot(aes(x = Prediction, y = Truth,fill = n)) +\n    geom_tile( colour = \"gray50\")+\n    geom_text(aes(label = n))+\n    scale_fill_gradient(low = \"white\", high = high)+\n    theme_minimal()+\n    theme(legend.position = \"none\")\n}\n\n# Random Forest\ncm1&lt;- cm_plot(last_rf_fit,\"#506432\")\n\n# XGBoost\ncm2&lt;- cm_plot(last_xgb_fit,\"#FFC500\")\n\n# LightGBM\ncm3 &lt;- cm_plot(last_lightgbm_fit,\"#80003A\")\n\ntitle_pane &lt;- ggplot()+\n  geom_richtext(\n    data = title_tib,\n    aes(x, y, label = label),\n    hjust = 0, vjust = 1, \n    label.color = NA\n  ) +\n  xlim(0, 1) + ylim(0, 1)+\n  theme_void()\n\ncm1+cm2+cm3+title_pane+\nplot_layout(ncol =2, widths = c(1,1.04))\n\n\n\n\n\n\n\n\n\n\nSolution 6.9 (Exercise 6.9). According to the confusion matrices, ROC-, and PR-Curve the LightGBM model performs best.\n\nSensitivity: \\[\n  \\frac{\\mathrm{TP}}{\\mathrm{P}} = \\frac{364}{364+28} = 0.9285714\n\\]\nPrecision: \\[\n  \\frac{\\mathrm{TP}}{\\mathrm{PP}} = \\frac{364}{364+43} = 0.8943489\n\\]\nAccuracy: \\[\n\\frac{\\mathrm{TP}+\\mathrm{TN}}{\\mathrm{P}+\\mathrm{N}} = \\frac{364+2097}{364+2097+28+43} = 0.9719589\n      \\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Boosting</span>"
    ]
  },
  {
    "objectID": "06_boosting.html#session-info",
    "href": "06_boosting.html#session-info",
    "title": "6  Boosting",
    "section": "6.4 Session Info",
    "text": "6.4 Session Info\n\nsessionInfo()\n\nR version 4.2.3 (2023-03-15 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22631)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=German_Germany.utf8  LC_CTYPE=German_Germany.utf8   \n[3] LC_MONETARY=German_Germany.utf8 LC_NUMERIC=C                   \n[5] LC_TIME=German_Germany.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lightgbm_4.3.0     ranger_0.16.0      ggtext_0.1.2       patchwork_1.3.0   \n [5] bonsai_0.3.1       finetune_1.2.0     vip_0.4.1          xgboost_1.7.7.1   \n [9] workflowsets_1.1.0 workflows_1.1.4    tune_1.2.1         rsample_1.2.1     \n[13] recipes_1.1.0      parsnip_1.2.1      modeldata_1.4.0    infer_1.0.7       \n[17] dials_1.3.0        scales_1.3.0       broom_1.0.7        tidymodels_1.2.0  \n[21] yardstick_1.3.1    lubridate_1.9.3    forcats_1.0.0      stringr_1.5.1     \n[25] dplyr_1.1.4        purrr_1.0.2        readr_2.1.5        tidyr_1.3.1       \n[29] tibble_3.2.1       ggplot2_3.5.1      tidyverse_2.0.0   \n\nloaded via a namespace (and not attached):\n [1] doParallel_1.0.17   DiceDesign_1.10     tools_4.2.3        \n [4] backports_1.4.1     utf8_1.2.3          R6_2.5.1           \n [7] rpart_4.1.23        colorspace_2.1-0    nnet_7.3-19        \n[10] withr_3.0.1         tidyselect_1.2.1    compiler_4.2.3     \n[13] cli_3.6.2           xml2_1.3.6          labeling_0.4.3     \n[16] commonmark_1.9.1    digest_0.6.35       rmarkdown_2.28     \n[19] pkgconfig_2.0.3     htmltools_0.5.8.1   parallelly_1.37.1  \n[22] lhs_1.1.6           fastmap_1.1.1       htmlwidgets_1.6.4  \n[25] rlang_1.1.3         rstudioapi_0.17.0   farver_2.1.1       \n[28] generics_0.1.3      jsonlite_1.8.8      magrittr_2.0.3     \n[31] Matrix_1.6-0        Rcpp_1.0.12         munsell_0.5.1      \n[34] fansi_1.0.4         GPfit_1.0-8         lifecycle_1.0.4    \n[37] furrr_0.3.1         stringi_1.8.3       yaml_2.3.8         \n[40] MASS_7.3-58.2       grid_4.2.3          parallel_4.2.3     \n[43] listenv_0.9.1       lattice_0.22-6      splines_4.2.3      \n[46] gridtext_0.1.5      hms_1.1.3           knitr_1.43         \n[49] pillar_1.9.0        markdown_1.13       future.apply_1.11.2\n[52] codetools_0.2-20    glue_1.6.2          evaluate_1.0.1     \n[55] data.table_1.15.4   vctrs_0.6.5         tzdb_0.4.0         \n[58] foreach_1.5.2       gtable_0.3.5        future_1.33.0      \n[61] xfun_0.43           gower_1.0.1         prodlim_2023.08.28 \n[64] class_7.3-22        survival_3.6-4      timeDate_4041.110  \n[67] iterators_1.0.14    hardhat_1.4.0       lava_1.8.0         \n[70] timechange_0.3.0    globals_0.16.3      ipred_0.9-14",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Boosting</span>"
    ]
  }
]