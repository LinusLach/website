[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Supplementary Material",
    "section": "",
    "text": "Preface\nThis (exercise) manuscript supplements the lecture notes provided for Prof. Dr. Yarema Okhrin’s lecture Machine Learning at the University of Augsburg.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html#what-this-manuscript-is",
    "href": "index.html#what-this-manuscript-is",
    "title": "Machine Learning Supplementary Material",
    "section": "What this manuscript is",
    "text": "What this manuscript is\nThe manuscript intends to provide more context to different areas usually neglected in lecture and exercise sessions. The exercise sessions can especially suffer from an imbalance between repeating the theoretical aspects of the lecture and applying the concepts thoroughly. Moreover, this manuscript is comprehensive, containing every exercise and solution presented in the exercise sessions. The solutions will be more detailed than the ones presented in the in-person sessions, which can help you prepare for the exam.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-this-manuscript-isnt",
    "href": "index.html#what-this-manuscript-isnt",
    "title": "Machine Learning Supplementary Material",
    "section": "What this manuscript isn’t",
    "text": "What this manuscript isn’t\nThe manuscript is not a replacement for the lecture and exercise sessions. This manuscript especially lacks is the interaction between the students and lecturers which is an important aspect of the in-person exercise sessions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#structure-of-the-manuscript",
    "href": "index.html#structure-of-the-manuscript",
    "title": "Machine Learning Supplementary Material",
    "section": "Structure of the manuscript",
    "text": "Structure of the manuscript\nDepending on the complexity of the topic, each chapter starts with a more or less technical summary and motivation. Following these summaries there will be code examples that feature functions and concepts needed for the exercises.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "prerequisites.html",
    "href": "prerequisites.html",
    "title": "1  Important R concepts",
    "section": "",
    "text": "1.1 Installing R, RStudio, and getting started with Quarto\nBefore starting the revision, we need to ensure that R and RStudio are installed. The installation process for both R and RStudio is straightforward and user-friendly. While R (the programming language) comes with a preinstalled graphical user interface, we will use the integrated development environment RStudio instead, as it looks more appealing and provides some features RGui lacks. It is important to note that in order to get RStudio to work, R needs to be installed first.\nAfter successfully installing R and RStudio, starting the latter should open a window that somewhat looks like the following (cf. RStudio User Guide).\nThe Source pane displays a .R file named ggplot2.R. While .R files are the standard file format for R scripts used for programming in R, we will use Quarto documents (files ending with .qmd). Quarto labels itself as the\nmeaning that a Quarto document allows\nQuarto is preinstalled in RStudio, so creating such documents is relatively simple.\nThis should be enough to get you started with Quarto Documents. For further reading, I recommend the Quarto Guide.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#exercise-1-visual-data-exploration",
    "href": "prerequisites.html#exercise-1-visual-data-exploration",
    "title": "1  A brief summary of R",
    "section": "2.1 Exercise 1: Visual Data Exploration",
    "text": "2.1 Exercise 1: Visual Data Exploration\nIn this first exercise, we want to familiarize ourselves with the data set. ggplot2 will be our primary library for visualizations as it offers a vast framework of plot types and customizations. If you are not yet familiar with ggplot2, you can check out the R Graphics Cookbook, which is an open-source practical guide for generating high-quality graphs with R, and in particular with ggplot2.\n\n2.1.1 Exercise 1a:\nWe first want to explore some of the demographics in our data set. The following code generates a histogram for the age of the customers.\n\nggplot(data = credit_info, aes(x = Customer_Age))+geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nUsing the binwidth option, create a histogram for the age of the customers such that each age has its own bin. The output of your code should look as follows:\n\nggplot(data = credit_info, aes(Customer_Age))+geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\n\n\n2.1.2 Exercise 1b:\nNow that the histogram looks a bit less messy, we want to add more information. For example, by setting the fill option to Gender, we can get an initial feeling for the distribution between male and female customers within each age group in the data set.\n\nggplot(data = credit_info, aes(Customer_Age, fill = Gender))+geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nInstead of visualizing the Gender as in the plot above, create a histogram with the Attrition_Flag as the fill option. Note, that in this context an attrited customer is someone who is either planning to cancel the credit card or has already handed in the cancellation form. The resulting plot should look like this:\n\nggplot(data = credit_info, aes(Customer_Age, fill = Attrition_Flag))+\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\n\n\n2.1.3 Exercise 1c:\nThe histograms above only provide limited insight to the demographics and customer status as it is relatively difficult to figure out the proportions of each group. To takes this one step further, consider the following histogram, which shows the Education_Level within every bin.\n\nggplot(data = credit_info, aes(Customer_Age, fill = Education_Level))+\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nInstead of using a stacked histogram, we can resort to the facet_wrap function, which generates a subplot for each group.\n\nggplot(data = credit_info, aes(Customer_Age, fill = Education_Level))+\n  geom_histogram(binwidth = 1) +\n  facet_wrap(\"Education_Level\")\n\n\n\n\n\n\n\n\n\n2.1.3.1 Exercise 1c i:\nCreate a histogram as above, but instead of grouping the Education_Level, group for the different credit card categories. The result should look like the following plot:\n\nggplot(data = credit_info, aes(Customer_Age, fill = Income_Category))+\n  geom_histogram(binwidth = 1) +\n  facet_wrap(\"Income_Category\")\n\n\n\n\n\n\n\n\n\n\n2.1.3.2 Exercise 1c ii:\nThe legend in our generated plot is in no particular order. Familiarize yourself with the factor function, which helps solve that problem. By overwriting the Income_Category with the output of the factor function, we can order the income categories. Assign levels to the Income_Category using the factor function and generate a new plot with ordered labels. The new plot should look similar to the following:\n\ncredit_info$Income_Category&lt;-factor(credit_info$Income_Category,\n          levels = c(\"Unknown\",\"Less than $40K\",\n                     \"$40K - $60K\",\"$60K - $80K\",\"$80K - $120K\",\"$120K +\"),\n          labels = c(\"Unknown\",\"Less than $40K\",\n                     \"$40K - $60K\",\"$60K - $80K\",\"$80K - $120K\",\"$120K +\"))\n\nggplot(data = credit_info, aes(Customer_Age, fill = Income_Category))+\n  geom_histogram(binwidth = 1) +\n  facet_wrap(\"Income_Category\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A brief summary of R</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#exercise-2-statistical-data-exploration-and-manipulation",
    "href": "prerequisites.html#exercise-2-statistical-data-exploration-and-manipulation",
    "title": "1  A brief summary of R",
    "section": "2.2 Exercise 2: Statistical Data Exploration and Manipulation",
    "text": "2.2 Exercise 2: Statistical Data Exploration and Manipulation\n\n2.2.1 Exercise 2a:\nOne important aspect of Data Exploration is already visible in Exercise 1. The Income_Category of many customers is not known. That is not only the case for this variable but many others! However, instead of using the standard values NA or NaN, missing values are encoded by Unknown. The following code converts all the “Unknown” values to NA. By changing the values to NA we can filter them more conveniently in the later exercises.\n\ncredit_info_clean &lt;-credit_info %&gt;%\n  mutate(across(where(is.character), ~na_if(.,\"Unknown\")),\n         Income_Category = factor(Income_Category,\n                                  levels = c(NA,\"Less than $40K\",\n                                            \"$40K - $60K\",\"$60K - $80K\",\n                                            \"$80K - $120K\",\"$120K +\")))\n\n\n2.2.1.1 Exercise 2a i:\nGiven the cleaned data set credit_info_clean, find out which columns contain missing values. You can check your answer with the output below:\n\ncredit_info_clean %&gt;% select_if(function(col) sum(is.na(col))&gt;0) %&gt;% names\n\n[1] \"Education_Level\" \"Marital_Status\"  \"Income_Category\"\n\n\n\n\n2.2.1.2 Exercise 2a ii:\nFor the same data set, find out how many of the values are missing. You can check your answer with the output below:\n\nmissing &lt;- credit_info_clean %&gt;% select_if(function(col) sum(is.na(col))&gt;0) %&gt;% names\ncolSums(is.na(credit_info_clean[missing]))\n\nEducation_Level  Marital_Status Income_Category \n           1519             749            1112 \n\n\n\n\n\n2.2.2 Exercise 2b:\nInstead of visualizing our data, it is sometimes more convenient to directly examine numerical representations. Consider the following code chunk which groups the different income categories and applies the mean and median function to the data set.\n\nby_inc &lt;- credit_info_clean %&gt;% group_by(Income_Category) %&gt;%\n  summarise(\n    meanlim = mean(Credit_Limit),\n    medlim = median(Credit_Limit)\n  )\nby_inc\n\n# A tibble: 6 × 3\n  Income_Category meanlim medlim\n  &lt;fct&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n1 Less than $40K    3754.   2766\n2 $40K - $60K       5462.   3682\n3 $60K - $80K      10759.   7660\n4 $80K - $120K     15810.  12830\n5 $120K +          19717.  18442\n6 &lt;NA&gt;              9517.   6380\n\n\n\n2.2.2.1 Exercise 2b i:\nModify the code snippet above to include the \\(25\\%\\) and \\(75 \\%\\) quantile. A solution could look like this:\n\nby_inc &lt;- credit_info_clean %&gt;% group_by(Income_Category) %&gt;%\n  summarise(\n    \"mean\" = mean(Credit_Limit),\n    \"median\" = median(Credit_Limit),\n    \"1stQlim\" = quantile(Credit_Limit,probs = 0.25),\n    \"3rdQlim\" = quantile(Credit_Limit,probs = 0.75)\n  )\nby_inc\n\n# A tibble: 6 × 5\n  Income_Category   mean median `1stQlim` `3rdQlim`\n  &lt;fct&gt;            &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Less than $40K   3754.   2766     2021      4271 \n2 $40K - $60K      5462.   3682     2436.     6725 \n3 $60K - $80K     10759.   7660     3661.    15220.\n4 $80K - $120K    15810.  12830     5511     25182.\n5 $120K +         19717.  18442     8466.    34516 \n6 &lt;NA&gt;             9517.   6380     3137     12420.\n\n\n\n\n2.2.2.2 Exercise 2b ii:\nModify your code to omit the NA values.\n\nby_inc &lt;- credit_info_clean %&gt;% group_by(Income_Category) %&gt;%\n  summarise(\n    \"mean\" = mean(Credit_Limit),\n    \"median\" = median(Credit_Limit),\n    \"1stQlim\" = quantile(Credit_Limit,probs = 0.25),\n    \"3rdQplim\" = quantile(Credit_Limit,probs = 0.75)\n  ) %&gt;% na.omit\nby_inc\n\n\n\n\n2.2.3 Exercise 2c:\nSometimes we only want to infer results for certain subgroups. The Blue Credit Card is by far the most common type of credit card. Gaining insights for this particular group allows us to retrieve information that might be useful in later analyses.\nHint: For the following exercises (especially ii and iii) the dplyr function filter will be useful.\n\n2.2.3.1 Exercise 2c i:\nFind out how many customers have a Blue credit card.\n\ncount(credit_info_clean, Card_Category)[\"n\"] %&gt;% max\n\n\n\n2.2.3.2 Exercise 2c ii:\nCreate a new tibble or data frame called credit_info_blue containing all customers that hold a Blue credit card.\n\ncredit_info_blue &lt;- credit_info_clean %&gt;% filter(Card_Category == \"Blue\")\n\n\n\n2.2.3.3 Exercise 2c iii:\nFind the number of Female Customers holding the Blue Card, that are at most 40 years old and have a credit limit above 10,000 USD.\n\ntib &lt;- credit_info_blue %&gt;% filter(Gender == \"F\" & Customer_Age &lt;= 40 & Credit_Limit &gt; 10000) %&gt;% count()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A brief summary of R</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#exercise-3-losses",
    "href": "prerequisites.html#exercise-3-losses",
    "title": "1  A short revision of important R functions",
    "section": "4.4 Exercise 3: Losses",
    "text": "4.4 Exercise 3: Losses\n\n4.4.1 Exercise 3a:\nIn the lecture, we talked about different loss functions, as well as their advantages and disadvantages. The goal of this exercise is to review some of the loss functions introduced.\n\n4.4.1.1 Exercise 3a i:\nWrite three different functions loss_mse, loss_mae, and loss_rmse that calculate the respective loss value. Each function should take two vectors y and yhat as an input and return the respective loss value.\n\nloss_mse &lt;- function(y, yhat){\n  mean((y - yhat)^2)\n}\nloss_mae &lt;- function(y, yhat){\n  mean(abs(y - yhat))\n}\nloss_rmse &lt;- function(y, yhat){\n  sqrt(mean((y - yhat)^2))\n}\n\nTest the loss functions with the following inputs. You can check your answers below.\n\ntargets &lt;- c(120, 97, 4, 25, 15)\npredictions &lt;- c(111, 92, 9, 29, 20)\n\nloss_mse(targets, predictions)\n\n[1] 34.4\n\nloss_mae(targets, predictions)\n\n[1] 5.6\n\nloss_rmse(targets, predictions)\n\n[1] 5.865151\n\n\n\n\n4.4.1.2 Exercise 3a ii:\nWrite a function loss_huber that returns the Huber loss for two given vectors y, yhat, and a threshold \\(\\delta\\). The Huber loss of two vectors can be calculated as follows:\n\\[\n\\mathrm{loss_{huber}} =\\frac{1}{\\text{length(y)}} \\sum_{i=1}^{\\mathrm{length(y)}} \\mathcal{L}(y_i,\\hat y_i)\n\\]\nwhere\n\\[\n\\mathcal{L}(y_i,\\hat y_i) = \\begin{cases} \\frac{1}{2}(y_i-\\hat y_i)^2, \\quad&\\text{if } |y_i-\\hat y_i| &lt;\\delta\\\\ \\delta (|y_i-\\hat y_i|-\\frac{1}{2}\\delta),\\quad &\\text{else}.   \\end{cases}\n\\]\n\nloss_huber &lt;-function(y, yhat, d){\n  res &lt;- 0\n  for (i in seq_along(y)) {\n    if(abs(y[i] - yhat[i]) &lt; d){\n      res = res + 0.5 * (y[i] - yhat[i])^2\n    } else{\n      res = res + d * (abs(y[i] - yhat[i]) - 0.5 * d)\n    }\n  }\n    return(res/length(y))\n}\n\n\nloss_huber_alternative &lt;- function(y, yhat, d) {\n  residual &lt;- (y - yhat)\n\n  L &lt;- if_else(\n    abs(residual) &lt; d, \n    residual^2 / 2, \n    d * (abs(residual) - d / 2)\n  )\n  \n  mean(L)\n}\n\nYou can test your function with the vectors targets and predictions with d=1.\n\nloss_huber_alternative(targets, predictions, 1)\n\n[1] 5.1\n\n\n\n\n\n4.4.2 Exercise 3b:\nThe goal of this exercise is to gain a deeper understanding of the different loss types calculated in the previous exercises. Consider the following scenarios and argue which loss function should be used.\n\n4.4.2.1 Exercise 3b i:\nDecide between MSE and RMSE:\n\nYou are developing a predictive model for housing prices. Given various features of a house like size (in \\(m^2\\)) , number of floors, and the name of the neighborhood. You want to estimate its market value in EUR.\n\n\n#RMSE should be used to measure the average prediction error. This is the case\n#since by using RMSE the error is in the same units as the target variable (e.g.,\n#EUR). This helps in quantifying the magnitude of prediction errors accurately.\n\n\n\n4.4.2.2 Exercise 3b ii:\nDecide between MAE and MSE:\n\nImagine you are working on a weather prediction model, where you aim to forecast daily temperatures for a location with strong and sudden changes in weather. The data contains average wind velocity, air pressure, humidity, and many more. You also have historical temperature data for training and the overall goal is to minimize prediction errors.\n\n\n#MAE should be used to measure the predictive error since it is more robust with\n#respect to outliers. Weather data can sometimes contain outliers due to extreme\n#weather events, sensor malfunctions, or other factors.\n\n\n\n4.4.2.3 Exercise 3b iii:\nDecide between MAE, MSE, and Huber loss\n\nYou are building a machine-learning model for autonomous vehicle control. The vehicles have to navigate through complex environments with all sorts of different obstacles varying in size and shape. The goal is to ensure the vehicle makes safe decisions based on some numerical value calculated based on the obstacles observed.\n\n\n# Huber Loss is the ideal choice because it strikes a balance between the\n#robustness of MAE and the sensitivity to larger errors of MSE. Especially for\n#varying values of delta a well balanced loss function can be achieved.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A short revision of important R functions</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#installing-r-rstudio-and-getting-started-with-quarto",
    "href": "prerequisites.html#installing-r-rstudio-and-getting-started-with-quarto",
    "title": "1  Important R concepts",
    "section": "",
    "text": "R (for Windows) can be downloaded here.\nRStudio can be downloaded here.\n\n\n\n\n\n\n\n\n\nnext-generation version of R Markdown,\n\n\n\nweaving together narrative text and code to produce elegantly formatted output as documents, web pages, books, and more.\n\n\n\nclick on New File -&gt; Quarto Document... in the File tab \nOnce the New Quarto Document window opens, you can modify the title and specify the output format. For the sake of simplicity, we will use the HTML output format. However, you could also choose PDF if you have a LaTeX installation on your device. Clicking on the create button will create a new Quarto Document. \nThe source pane now displays a sample Quarto document that can be modified. You might have to install rmarkdown (cf. mark 1). The Quarto document can then be modified by clicking on the respective sections. R Code cells can be executed by clicking on the green arrow (cf. mark 2). To insert new R code cells between paragraphs, click on the Insert tab and select Executable Cell -&gt; R (cf. mark 3).\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nWhile solving the (programming) exercises, you can create a new paragraph using # for a new chapter to make your document more readable. Additionally, you can simply create a section using ##.\nWriting down some details on how you approached the programming exercises in a paragraph above or below can help you understand your approach when repeating the exercises later.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#working-directories-and-projects",
    "href": "prerequisites.html#working-directories-and-projects",
    "title": "1  Important R concepts",
    "section": "1.2 Working directories and projects",
    "text": "1.2 Working directories and projects\nAfter opening RStudio, execute the getwd() command in the console pane, which returns the current working directory. The working directory displays the directory of the R process. For example, if the return value of the getwd() command is C:\\Users\\lachlinu\\Desktop\\ML, then R can access any file in the ML directory. One way to change the working directory is using the setwd() command, which changes the current working directory. Manually changing the directory in every .qmd document might become tedious after a while, so a more practical alternative is setting up a project. RStudio projects allow the creation of an individual working directory for multiple contexts. For example, you might use R not only for solving Machine Learning exercises but also for your master’s thesis. Then, setting up two different projects will help you organize working directories and workspaces for each project individually. To set up a project for this course\n\nGo to the File tab and select New Project....\n\n\n\n\n\nChoose Existing Directory and navigate to the directory in which you want to create the project in and click the Create Project button.\n\n\n\n\n\nYou can now open the project by double clicking on the icon which should open a new RStudio window.\n\n\n\n\n\n\nOnce the project is opened, running the getwd() command in the console pane returns its path. Any file in this directory can be directly accessed without specifying the preceding path.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#importing-data",
    "href": "prerequisites.html#importing-data",
    "title": "1  Important R concepts",
    "section": "3.1 Importing data",
    "text": "3.1 Importing data\nTo manipulate data, we first need to import it. R has quite a few preinstalled data sets; however, I prefer data sets that are either more complex, related to everyday life, or just more fun to explore. This course will provide most of the data in .csv or .txt files. Before we can start manipulating data, it’s essential to import it correctly. This ensures that the data is in the right format for further analysis.\nConsider the Netflix Movies and TV Shows data set, which can be downloaded from the data science community website Kaggle or directly below.\n\nDownload Netflix Data\n\nOnce you have downloaded the data and placed it in your current working directory, you can import it using the read.csv command:\n\ndata_netflix &lt;- read.csv(\"netflix_titles.csv\")\n\n\n\n\n\n\n\nPro Tip\n\n\n\nTo maintain structure in your project folder, it is advisable to create a separate directory for the data and import it from there. For example, if the project directory contains a data directory with the netflix_title.csv file inside, it can be imported using\n\ndata_netflix &lt;- read.csv(\"data/netflix_titles.csv\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#essential-functions-and-libraries",
    "href": "prerequisites.html#essential-functions-and-libraries",
    "title": "1  A short revision of important R functions",
    "section": "4.2 Essential functions and libraries",
    "text": "4.2 Essential functions and libraries\nOne of the most important collection of libraries in the context of data science with R is the {tidyverse} library which includes\n\n{ggplot} for creating beautiful graphics,\n{dplyr} for data manipulation,\n{stringr} for working with strings, and\n{tibble} for storing data effectively.\n\nA great in-depth introduction to the tidyverse called R for Data Science is freely available online if that peaks your interest. In this introduction, the focus is on a few core functions that will be useful throughout the course. As every other library {tidyverse} can be attached using the library function, once it has been installed:\n\nlibrary(tidyverse)\n\nOnce the library has been added, every function contained is available. For example, the glimpse function can be used on the data_netflix data set to get a short overview of the data types and contents:\n\nglimpse(data_netflix)\n\nRows: 8,807\nColumns: 12\n$ show_id      &lt;chr&gt; \"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"s6\", \"s7\", \"s8\", \"s9\", \"s1…\n$ type         &lt;chr&gt; \"Movie\", \"TV Show\", \"TV Show\", \"TV Show\", \"TV Show\", \"TV …\n$ title        &lt;chr&gt; \"Dick Johnson Is Dead\", \"Blood & Water\", \"Ganglands\", \"Ja…\n$ director     &lt;chr&gt; \"Kirsten Johnson\", \"\", \"Julien Leclercq\", \"\", \"\", \"Mike F…\n$ cast         &lt;chr&gt; \"\", \"Ama Qamata, Khosi Ngema, Gail Mabalane, Thabang Mola…\n$ country      &lt;chr&gt; \"United States\", \"South Africa\", \"\", \"\", \"India\", \"\", \"\",…\n$ date_added   &lt;chr&gt; \"September 25, 2021\", \"September 24, 2021\", \"September 24…\n$ release_year &lt;int&gt; 2020, 2021, 2021, 2021, 2021, 2021, 2021, 1993, 2021, 202…\n$ rating       &lt;chr&gt; \"PG-13\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"PG…\n$ duration     &lt;chr&gt; \"90 min\", \"2 Seasons\", \"1 Season\", \"1 Season\", \"2 Seasons…\n$ listed_in    &lt;chr&gt; \"Documentaries\", \"International TV Shows, TV Dramas, TV M…\n$ description  &lt;chr&gt; \"As her father nears the end of his life, filmmaker Kirst…\n\n\nRows: 8,807 means that the data set has 8,807 entries and Columns: 12 that the data set has 12 variables respectively. The first column contains the variable names, following the data type and some values in third column. We can already see, that except for one variable (release_year), every other variable is of type chr which stands for character or string.\n\n4.2.1 Filtering, grouping, and summarizing data sets\nFunctions frequently encountered while working with data are filter, group_by, and summarise. Say we want to find out how many movies and series were released in each year subsequent to the year 2010 according to the data set. Without using the {tidyverse} framework and going too much into detail, the code could look something like this:\n\nnetflix_filtered &lt;- data_netflix[data_netflix$release_year &gt; 2010, ]\nresult &lt;- aggregate(rep(1, nrow(netflix_filtered)), \n                    by = list(netflix_filtered$release_year), \n                    FUN = sum)\ncolnames(result) &lt;- c(\"release_year\", \"n\")\nresult\n\n   release_year    n\n1          2011  185\n2          2012  237\n3          2013  288\n4          2014  352\n5          2015  560\n6          2016  902\n7          2017 1032\n8          2018 1147\n9          2019 1030\n10         2020  953\n11         2021  592\n\n\nor this:\n\nnetflix_filtered &lt;- data_netflix[data_netflix$release_year &gt; 2010, ]\nresult &lt;- as.data.frame(table(netflix_filtered$release_year))\ncolnames(result) &lt;- c(\"release_year\", \"n\")\nresult\n\n   release_year    n\n1          2011  185\n2          2012  237\n3          2013  288\n4          2014  352\n5          2015  560\n6          2016  902\n7          2017 1032\n8          2018 1147\n9          2019 1030\n10         2020  953\n11         2021  592\n\n\nThe first code cell seems a lot more complicated than the second, yet it returns the same result. However, things can be simplified even more using the {dplyr} library that is contained in the {tidyverse}:\n\nnetflix_filtered &lt;- data_netflix %&gt;%\n  filter(release_year&gt;2010) %&gt;%\n  group_by(release_year) %&gt;%\n  summarise(n= n())\n\nLet’s break down the code snippet above:\n\nIn line 1 we use the pipe operator %&gt;%. It is part of the {magrittr} package and forwards an object into a function of call expression. Figuratively, a pipe does exactly what is expected: channel an object from one and to another. In this case, the pipe operator %&gt;% passes the data_netflix data set into the filter function.\nIn line 2 the filter function selects a subset of a data set that satisfies a given condition. Here, the condition is, that the release year of the movie or series should be after 2010 which is indicated by the &gt; condition.\n\n\n\n\n\n\nNote\n\n\n\nWithout the pipe operator, the first and second line can be merged into\n\nfilter(data_netflix,release_year&gt;2010)\n\nhowever, concatenating multiple functions causes the code to be unreadable and should thus be avoided.\n\n\nResults of the filtering procedure are then passed to the group_by function via the pipe operator again. The group_by function converts the underlying data set into a grouped data set where operations can be performed group wise. In the third line of the code cell, the group_by function is applied to the release_year variable, meaning that the data set now contains a group for every release year.\nThis can be seen as a pre processing step for the summarise function that is applied in the next line. The summarise function creates a new data set based on the functions that are passed as arguments. These functions are applied to every group created by the previous step. In the example above, the function applied is n() which returns the group size. Thus, setting n=n() as the argument creates a new column named n which contains the number of samples within each group.\n\n\n\n4.2.2 Mutating data sets\nBesides filtering, grouping, and summarizing, another important concept is mutating the data, i.e., modifying the content of the data set.\nThe mutate function either creates new columns or modifies existing columns based on the passed column names and functions that are passed. It is useful for modifying data and their types, creating new variables based on existing ones, and removing unwanted variables. In the example below, the mutate function is used to modify the variable date_added, create a new variable called is_show and delete the variable type.\n\ndata_netflix &lt;- data_netflix %&gt;% \n  mutate(date_added = mdy(date_added),\n         is_show = if_else(type==\"TV Show\",TRUE,FALSE),\n         type = NULL\n         )\n\n\nIn the first line of the code cell, the data set data_netflix is specified to be overwritten by it’s mutated version. This is achieved by reassigning the data_netlifx object to the output of the pipe concatenation of the following code lines.\nIn the second line the original data_netflix data set is passed into the mutate function. Here, the variable date_written is overwritten by the output of the mdy function with argument date_added. The mdy function is a function in the {lubridate} library that transforms dates that are stored in strings to date objects that are easier to handle. Note, that we can directly pass column names into functions as we have previously passed the data set into the mutate function using the %&gt;% operator.\nIn the third line a new variable is_show is created which takes the value TRUE, if the type of an entry in the data set is \"TV Show\" and FALSE if it isn’t. This is achieved by the if_else function.\nBy setting the type variable to NULL, the variable is effectively removed from the data set.\n\n\n\n\n\n\n\nNote\n\n\n\nAssigning values in functions is achieved by using the = symbol. Assigning new variables outside of functions can also be done with the = symbol, but it is rarely used and except for some pathological cases there is no difference. However, most R users prefer assigning environment variables using &lt;- which does not work in function calls.\n\n\n\n\n\n\n\n\nPro Tip\n\n\n\nIn the previous code cell a line break was added after the %&gt;% and each argument in the mutate function for readability purposes. The code also works without adding the line breaks, but it can get messy fast:\n\ndata_netflix &lt;- data_netflix %&gt;% mutate(date_added = mdy(date_added), is_show = if_else(type==\"TV Show\",TRUE,FALSE), type = NULL)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A short revision of important R functions</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#visualizing-data-with-ggplot",
    "href": "prerequisites.html#visualizing-data-with-ggplot",
    "title": "1  Important R concepts",
    "section": "3.3 Visualizing data with ggplot",
    "text": "3.3 Visualizing data with ggplot\nAnother critical aspect of data science and machine learning is graphical storytelling. Describing an algorithm strictly using mathematical notation or exploring a data set using descriptive and inductive statistics alone can make it challenging to understand the message. While R offers some base functions for creating graphics, this course primarily uses the library {ggplot2}. A comprehensive introduction to {ggplot2} can be found in Hadley Wickham’s book Elegant Graphics for Data Analysis. A short summary can be found below.\nFor the following example, we will use the netflix_filtered data set (see Section 3.2.1)\nA graphic created with {ggplot2} consists of the following three base components:\n\nThe data itself.\n\nggplot(data = netflix_filtered)\n\n\n\n\n\n\n\n\nNote, that the plot does not show any axis, ticks, and variables.\nA set of aesthetics mappings that describe how variables in the data are mapped to visual properties.\n\nggplot(aes(x=release_year, y=n), data = netflix_filtered)\n\n\n\n\n\n\n\n\nUsing the aes function, we have specified that the release year should be mapped to the \\(x\\)-axis, and \\(n\\) to the \\(y\\)-axis.\nLastly, the geom-layer (component) describes how each observation in the data set is represented.\n\nggplot(aes(x=release_year, y=n), data = netflix_filtered)+\n  geom_col()+\n  geom_point()+\n  geom_line()\n\n\n\n\n\n\n\n\nCompared to the previous two code cells, a lot is going on here. So, let us break it down.\n\nThe plus at the end of line 1 is used to add another layer.\ngeom_col adds a column chart to the canvas, creating columns starting at 0 and ending at \\(n\\). Then, + indicates that another layer is added.\ngeom_point represents the data as points on the plane, i.e., an \\(x\\) and \\(y\\)-coordinate. The + indicates that yet another layer is added afterward.\nLastly, the geom_line function adds a line connecting each data point with the one following.\n\n\n\n\n\n\n\nPro Tips\n\n\n\n\nAs before, the data set can also directly be piped into the ggplot function:\n\nnetflix_filtered %&gt;%\nggplot(aes(x=release_year, y=n))+\n geom_col()+\n geom_point()+\n geom_line()\n\nBy changing the order of the layers, you can specify which layer should be added first and last. In this example, since geom_col was added first and every other layer is placed on top of the column plot.\n\n\n\n\nThere are a lot more functions and settings that can be applied to each function. A selection of those is discussed in the exercises.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#exercise-1-statistical-data-exploration-and-manipulation",
    "href": "prerequisites.html#exercise-1-statistical-data-exploration-and-manipulation",
    "title": "1  A short revision of important R functions",
    "section": "4.1 Exercise 1: Statistical Data Exploration and Manipulation",
    "text": "4.1 Exercise 1: Statistical Data Exploration and Manipulation\nSimilar to Section 3.2 we will start out by getting a feeling for the data and performing some basic data manipulation steps.\n\n4.1.1 \nImport the data set and use the glimpse function to generate a short summary of the data set.\n\n\n4.1.2 \nAssume that the data set has been imported and saved as an object called credit_info. Explain the following code snippet both syntactically and semantically. Hint: use the help function for any function you do not know.\n\ncredit_info %&gt;%\n  select_if(is.character) %&gt;%\n  map(table)\n\n$Attrition_Flag\n\nAttrited Customer Existing Customer \n             1627              8500 \n\n$Gender\n\n   F    M \n5358 4769 \n\n$Education_Level\n\n      College     Doctorate      Graduate   High School Post-Graduate \n         1013           451          3128          2013           516 \n   Uneducated       Unknown \n         1487          1519 \n\n$Marital_Status\n\nDivorced  Married   Single  Unknown \n     748     4687     3943      749 \n\n$Income_Category\n\n       $120K +    $40K - $60K    $60K - $80K   $80K - $120K Less than $40K \n           727           1790           1402           1535           3561 \n       Unknown \n          1112 \n\n$Card_Category\n\n    Blue     Gold Platinum   Silver \n    9436      116       20      555 \n\n\n\n\n4.1.3 \nConvert the variables Income_Category and Education_Level into ordered factors and overwrite the previously imported data set.\n\n\n4.1.4 \nGroup the data set by income category and find out the mean and median credit limit for each group.\n\n\n4.1.5 \nWhich income group has the highest mean credit limit?\n\n\n4.1.6 \nUse the following code snippet to modify the dataset by incorporating it into the mutate function. The snippet converts all \"Unknown\" values into NA values which are easier to handle.\n\n  across(where(is.character), ~na_if(.,\"Unknown\"))\n\n\n\n4.1.7 \nApply the na.omit() function to the data set to remove all samples in the dataset that contain NA values. How many samples are removed in total?\n\n\n4.1.8 \nSometimes we only want to infer results for certain subgroups. The Blue Credit Card is by far the most common type of credit card. Gaining insights for this particular group allows us to retrieve information that might be useful in later analyses.\nFind out how many customers have a Blue credit card.\n\n\n4.1.9 \nCreate a new tibble or data frame called credit_info_blue containing all customers that hold a Blue credit card.\n\ncredit_info_blue &lt;- credit_info_clean %&gt;% filter(Card_Category == \"Blue\")\n\n\n\n4.1.10 \nFind the number of Female Customers holding the Blue Card, that are at most 40 years old and have a credit limit above 10,000 USD.\n\ncredit_info_blue %&gt;% filter(Gender == \"F\" & Customer_Age &lt;= 40 & Credit_Limit &gt; 10000) %&gt;% count()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A short revision of important R functions</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#exercise-2-visual-data-exploration",
    "href": "prerequisites.html#exercise-2-visual-data-exploration",
    "title": "1  A short revision of important R functions",
    "section": "4.2 Exercise 2: Visual Data Exploration",
    "text": "4.2 Exercise 2: Visual Data Exploration\n\n4.2.1 Exercise 2a:\nWe first want to explore some of the demographics in our data set. The following code generates a histogram for the age of the customers.\n\nggplot(data = credit_info, aes(x = Customer_Age))+geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nUsing the binwidth option, create a histogram for the age of the customers such that each age has its own bin. The output of your code should look as follows:\n\nggplot(data = credit_info, aes(Customer_Age))+geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\n\n\n4.2.2 Exercise 2b:\nNow that the histogram looks a bit less messy, we want to add more information. For example, by setting the fill option to Gender, we can get an initial feeling for the distribution between male and female customers within each age group in the data set.\n\nggplot(data = credit_info, aes(Customer_Age, fill = Gender))+geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nInstead of visualizing the Gender as in the plot above, create a histogram with the Attrition_Flag as the fill option. Note, that in this context an attrited customer is someone who is either planning to cancel the credit card or has already handed in the cancellation form. The resulting plot should look like this:\n\nggplot(data = credit_info, aes(Customer_Age, fill = Attrition_Flag))+\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\n\n\n4.2.3 Exercise 2c:\nThe histograms above only provide limited insight to the demographics and customer status as it is relatively difficult to figure out the proportions of each group. To takes this one step further, consider the following histogram, which shows the Education_Level within every bin.\n\nggplot(data = credit_info, aes(Customer_Age, fill = Education_Level))+\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nInstead of using a stacked histogram, we can resort to the facet_wrap function, which generates a subplot for each group.\n\nggplot(data = credit_info, aes(Customer_Age, fill = Education_Level))+\n  geom_histogram(binwidth = 1) +\n  facet_wrap(\"Education_Level\")\n\n\n\n\n\n\n\n\n\n4.2.3.1 Exercise 2c i:\nCreate a histogram as above, but instead of grouping the Education_Level, group for the different credit card categories. The result should look like the following plot:\n\nggplot(data = credit_info, aes(Customer_Age, fill = Income_Category))+\n  geom_histogram(binwidth = 1) +\n  facet_wrap(\"Income_Category\")\n\n\n\n\n\n\n\n\n\n\n4.2.3.2 Exercise 2c ii:\nThe legend in our generated plot is in no particular order. Familiarize yourself with the factor function, which helps solve that problem. By overwriting the Income_Category with the output of the factor function, we can order the income categories. Assign levels to the Income_Category using the factor function and generate a new plot with ordered labels. The new plot should look similar to the following:\n\ncredit_info$Income_Category&lt;-factor(credit_info$Income_Category,\n          levels = c(\"Unknown\",\"Less than $40K\",\n                     \"$40K - $60K\",\"$60K - $80K\",\"$80K - $120K\",\"$120K +\"),\n          labels = c(\"Unknown\",\"Less than $40K\",\n                     \"$40K - $60K\",\"$60K - $80K\",\"$80K - $120K\",\"$120K +\"))\n\nggplot(data = credit_info, aes(Customer_Age, fill = Income_Category))+\n  geom_histogram(binwidth = 1) +\n  facet_wrap(\"Income_Category\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A short revision of important R functions</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#sec-essentials",
    "href": "prerequisites.html#sec-essentials",
    "title": "1  Important R concepts",
    "section": "3.2 Essential functions and libraries",
    "text": "3.2 Essential functions and libraries\nOne of the most versatile and essential collection of libraries in the context of data science with R is the {tidyverse} library, which includes\n\n{ggplot} for creating beautiful graphics,\n{dplyr} for data manipulation,\n{stringr} for working with strings, and\n{tibble} for storing data effectively.\n\nAn excellent in-depth introduction to the tidyverse called R for Data Science is freely available online if that piques your interest. This introduction focuses on a few core functions that will be useful throughout the course. As every other library, {tidyverse} can be attached using the library function once it has been installed:\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nOnce the library has been added, every function contained is available. For example, the glimpse function can be used on the data_netflix data set to get a short overview of the data types and contents:\n\nglimpse(data_netflix)\n\nRows: 8,807\nColumns: 12\n$ show_id      &lt;chr&gt; \"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"s6\", \"s7\", \"s8\", \"s9\", \"s1…\n$ type         &lt;chr&gt; \"Movie\", \"TV Show\", \"TV Show\", \"TV Show\", \"TV Show\", \"TV …\n$ title        &lt;chr&gt; \"Dick Johnson Is Dead\", \"Blood & Water\", \"Ganglands\", \"Ja…\n$ director     &lt;chr&gt; \"Kirsten Johnson\", \"\", \"Julien Leclercq\", \"\", \"\", \"Mike F…\n$ cast         &lt;chr&gt; \"\", \"Ama Qamata, Khosi Ngema, Gail Mabalane, Thabang Mola…\n$ country      &lt;chr&gt; \"United States\", \"South Africa\", \"\", \"\", \"India\", \"\", \"\",…\n$ date_added   &lt;chr&gt; \"September 25, 2021\", \"September 24, 2021\", \"September 24…\n$ release_year &lt;int&gt; 2020, 2021, 2021, 2021, 2021, 2021, 2021, 1993, 2021, 202…\n$ rating       &lt;chr&gt; \"PG-13\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"PG…\n$ duration     &lt;chr&gt; \"90 min\", \"2 Seasons\", \"1 Season\", \"1 Season\", \"2 Seasons…\n$ listed_in    &lt;chr&gt; \"Documentaries\", \"International TV Shows, TV Dramas, TV M…\n$ description  &lt;chr&gt; \"As her father nears the end of his life, filmmaker Kirst…\n\n\nRows: 8,807 means that the data set has 8,807 entries, and Columns: 12 means that the data set has 12 variables, respectively. The first column presents the variable names, their data types, and some initial values, providing a clear structure to the data set. We can already see that except for one variable (release_year), every other variable is of type chr, which stands for character or string.\n\n3.2.1 Filtering, grouping, and summarizing data sets\nFunctions frequently encountered while working with data are filter, group_by, and summarise. Let’s say we want to find out, according to the data set, how many movies and series were released in each year following 2010. Now, if we were to tackle this problem without the {tidyverse} framework, our code might look a little something like this:\n\nnetflix_filtered &lt;- data_netflix[data_netflix$release_year &gt; 2010, ]\nresult &lt;- aggregate(rep(1, nrow(netflix_filtered)), \n                    by = list(netflix_filtered$release_year), \n                    FUN = sum)\ncolnames(result) &lt;- c(\"release_year\", \"n\")\nresult\n\n   release_year    n\n1          2011  185\n2          2012  237\n3          2013  288\n4          2014  352\n5          2015  560\n6          2016  902\n7          2017 1032\n8          2018 1147\n9          2019 1030\n10         2020  953\n11         2021  592\n\n\nor this:\n\nnetflix_filtered &lt;- data_netflix[data_netflix$release_year &gt; 2010, ]\nresult &lt;- as.data.frame(table(netflix_filtered$release_year))\ncolnames(result) &lt;- c(\"release_year\", \"n\")\nresult\n\n   release_year    n\n1          2011  185\n2          2012  237\n3          2013  288\n4          2014  352\n5          2015  560\n6          2016  902\n7          2017 1032\n8          2018 1147\n9          2019 1030\n10         2020  953\n11         2021  592\n\n\nThe first code cell seems much more complicated than the second, yet it returns the same result. However, things can be simplified even more using the {dplyr} library that is contained in the {tidyverse}:\n\nnetflix_filtered &lt;- data_netflix %&gt;%\n  filter(release_year&gt;2010) %&gt;%\n  group_by(release_year) %&gt;%\n  summarise(n= n())\n\nLet us break down the code snippet above:\n\nIn line 1 we use the pipe operator %&gt;%. It is part of the {magrittr} package and forwards an object into a function of call expression. Figuratively, a pipe does precisely what is expected: channel an object from one and to another. In this case, the pipe operator %&gt;% passes the data_netflix data set into the filter function.\nIn line 2 the filter function selects a subset of a data set that satisfies a given condition. Here, the condition is that the movie or series’ release year should be after 2010, which is indicated by the &gt; condition.\n\n\n\n\n\n\nNote\n\n\n\nWithout the pipe operator, the first and second line can be merged into\n\nfilter(data_netflix,release_year&gt;2010)\n\nhowever, concatenating multiple functions causes the code to be unreadable and should thus be avoided.\n\n\nResults of the filtering procedure are then passed to the group_by function via the pipe operator again. The group_by function converts the underlying data set into a grouped one where operations can be performed group-wise. In the third line of the code cell, the group_by function is applied to the release_year variable, meaning that the data set now contains a group for every release year.\nThis can be seen as a pre-processing step for the summarise function applied in the following line. The summarise function creates a new data set based on the functions passed as arguments. These functions are applied to every group created by the previous step. In the example above, the function applied is n(), which returns the group size. Thus, setting n=n() as the argument creates a new column named n, which contains the number of samples within each group.\n\n\n\n3.2.2 Mutating data sets\nBesides filtering, grouping, and summarizing, another important concept is mutating the data, i.e., modifying the content of the data set.\nThe mutate function either creates new columns or modifies existing columns based on the passed column names and functions that are passed. It is helpful for modifying data and their types, creating new variables based on existing ones, and removing unwanted variables. In the example below, the mutate function is used to modify the variable date_added, create a new variable called is_show and delete the variable type.\n\ndata_netflix &lt;- data_netflix %&gt;% \n  mutate(date_added = mdy(date_added),\n         is_show = if_else(type==\"TV Show\",TRUE,FALSE),\n         type = NULL\n         )\n\n\nIn the first line of the code cell, the data set data_netflix is specified to be overwritten by its mutated version. Overwriting a data set is achieved by reassigning the data_netlifx object to the output of the pipe concatenation of the following code lines.\nThe original data_netflix data set is passed into the mutate function in the second line. Here, the variable date_written is overwritten by the output of the mdy function with argument date_added. The mdy function is a function in the {lubridate} library that transforms dates stored in strings to date objects that are easier to handle. Note that we can directly pass column names into functions as we have previously passed the data set into the mutate function using the %&gt;% operator.\nIn the third line, a new variable is_show is created, which takes the value TRUE, if the type of an entry in the data set is \"TV Show\" and FALSE if it is not. The if_else function achieves this.\nSetting the type variable to NULL effectively removes it from the data set.\n\n\n\n\n\n\n\nNote\n\n\n\nAssigning values in functions is achieved by using the = symbol. Assigning new variables outside of functions can also be done with the = symbol, but it is rarely used and except for some pathological cases there is no difference. However, most R users prefer assigning environment variables using &lt;- which does not work in function calls.\n\n\n\n\n\n\n\n\nPro Tip\n\n\n\nIn the previous code cell a line break was added after the %&gt;% and each argument in the mutate function for readability purposes. The code also works without adding the line breaks, but it can get messy fast:\n\ndata_netflix &lt;- data_netflix %&gt;% mutate(date_added = mdy(date_added), is_show = if_else(type==\"TV Show\",TRUE,FALSE), type = NULL)\n\n\n\n\n\n3.2.3 Factor Variables\nAn important data type that can handle both ordinal (data with some notion of order) and nominal data are so-called factor variables.\nConsider the following toy data set containing seven people with corresponding age groups and eye colors.\n\n\n# A tibble: 7 × 3\n  names   age_groups eye_color\n  &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;    \n1 Alice   18-25      Blue     \n2 Bob     &lt;18        Brown    \n3 Charlie 26-35      Green    \n4 Diana   36-45      Hazel    \n5 Eve     18-25      Brown    \n6 Frank   60+        Blue     \n7 Grace   26-35      Green    \n\n\nSince the variable age_group only specifies a range of ages, it does not make sense to encode them as integers rather than ordinal variables. The’ mutate’ function can encode age groups as ordinal variables. This involves setting the age_groups variable to a factor with levels and labels. Levels specify the order of the values, and labels can be used to rename these categories.\n\ndata_example &lt;- data_example %&gt;%\n  mutate( \n    age_groups = factor(\n      age_groups,\n      levels = c(\"&lt;18\", \"18-25\", \"26-35\", \"36-45\", \"60+\"),\n      labels = c(\"child\",\"adult\",\"adult\",\"adult\",\"senior\"),\n      ordered = TRUE\n    )\n  )\n\n\nSimilar to the previous example, we should specify that we overwrite the data_example data set with a mutated version.\nThe mutate function is applied to the age_groups variable.\nSetting age_groups = factor(age_groups, ...) converts the age_groups column into a (so far unordered) factor, allowing for specific levels (categories) and labels.\nlevels = c(\"&lt;18\", \"18-25\",...) specifies the predefined levels for the age groups.\nordered=TRUE specifies that the age groups are ordered according to the specified levels.\nLast but not least, labels = c(\"child\", \"adult\", ...) specifies the labels that replace the numeric age groups. For instance, &lt;18 is labeled as \"child\", the ranges 18-25, 26-35, and 36-45 are labeled as \"adult\", and 60+ is labeled as \"senior\".\n\nSimilarly, the variable eye_color can also be converted to a nominal factor variable:\n\ndata_example &lt;- data_example %&gt;%\n  mutate(\n    eye_color = factor(eye_color)\n  )\n\nTo confirm that the variable age_group is indeed ordered, calling the feature shows the ordered levels:\n\ndata_example$age_groups\n\n[1] adult  child  adult  adult  adult  senior adult \nLevels: child &lt; adult &lt; senior",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#visual-data-exploration",
    "href": "prerequisites.html#visual-data-exploration",
    "title": "1  Important R concepts",
    "section": "4.2 Visual Data Exploration",
    "text": "4.2 Visual Data Exploration\n\nExercise 4.11 We now want to explore some of the demographics in our data set. Create a histogram for the age of the customers using the geom_histogram function. Note that only one variable is required for the aesthetics to create a histogram.\n\n\nExercise 4.12 Using the default parameters in the geom_histogram function, the message “stat_bin() using bins = 30. Pick better value with binwidth.” is displayed. Modify the code so that each age gets its own bin.\n\n\nExercise 4.13 Now that the histogram looks more organized, we want to add more information. For example, by setting the fill option to Gender, we can create two overlapping histograms showing the number of male and female customers within each age group.\n\n\nExercise 4.14 Instead of visualizing the Gender as in the plot above, we now want to analyze the continuous variable Credit_Limit. Therefore, instead of a histogram, use the geom_density function that plots an estimate of the underlying probability density.\n\n\nExercise 4.15 The histograms and density plots only provide limited insight into the demographics and customer status as it is relatively complex to figure out the proportions of each group. To take this one step further, consider the following histogram, which shows the Education_Level within every bin.\n\nggplot(data = credit_info_clean, aes(Customer_Age, fill = Education_Level))+\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nWe can use the geom_histogram function and the facet_wrap function, which generates a subplot for each group. Apply the facet_wrap function to create a subplot for each education level.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#statistical-data-exploration-and-manipulation",
    "href": "prerequisites.html#statistical-data-exploration-and-manipulation",
    "title": "1  Important R concepts",
    "section": "4.1 Statistical Data Exploration and Manipulation",
    "text": "4.1 Statistical Data Exploration and Manipulation\nWe will start by getting a feeling for the data and performing some basic data manipulation steps.\n\nExercise 4.1 Import the data set and use the glimpse function to generate a summary of the data set.\n\n\nExercise 4.2 Assume that the data set has been imported and saved as an object called credit_info. Explain the following code snippet both syntactically and semantically. Hint: Use the help function for any function you do not know.\n\ncredit_info %&gt;%\n  select_if(is.character) %&gt;%\n  sapply(table)\n\n$Attrition_Flag\n\nAttrited Customer Existing Customer \n             1627              8500 \n\n$Gender\n\n   F    M \n5358 4769 \n\n$Education_Level\n\n      College     Doctorate      Graduate   High School Post-Graduate \n         1013           451          3128          2013           516 \n   Uneducated       Unknown \n         1487          1519 \n\n$Marital_Status\n\nDivorced  Married   Single  Unknown \n     748     4687     3943      749 \n\n$Income_Category\n\n       $120K +    $40K - $60K    $60K - $80K   $80K - $120K Less than $40K \n           727           1790           1402           1535           3561 \n       Unknown \n          1112 \n\n$Card_Category\n\n    Blue     Gold Platinum   Silver \n    9436      116       20      555 \n\n\n\n\nExercise 4.3 Overwrite the variables Income_Category and Education_Level into ordered factors. When setting the levels for each group, set \"Unknown\" as the lowest level. Use this cleaned data set for the remaining exercises.\n\n\nExercise 4.4 Group the data set by income category and find out each group’s mean and median credit limit.\n\n\nExercise 4.5 Which income group has the highest mean credit limit?\n\n\nExercise 4.6 Use the following code snippet to modify the data set by incorporating it into the mutate function. The snippet converts all \"Unknown\" values contained in character or factor columns into NA values, which are easier to handle.\n\nacross(where(~ is.character(.) | is.factor(.)),\n       ~na_if(.,\"Unknown\"))\n\n\n\nExercise 4.7 Apply the na.omit() function to the data set to remove all samples in the data set that contain NA values. How many samples have been removed in total?\n\nSometimes, we only want to infer results for specific subgroups. The Blue Credit Card is the most common type of credit card. Gaining insights for this particular group allows us to retrieve information that might be useful in later analyses.\n\nExercise 4.8 Find out how many customers have a Blue credit card.\n\n\nExercise 4.9 Create a new data set credit_info_blue containing all customers that hold a Blue credit card.\n\n\nExercise 4.10 Find the number of female customers holding the Blue Card who are, at most, 40 years old and have a credit limit above 10,000 USD.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#section-13",
    "href": "prerequisites.html#section-13",
    "title": "1  A short revision of important R functions",
    "section": "4.3 ",
    "text": "4.3 \nInstead of visualizing the Gender as in the plot above, we now want to analyze the continuous variable Credit_Limit. Therefore, instead of using a histogram, use the geom_density function that plots an estimate of the underlying probability density.\n\n4.3.1 \nThe histograms and density plots only provide limited insight to the demographics and customer status as it is relatively difficult to figure out the proportions of each group. To takes this one step further, consider the following histogram, which shows the Education_Level within every bin.\n\nggplot(data = credit_info, aes(Customer_Age, fill = Education_Level))+\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nAdditionally to using the geom_histogram function, we can use the facet_wrap function, which generates a subplot for each group. Apply the facet_wrap function to create a subplot for each education level.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A short revision of important R functions</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#loss-functions",
    "href": "prerequisites.html#loss-functions",
    "title": "1  Important R concepts",
    "section": "4.3 Loss functions",
    "text": "4.3 Loss functions\nIn future exercises, different loss functions will be deployed to measure how far some regression results deviate from actual values. This exercise, therefore, briefly discusses the advantages and disadvantages of some loss functions and introduces them in R.\nData-wise, we will consider the credit_info dataset and a simple linear model that is used to predict each customer’s credit limit.\nThe following Code snippet reads the unmodified data, removes the features Total_Revolving_Bal and Avg_Open_To_Buy and trains a linear model with target variable Credit_Limit on all the remaining features. It’s important to note that the model is intentionally kept simple for demonstrative purposes, making it easier for you to grasp and apply the concepts.\nCopy the snippet into your own notebook and run it. Hint: You might have to change the path in the read.csv function to your specified data path (Exercise 4.1) and install the libraries that are attached.\n\nlibrary(tidymodels)\nlibrary(yardstick)\n\ncredit_info &lt;- read.csv(\"data/BankChurners.csv\")\n\nmodel_linear_data &lt;- credit_info %&gt;%\n  select(-c(Total_Revolving_Bal,Avg_Open_To_Buy))\n\nmodel_linear_res &lt;- linear_reg() %&gt;%\n  fit(Credit_Limit ~., data = model_linear_data) %&gt;%\n  augment(model_linear_data)\n\nThe object model_linear_res now contains our model’s original data set and predictions. Do not worry if you do not understand every line in the snippet above. We will consider training models in future exercises more thoroughly.\n\n4.3.1 MAE Loss\nThe first loss function we explore is the Mean Absolute Error (MAE) loss defined as\n\\[\\begin{equation*}\n  \\mathrm{MAE} := \\mathrm{MAE}(y,\\hat{y}):=\\frac{1}{n}\\sum_{i=1}^n |y_i-\\hat{y_i}|,\n\\end{equation*}\\]\nwhere \\(y=(y_1,...,y_n)\\) are target values and \\(\\hat{y}=(\\hat{y_1},...,\\hat{y_n})\\) are estimates of the target values.\n\nExercise 4.16 Briefly explain how the MAE loss can be interpreted regarding the target features scale.\n\n\nExercise 4.17 The mae loss is a function in the {yardstick} library. If not already done, install the {yardstick} library and read the help function of the mae function. Then, apply it tot the model_linear_res data set and interpret the result.\n\n\n\n4.3.2 (R)MSE\nAnother widely used loss function is the (Root)MeanSquareError. It is defined as\n\\[\\begin{align*}\n  \\mathrm{RMSE} &:= \\mathrm{RMSE}(y,\\hat{y}) := \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2}\\\\\n  \\mathrm{MSE} &:= \\mathrm{MSE}(y,\\hat{y}) := \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2\n\\end{align*}\\]\n\nExercise 4.18 Repeat the exercise Exercise 4.16 and Exercise 4.17 for the RMSE and MSE.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#section-15",
    "href": "prerequisites.html#section-15",
    "title": "1  A short revision of important R functions",
    "section": "4.4 ",
    "text": "4.4 \nThe first loss function we consider is the MAE loss defined as\n\\[\\begin{equation*}\n  \\mathrm{MAE} := \\mathrm{MAE}(y,\\hat{y}):=\\frac{1}{n}\\sum_{i=1}^n |y_i-\\hat{y_i}|,\n\\end{equation*}\\]\nwhere \\(y=(y_1,...,y_n)\\) are target values and \\(\\hat{y}=(\\hat{y_1},...,\\hat{y_n})\\) are estimates of the target values\nTest the loss functions with the following inputs. You can check your answers below.\n\ntargets &lt;- c(120, 97, 4, 25, 15)\npredictions &lt;- c(111, 92, 9, 29, 20)\n\n\n4.4.0.1 Exercise 3a ii:\nWrite a function loss_huber that returns the Huber loss for two given vectors y, yhat, and a threshold \\(\\delta\\). The Huber loss of two vectors can be calculated as follows:\n\\[\n\\mathrm{loss_{huber}} =\\frac{1}{\\text{length(y)}} \\sum_{i=1}^{\\mathrm{length(y)}} \\mathcal{L}(y_i,\\hat y_i)\n\\]\nwhere\n\\[\n\\mathcal{L}(y_i,\\hat y_i) = \\begin{cases} \\frac{1}{2}(y_i-\\hat y_i)^2, \\quad&\\text{if } |y_i-\\hat y_i| &lt;\\delta\\\\ \\delta (|y_i-\\hat y_i|-\\frac{1}{2}\\delta),\\quad &\\text{else}.   \\end{cases}\n\\]\n\nloss_huber &lt;-function(y, yhat, d){\n  res &lt;- 0\n  for (i in seq_along(y)) {\n    if(abs(y[i] - yhat[i]) &lt; d){\n      res = res + 0.5 * (y[i] - yhat[i])^2\n    } else{\n      res = res + d * (abs(y[i] - yhat[i]) - 0.5 * d)\n    }\n  }\n    return(res/length(y))\n}\n\n\nloss_huber_alternative &lt;- function(y, yhat, d) {\n  residual &lt;- (y - yhat)\n\n  L &lt;- if_else(\n    abs(residual) &lt; d, \n    residual^2 / 2, \n    d * (abs(residual) - d / 2)\n  )\n  \n  mean(L)\n}\n\nYou can test your function with the vectors targets and predictions with d=1.\n\nloss_huber_alternative(targets, predictions, 1)\n\n[1] 5.1\n\n\n\n\n4.4.1 Exercise 3b:\nThe goal of this exercise is to gain a deeper understanding of the different loss types calculated in the previous exercises. Consider the following scenarios and argue which loss function should be used.\n\n4.4.1.1 Exercise 3b i:\nDecide between MSE and RMSE:\n\nYou are developing a predictive model for housing prices. Given various features of a house like size (in \\(m^2\\)) , number of floors, and the name of the neighborhood. You want to estimate its market value in EUR.\n\n\n#RMSE should be used to measure the average prediction error. This is the case\n#since by using RMSE the error is in the same units as the target variable (e.g.,\n#EUR). This helps in quantifying the magnitude of prediction errors accurately.\n\n\n\n4.4.1.2 Exercise 3b ii:\nDecide between MAE and MSE:\n\nImagine you are working on a weather prediction model, where you aim to forecast daily temperatures for a location with strong and sudden changes in weather. The data contains average wind velocity, air pressure, humidity, and many more. You also have historical temperature data for training and the overall goal is to minimize prediction errors.\n\n\n#MAE should be used to measure the predictive error since it is more robust with\n#respect to outliers. Weather data can sometimes contain outliers due to extreme\n#weather events, sensor malfunctions, or other factors.\n\n\n\n4.4.1.3 Exercise 3b iii:\nDecide between MAE, MSE, and Huber loss\n\nYou are building a machine-learning model for autonomous vehicle control. The vehicles have to navigate through complex environments with all sorts of different obstacles varying in size and shape. The goal is to ensure the vehicle makes safe decisions based on some numerical value calculated based on the obstacles observed.\n\n\n# Huber Loss is the ideal choice because it strikes a balance between the\n#robustness of MAE and the sensitivity to larger errors of MSE. Especially for\n#varying values of delta a well balanced loss function can be achieved.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A short revision of important R functions</span>"
    ]
  },
  {
    "objectID": "linear_models.html",
    "href": "linear_models.html",
    "title": "2  Linear Models",
    "section": "",
    "text": "3 Exercises\nlibrary(\"tidyverse\")\nlibrary(\"tidymodels\")\nlibrary(\"ggtext\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "linear_models.html#exercise-1-bias-variance-trade-off",
    "href": "linear_models.html#exercise-1-bias-variance-trade-off",
    "title": "2  Linear Models",
    "section": "3.1 Exercise 1: Bias-Variance Trade-Off",
    "text": "3.1 Exercise 1: Bias-Variance Trade-Off\nIn the lecture, we have already briefly discussed the issue of overfitting by considering a simple example. The goal of this exercise is to examine a real-world dataset and see how we can recreate the phenomenon of under and overfitting with a simple linear model.\n\n3.1.1 Exercise 1a: Data exploration and manipulation\n\ndata_aux_filtered &lt;- read_csv(\"data/rent_aux.csv\")\n\nThe rent_aux dataset is a preprocessed subset of the Apartment rental offers in Germany dataset. It contains 239 unique rental listings for flats in Augsburg. The data was sourced at three different dates in 2018 and 2019 and contains 28 different variables.\nInstead of focusing on a comprehensive data cleaning and manipulation process, we will simply use the two variables livingSpace measuring the area of living in \\(m^2\\) of a listing and totalRent in EUR, representing the total amount of rent for a month (base rent + utilities).\n\n3.1.1.1 Exercise 1a i:\nVisualize the relationship between the two variables livingSpace and totalRent. The output could look like the plot below.\n\ndata_aux_filtered %&gt;% \n  ggplot(aes(x = livingSpace, y = totalRent)) +\n  geom_point() +\n  labs(\n    x = \"living space\",\n    y = \"total rent\"\n  )\n\n\n\n\n\n\n\n\n\n\n3.1.1.2 Exercise 1a ii:\nWithout conducting a thorough outlier analysis we decide to remove every listing that either costs more than \\(2500\\) EUR or is bigger than \\(200\\: m^2\\). Use the filter function to remove those outliers.\n\ndata_aux_filtered &lt;- data_aux_filtered %&gt;%\n  filter(totalRent &lt;= 2500, livingSpace &lt;= 200)\n\n\n\n\n3.1.2 Exercise 1b: Training a simple model\nIn this exercise, we want to fit an overly simple model to demonstrate it’s low performance on both, a training and test set.\n\n3.1.2.1 Exercise 1b i:\nIn order to ensure consistent results, we set a seed. Furthermore, the vector sampled that is filled \\(70\\%\\) with TRUE values and \\(30 \\%\\) FALSE values is given. Its length matches the length of the data_aux_filtered dataset. The goal is to create a simple training and testing set by randomly splitting the data_aux_filtered dataset into two subsets.\n\nset.seed(2)\nsampled &lt;- sample(c(TRUE, FALSE),\n                 nrow(data_aux_filtered),\n                 replace=TRUE, prob=c(0.7, 0.3)\n                 )\nsampled\n\n  [1]  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n [13] FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n [37] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n [49]  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n [61] FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE\n[109]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE\n[121]  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[133]  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[145]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\n[157] FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n[169]  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n[181] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n[193] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE\n[205]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n[217]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n[229] FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n\n\n\n3.1.2.1.1 Exercise 1b i a:\nUse the vector sampled to create a tibble rent_train consisting of all samples in data_aux_filtered that match the position of TRUE values in the vector sample. Subsequently, create a list rent_test that consists of all samples in data_aux_filtered that match the position of FALSE values in the vector sampled.\n\nrent_train &lt;- data_aux_filtered[sampled, ]\nrent_test &lt;- data_aux_filtered[!sampled, ]\n\n\n\n3.1.2.1.2 Exercise 1b i b:\nUsing the if_else function and %in% operator, add a column called label to data_aux_filtered that consists of the strings \"train\" and \"test\" at the respective positions where a sample is either in rent_train or rent_test. You can use the variable scoutId to match the samples in rent_train and rent_test with samples in data_aux_filtered.\n\ndata_aux_filtered &lt;- data_aux_filtered %&gt;% mutate(\n  label = if_else(\n    scoutId %in% rent_train$scoutId, \n    \"train\",\n    \"test\"\n  )\n)\n\n\n\n\n3.1.2.2 Exercise 1b ii:\nCreate a model simple_model using the lm function and fit the variable totalRent on totalRent. The underlying data should be the previously created list rent_train. By following this kind of weird procedure, we create a model that simply returns the mean of totalRent as a predictor. There will most likely be a warning that we can (at least this time) safely ignore. Also, watch out that the output of lm() will show a rounded value as an intercept but in further calculations, the output simple_model contains the exact value.\n\nsimple_model &lt;- lm(\n  formula = totalRent ~ totalRent, \n  data = rent_train\n)\n\n\n\n3.1.2.3 Exercise 1b iii:\nCalculate the RMSE loss of the training set rent_train and the testing set rent_test by using the loss_rmse function below. The fitted values on the training set can be accessed by using simple_model$fitted.values. In order to predict the values of the rent_train set, familiarize yourself with the predict function. For reference, you can find some plausible values below.\n\nloss_rmse &lt;- function(y, yhat){\n  sqrt(mean((y - yhat)^2))\n}\n\n\nrmse_simple_model_train &lt;- loss_rmse(\n  rent_train$totalRent,\n  simple_model$fitted.values\n)\nrmse_simple_model_test &lt;- loss_rmse(\n  rent_test$totalRent,\n  predict(simple_model, rent_test)\n)\n\nglue::glue(\n\"Training error: {round(rmse_simple_model_train,2)}\\n\nTesting error: {round(rmse_simple_model_test,2)}\n\")\n\nTraining error: 419.43\n\nTesting error: 474.93\n\n\n\n\n3.1.2.4 Exercise 1b iv:\nComplete the following code snippet such that the plot looks like the one displayed below. What does the graph below show?\n\ndata_aux_filtered %&gt;% \n  ggplot(\n    aes(\n    ###############\n    ## Fill Here ##\n    ###############\n    )\n  ) +\n  geom_point(size = 3, alpha = 0.5)+\n  geom_hline(\n    aes(yintercept = simple_model$coefficients),\n    linewidth = 2\n  ) +\n  labs(\n    x = \"Living space (in sq.m)\",\n    y = \"Total rent (in Euro)\",\n    color = 'Data Set',\n    title = \"Our Model's fitted line doesn't fit particularly well\"\n  ) +\n  theme_minimal(base_size = 16)+\n  scale_color_brewer(palette = 'Set2')\n\n\ndata_aux_filtered %&gt;% \n  ggplot(\n    aes(\n      x=livingSpace,\n      y = totalRent,\n      colour = label\n    )\n  ) +\n  geom_point(size = 3, alpha = 0.5)+\n  geom_hline(\n    aes(yintercept = simple_model$coefficients),\n    linewidth = 2\n  ) +\n  labs(\n    x = \"Living space (in sq.m)\",\n    y = \"Total rent (in Euro)\",\n    color = 'Data Set',\n    title = \"Our Models's fitted line doesn't fit particularly well\"\n  ) +\n  theme_minimal(base_size = 16)+\n  scale_color_brewer(palette = 'Set2')\n\n\n\n\n\n\n\n\n\n\n\n3.1.3 Exercise 1c: Overfitting a model\nNow that we have seen how robust (low variance) a simple model is, we will consider a more complex linear model in this exercise.\n\n3.1.3.1 Exercise 1c i:\nCreate a model named model on the rent_train dataset, again, with totalRent as the response variable, but instead of simply using the livingSpace variable as a single predictor, use the poly(livingSpace,20) object instead.\n\nmodel &lt;- lm(\n  formula = totalRent ~ poly(livingSpace, 20), \n  data = rent_train\n)\n\n\n\n3.1.3.2 Exercise 1c ii:\nCalculate the RMSE. For reference, you can find some plausible values below.\n\nrmse_model_train&lt;- loss_rmse(\n  rent_train$totalRent,\n  model$fitted.values\n)\nrmse_model_test &lt;- loss_rmse(\n  rent_test$totalRent,\n  predict(model, rent_test)\n)\nglue::glue(\n\"training error: {round(rmse_model_train,2)}\\n\ntesting error: {round(rmse_model_test,2)}\"\n)\n\ntraining error: 125.43\n\ntesting error: 839.96\n\n\n\n\n3.1.3.3 Exercise 1c iii:\nExplain in your own words the phenomenon observed in the following plot.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "linear_models.html#intermezzo-good-practices-for-applied-machine-learning",
    "href": "linear_models.html#intermezzo-good-practices-for-applied-machine-learning",
    "title": "2  Linear Models",
    "section": "3.2 Intermezzo: Good practices for applied Machine Learning",
    "text": "3.2 Intermezzo: Good practices for applied Machine Learning\nIn previous courses, we mainly focused on how to fit a certain model to a given dataset. However, this process could be described as model specification, rather than model development. So, what’s the difference between specifying a model and actually building a model?\n\n3.2.1 Developing a model (What we have done so far!):\n\nThe given dataset has been cleaned, transformed, and manipulated using a multitude of different packages and libraries.\nResampling methods have been applied but training the model on each subset or newly generated dataset is usually performed by using a loop or similar methods.\nSimilar to applying resampling methods, hyperparameter tuning is applied by using a loop or similar methods.\n\nIn summary, we have only basically specified the model we want to train and used a rather arbitrary and inconsistent approach for everything else.\nOne of the biggest issues we face, however, is when switching the model. The approach we have been using so far emphasizes working with one selected model that we wish to keep using after data preprocessing.\n\n\n3.2.2 Developing a model (What we want to do moving forward!):\nThe main difference between the old approach and the new approach comes down to leveraging the advantages of the {tidyverse} and {tidymodels} frameworks. These frameworks allow for consistently preprocessing the data, setting model specifications, and performing steps like resampling and hyperparameter tuning all at once.\nAnother huge advantage is, that by following this procedure we can also swiftly switch between different ML models. For example, applying a random forest algorithm and switching to a neural network approach for the same data works is only a matter of changing a few lines of code as we will see in later exercises.\nSo where is the catch? At first, the process might seem a bit difficult or even “overkill” for the models we use. However, as the lecture progresses, our models will also (at least sometimes) become increasingly sophisticated. So we want to get used to this new process as early as possible in order to get used to it.\nThe biggest takeaways are:\n\nConsistency: Independent of what the dataset or desired model looks like, we can (almost) always use the same procedure when building a model.\nEffectiveness: Once we get used to this new approach, we can develop our models a lot more effectively than before.\nSafety: Developing an ML model has many pitfalls and potholes on the way and by design, {tidymodels} helps us to avoid those.\n\nWe will introduce and explore some of the aforementioned concepts in the next exercise and dive deeper in later sessions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "linear_models.html#exercise-2-cross-validation",
    "href": "linear_models.html#exercise-2-cross-validation",
    "title": "2  Linear Models",
    "section": "3.3 Exercise 2: Cross-Validation",
    "text": "3.3 Exercise 2: Cross-Validation\nOur first step down the {tidymodels} rabbit hole has already been broached in the lecture. By splitting the data into a training set and test set we can effectively evaluate our trained model on previously unseen data. However, how can we ensure that splitting the data once into a training set and test set yields a good model? A simple answer is: We can’t.\nThat is where cross-validation comes into play.\n\n3.3.1 Exercise 2a:\nSimilar to the lecture slides we want to build a simple model for predicting the response variable totalRent in Augsburg by using the single predictor livingSpace.\n\n3.3.1.1 Exercise 2a: i\nFamiliarize yourself with the initial_split function and create a split named split on the dataset data_aux_filtered.\n\nsplit &lt;- initial_split(data_aux_filtered)\n\n\n\n3.3.1.2 Exercise 2a ii:\nUtilizing the newly created split, create the objects data_train and data_test by using the training and testing functions respectively. Note, that we are basically doing the same thing as in Exercise 1b, but instead of using the base R function sample, we’re now working with the {rsample} package that is part of the {tidyverse} framework.\n\ndata_train &lt;- training(split)\ndata_test &lt;- testing(split)\n\n\n\n3.3.1.3 Exercise 2a iii:\nFamiliarize yourself with the functionality of the vfold_cv function and create an instance named folds with the dataset data_train and the parameter v = 10.\n\nfolds &lt;- vfold_cv(data_train, v = 10)\n\nThe function vfold_cv randomly splits the data into v roughly equally sized subsets. One resample then consists of v-1 of those subsets. The picture below depicts how that process works for v = 5.\n\n\n\n5-fold Cross Validation\n\n\n\n\n3.3.1.4 Exercise 2a iv:\nEach split in the fold can be accessed as a variable by using the $ operator. In order to access split i, we have to use the notation folds$splits[[i]]. By using the analysis function we can then access the v-1 subsets of the original data set that are returned as a data frame. The assessment function returns the hold-out subset of the split.\nFamiliarize yourself with the functionality of the analysis and assessment function. For the first split, create a scatter plot for both of the subsets subsets. An example of what such plots could look like can be found below.\n\nfolds$splits[[1]] %&gt;% \n  analysis %&gt;%\n  ggplot(aes(x = livingSpace, y = totalRent)) +\n  geom_point() +\n  labs(\n    x = \"living space\",\n    y = \"total rent\",\n    title = \"Training sample in split 1\"\n  )\n\n\n\n\n\n\n\n\n\nfolds$splits[[1]] %&gt;% \n  assessment %&gt;%\n  ggplot(aes(x = livingSpace, y = totalRent)) +\n  geom_point()+\n  labs(\n    x = \"living space\",\n    y = \"total rent\",\n    title = \"Testing sample in split 1\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 Exercise 2b:\nNow that we have set up our data set for cross-validation, we can continue developing our model. The next step is to assemble the model that we wish to use. In this case, we will choose a simple linear model once again.\n\n3.3.2.1 Exercise 2b i:\nInstead of using a model created with the lm function of the {stats} package as in Exercise 1b ii, we will continue working with the {tidymodels} framework. The {parsnip} package offers a standardized interface for fitting models as well as the return values. A linear model can be created with the linear_reg function. Create a linear model called lm_mod using the linear_reg function. Note, that you do not have to pass any arguments to the function linear_reg at this point.\n\nlm_mod &lt;- linear_reg()\n\nIf calling the lm_mod object returns the same output as the one you can find below, you have solved this exercise correctly!\n\nlm_mod \n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n3.3.2.2 Exercise 2b ii:\nAs a next step, we want to create a so-called recipe. According to the R Documentation, a recipe is a description of the steps to be applied to a data set in order to prepare it for data analysis. So a recipe can be thought of figuratively as a sequential procedure to cook our model!\nIn this exercise, our recipe is rather simple. Create a recipe called lm_recipe by using the recipe function. As arguments for the recipe function, you can pass the following formula. The second argument concerns the data that is given by the data_train dataset in our case.\n\nformula &lt;- totalRent ~ livingSpace\n\nIf calling the lm_recipe object returns the same output as the one you can find below, you have solved this exercise correctly!\n\nlm_recipe &lt;- recipe(formula, data = data_train)\n\nlm_recipe %&gt;% prep %&gt;% bake(data_train)\n\n# A tibble: 179 × 2\n   livingSpace totalRent\n         &lt;dbl&gt;     &lt;dbl&gt;\n 1        55         710\n 2        40         620\n 3        53         666\n 4        87        1282\n 5        75         950\n 6       121        1470\n 7        60.8      1060\n 8       105        1285\n 9        58.1       974\n10        59         825\n# ℹ 169 more rows\n\n\n\n\n3.3.2.3 Exercise 2b iii:\nAfter creating our recipe, we create a so-called workflow. The workflow object aggregates information required to fit and predict from a model. In this case, our model lm_mod and the recipe lm_recipe. This whole procedure might seem quite complicated by now. However, later on, it will prove much more efficient and consistent this way.\nGiven the following code chunk, complete the workflow by adding the model lm_mod using the add_model function and the lm_recipe using the function add_recipe.\n\nlm_wf &lt;- \n  workflow() %&gt;%\n  ###################\n  ## Continue here ##\n  ###################\n\nAs above, you can check whether your workflow is correct by calling it and comparing it to the following output.\n\nlm_wf &lt;- \n  workflow() %&gt;%\n  add_model(lm_mod) %&gt;%\n  add_recipe(lm_recipe)\n\nlm_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n3.3.2.4 Exercise 2b iii:\nAs a next step, we will specify the metrics we want to use for evaluating our model. The metric_set function allows us to combine multiple metric functions such as the RMSE, MAE, and Huber Loss.\nCreate an instance called multi_metric using the metric_set function. As arguments, you can pass rmse,mae, and, huber_loss.\n\nmulti_metric &lt;- metric_set(rmse, mae, huber_loss)\n\nAs above, you can check whether your metric set multi_metric is correct by calling it and comparing it to the following output.\n\nmulti_metric\n\nA metric set, consisting of:\n- `rmse()`, a numeric metric       | direction: minimize\n- `mae()`, a numeric metric        | direction: minimize\n- `huber_loss()`, a numeric metric | direction: minimize\n\n\n\n\n3.3.2.5 Exercise 2b iv:\nNow that we have fully specified our model development process we can fit the model by piping the workflow lm_wf to the fit_resamples function. As the name suggests, the fit_resamples function fits the linear model on each of the folds provided. We simply have to specify the vfold_cv object and the metric_set in order to train the model and calculate the metrics.\n\nlm_fit_rs &lt;-\n  lm_wf %&gt;% \n  fit_resamples(folds, metrics = multi_metric)\n\nFamiliarize yourself with the collect_metrics function and apply it to the lm_fit_rs object. The function returns the average for the respective metrics over all hold-out folds of all the splits (i.e. the average of all the blue cells in the illustration in Exercise 2a iii) and a few more key figures such as the standard deviation of the metric across all folds and the number of folds. A possible output can be found below.\n\nlm_fit_rs %&gt;% collect_metrics()\n\n# A tibble: 3 × 6\n  .metric    .estimator  mean     n std_err .config             \n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 huber_loss standard    130.    10    9.44 Preprocessor1_Model1\n2 mae        standard    131.    10    9.44 Preprocessor1_Model1\n3 rmse       standard    160.    10    9.62 Preprocessor1_Model1\n\n\n\n\n\n3.3.3 Exercise 2c:\nIn this last exercise, we will first look at a demonstration of how to properly train multiple models within the {tidymodels} framework. The idea is to fit multiple polynomial regressions using cross-validation. Subsequently, it is your task to extract different hold-out sample losses and visualize them properly.\n\n3.3.3.1 Exercise 2c i:\nCarefully read the following paragraph and insert the code chunks into your own worksheet in order to solve the subsequent exercises.\n\nWe first set the degree values for the polynomials we wish to fit. Similar to Exercise 2 b ii, we then create a recipe, by setting a formula and the data we want to train on. The recipe rec_poly is then passed to the step_poly function that basically creates new columns that are basis expansions of the livinSpace variable. Note, that as for the degree of the step_poly function, we did not pass degree_values rather than the {hardhat} function tune(). tune() is a placeholder for the grid degree_values. Why this is important will be elaborated in the explanation of the next cell (🐻 with me). The raw = TRUE parameter is passed because, in this specific example, we want to use the original (or in this case raw) values of the variable livingSpace.\n\ndegree_values &lt;- 2:10\n\nrec_poly &lt;- lm_recipe %&gt;% \n    step_poly(\n      livingSpace, \n      degree = tune(), \n      options = list(raw = TRUE)\n    )\n\nSimilarly to Exercise 2b iii, we then create a workflow where we pass the model lm_mod we created in Exercise 2b i.\nThe real magic then happens when passing the workflow poly_wf to the tune_grid function. The tune_grid function basically iterates along the degree_values, trains the model on all the folds sequentially, and even saves the previously specified metrics in the multi_metric vector to the tune_poly object.\n\npoly_wf &lt;- workflow() %&gt;%\n    add_model(lm_mod) %&gt;%\n    add_recipe(rec_poly)\n\ntune_poly &lt;- poly_wf %&gt;%\n  tune_grid(\n    grid = tibble(degree = degree_values),\n    metrics = multi_metric,\n    resamples = folds\n  )\n\nSimilarly to Exercise 2b iv, we can collect the metrics for the average of all hold-out samples by using the collect_metrics function. However, as we have trained polynomials up to degree n = 10, the function returns the metrics for all polynomials with degree n = 2,…,10.\n\n\n\n3.3.3.2 Exercise 2c ii:\nGiven the tune_poly tibble from the paragraph above, we now want to evaluate the metrics graphically. To do so, create a tibble metrics_cv by applying the collect_metrics() to the tune_poly tibble. The result should look similar to the output below.\n\nmetrics_cv &lt;- tune_poly %&gt;%\n  collect_metrics()\n\n\nhead(metrics_cv)\n\n# A tibble: 6 × 7\n  degree .metric    .estimator  mean     n std_err .config             \n   &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1      2 huber_loss standard    124.    10    9.69 Preprocessor1_Model1\n2      2 mae        standard    124.    10    9.69 Preprocessor1_Model1\n3      2 rmse       standard    158.    10   11.1  Preprocessor1_Model1\n4      3 huber_loss standard    124.    10    9.55 Preprocessor2_Model1\n5      3 mae        standard    125.    10    9.55 Preprocessor2_Model1\n6      3 rmse       standard    157.    10   11.0  Preprocessor2_Model1\n\n\n\n\n3.3.3.3 Exercise 2c iii:\nUsing the metrics_cv tibble, create a line plot that displays different mean losses for each polynomial model. The plot should look similar to the one shown below.\n\nmetrics_cv %&gt;%\n  ggplot(aes(x = degree, y = mean, color = .metric)) +\n  geom_line(linewidth = 1) +\n  ylab(\"Mean loss of hold-out sample\") +\n  xlab(\"Degree of fitted polonomial\") +\n  labs(\n    title = 'Mean Metrics across Folds',\n    col = 'Metric',\n  )",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "03_regularization.html",
    "href": "03_regularization.html",
    "title": "3  Regularization",
    "section": "",
    "text": "4 Exercises\nlibrary(\"tidyverse\")\nlibrary(\"tidymodels\")\nlibrary(\"glmnet\")\nWarning: package 'ggtext' was built under R version 4.4.1",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "03_regularization.html#exercise-1-loss-functions-in-the-context-of-ols-ridge-and-lasso-regression",
    "href": "03_regularization.html#exercise-1-loss-functions-in-the-context-of-ols-ridge-and-lasso-regression",
    "title": "3  Regularization",
    "section": "4.1 Exercise 1: Loss functions in the context of OLS, ridge, and lasso regression",
    "text": "4.1 Exercise 1: Loss functions in the context of OLS, ridge, and lasso regression\nIn Statistics I/II, we learned that OLS is the cornerstone of linear regression analysis. It allows us to explore and quantify the relationship between the response variable and the regressors in a relatively simple but meaningful way. We can extend the idea of a simple linear regression by adding a penalty term to the loss function we want to minimize. This process is called regularization and has been introduced in the lecture in terms of ridge and lasso regression.\nThe goal of this initial exercise is to review some theoretical aspects of OLS, ridge, and lasso regression.\n\n4.1.1 Exercise 1a : OLS\nConsider a simple linear model, with a quantitative response variable \\(Y\\) and a single predictor \\(X\\). The simple linear model then assumes (among other things) that there is approximately a linear relationship between \\(Y\\) and \\(X\\), i.e.,\n\\[ Y \\approx \\beta_0 + \\beta_1 X. \\]\nwith unknown coefficients \\(\\beta_0,\\beta_1\\). In order to obtain the best estimate \\(\\beta_0\\) and \\(\\beta_1\\) for a given sample we can minimize the MSE\n\\[ \\begin{equation} \\min_{\\beta_0,\\beta_1} MSE = \\frac{1}{N}\\sum_{n=1}^{N} (y_n - (\\beta_0 + \\beta_1 x_n))^2 \\end{equation} \\]\nwhere \\(N = \\mathrm{length(Y)}\\), \\(y_1,…,y_N\\) is a realized sample of \\(Y\\), and \\(x_1,…,x_N\\) is a realized sample of \\(X\\).\nShow, that\n\\[\n\\begin{align*}\n\\hat \\beta_1 &= \\frac{\\sum_{n=1}^N (x_n - \\bar x)(y_n-\\bar y)}{\\sum_{n=1}^{N}(x_n-\\bar x)^2}\\\\\n\\hat\\beta_0 &= \\bar y - \\hat\\beta_1\\bar x.\n\\end{align*}\n\\]\nwith \\(\\bar x = \\frac{1}{N}\\sum_{n=1}^{N}x_n\\) and \\(\\bar y = \\frac{1}{N}\\sum_{n=1}^{N}y_n\\) minimizes the minimization problem above. You can assume that the critical points calculated using the partial derivatives are in fact minima and that \\(\\sum_{n=1}^{N}(x_n-\\bar x)^2\\neq 0\\).\n\n\n\n\n\n\nSolution\n\n\n\nFirst, calculate the partial derivatives of\n\\[\nL(\\beta_0,\\beta_1) = \\frac{1}{N}\\sum_{n=1}^{N} (y_n - (\\beta_0 + \\beta_1 x_n))^2\n\\tag{4.1}\\]\nwith respect to \\(\\beta_1\\) and \\(\\beta_0\\).\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\beta_0} L(\\beta_0,\\beta_1) &= \\frac{1}{N}\\sum_{n=1}^{N} -2(y_n-(\\beta_0 + \\beta_1x_n))\\\\\n&= -2\\bar y + 2\\beta_0 +2\\beta_1 \\bar x \\\\\n&\\overset{!}{=}0 \\\\\n\\frac{\\partial}{\\partial \\beta_1} L(\\beta_0,\\beta_1) &= \\frac{1}{N}\\sum_{n=1}^{N} -x_n(2(y_n-(\\beta_0+\\beta_1x_n))) \\\\\n&= -2\\overline{xy} + 2\\frac{1}{N}\\sum_{n=1}^{N} x_n\\beta_0+\\beta_1x_n^2\\\\\n&= -2\\overline{xy} + 2\\beta_0 \\bar x + 2\\beta_1 \\overline{x^2}\\\\\n&\\overset{!}{=}0\n\\end{align*}\n\\]\nSolving the first term for \\(\\beta_0\\) yields\n\\[\n\\beta_0 = \\bar y - \\beta_1\\bar x.\n\\]\nIn order to solve the second term for \\(\\beta_1\\) we can utilize this newly acquired representation of \\(\\beta_0\\):\n\\[\n\\begin{align*}\n-2\\overline{xy} + 2\\beta_0 \\bar x + 2\\beta_1 \\overline{x^2} = -2\\overline{xy} + 2(\\bar y - \\beta_1\\bar x) \\bar x + 2\\beta_1 \\overline{x^2} \\overset{!}{=}0.\n\\end{align*}\n\\]\nThen,\n\\[\n\\begin{align*}\n-2\\overline{xy} + 2(\\bar y - \\beta_1\\bar x) \\bar x + 2\\beta_1 \\overline{x^2}\n&= -2\\overline{xy} + 2\\bar y \\bar x- 2\\beta_1\\bar x^2+ 2\\beta_1 \\overline{x^2}\\\\\n&\\overset{!}{=} 0.\n\\end{align*}\n\\]\nThis can easily be solved for \\(\\beta_1\\), which yields\n\\[\n\\beta_1 = \\frac{\\sum_{n=1}^N (x_n - \\bar x)(y_n-\\bar y)}{\\sum_{n=1}^{N}(x_n-\\bar x)^2}.\n\\]\nNote, that\n\\[\n\\begin{align*}\n\\sum_{n=1}^N (x_n - \\bar x)(y_n-\\bar y) &= \\sum_{n=1}^N x_n y_n -\\bar xy_n-x_n\\bar y + \\bar x \\bar y\\\\\n&= N(\\overline{xy} - \\bar x\\bar y - \\bar x \\bar y + \\bar x \\bar y)\\\\\n&= N(\\overline{xy}-\\bar x\\bar y)\n\\end{align*}\n\\]\nand\n\\[\n\\begin{align*}\n\\sum_{n=1}^{N}(x_n-\\bar x)^2 &= \\sum_{n=1}^{N}x_n^2 - 2x_n\\bar x + \\bar x^2\\\\\n&= N(\\overline{x^2} - 2\\bar x ^2 + \\bar x^2)\\\\\n&= N(\\overline{x^2} - \\bar x ^2)\n\\end{align*}\n\\]\n\n\n\n\n4.1.2 Exercise 1b: Ridge and lasso regression\nConsider a linear model, where there is a quantitative response variable \\(Y\\) and a predictor \\(X = (1, X_1,...,X_k)\\), i.e. there are \\(k\\in\\mathbb{N}\\) different features. The linear model then assumes (among other things) that there is approximately a linear relationship between \\(Y\\) and \\(X\\), i.e.,\n\\[ Y \\approx \\beta_0 + \\beta_1 X_1 + ... +\\beta_k X_k. \\]\nwith unknown coefficients \\(\\beta_0,...,\\beta_k\\).\nIn the lecture, we have already seen that the loss function for ridge and lasso regression is given by\n\\[\n\\mathcal{L}_{\\mathrm{ridge}}(\\beta,\\lambda) = \\sum_{n=1}^{N} (y_n - \\hat{y}_n)^2 + \\lambda \\sum_{i=0}^{k} \\beta_j^2\n\\]\nand\n\\[\n\\mathcal{L}_{\\mathrm{lasso}}(\\beta,\\lambda) = \\sum_{n=1}^{N} (y_n - \\hat{y}_n)^2 + \\lambda \\sum_{i=0}^{k} |\\ \\beta_j\\ |,\n\\]\nwhere \\(\\lambda \\in [0,\\infty)\\) .\n\n4.1.2.1 Exercise 1b i:\nExplain the following statement from the lecture:\n“Ridge regression reduces the variance, but introduces bias.”\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nBias-Variance Trade-Off\n\n\nThe key point of this quote addresses the bias-variance trade-off.\nFrom the lecture, we know that\n\\[\n\\hat \\beta_{\\mathrm{ridge}}(\\lambda) = \\frac{\\hat\\beta_{\\mathrm{OLS}}}{1+\\lambda}\n\\]\nif the features are standardized and orthogonal.\n\nBias:\nBy growing the parameter \\(\\lambda\\), the parameter \\(\\hat\\beta_{\\mathrm{OLS}}\\) shrinks. In other words, the regularization term encourages the model to have smaller coefficient values, which means it may not fit the training data as closely as an unregularized model. This means that systematic errors are introduced to the model’s predictions.\nVariance:\nBy growing the parameter \\(\\lambda\\), we reduce variance by shrinking the coefficients’ values, which discourages them from taking very high values. This effectively constrains the model’s complexity and makes it less prone to overfitting.\n\n\n\n\n\n4.1.2.2 Exercise 1b ii:\nConsider the following statements and decide whether ridge or lasso regression should be applied.\n\nYou are building a predictive model for stock price prediction, and you have a large number of potential predictors. Some of these predictors might be highly correlated with each other.\nYou are modeling housing prices, and you want to prevent the model from overfitting to the training data.\nYou are working on a marketing project where you have a dataset with a mix of numerical and categorical features. You need to build a regression model to predict customer lifetime value.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\nLasso regression should be used in this scenario because it can perform feature selection by driving some coefficients to zero. This is especially helpful if there are many features as it helps in dealing with correlated predictors.\nRidge regression is more suitable because it provides a smoother and more continuous shrinkage of coefficients, which reduces the risk of overfitting.\nLasso regression might be a more suitable choice as it can perform feature selection and even drive the coefficient for some categorical values to 0.\n\n\n\n\n\n4.1.2.3 Exercise 1b iii:\nCome up with a scenario where a mixed model, i.e. an elastic net might be a good choice.\n\n\n\n\n\n\nSolution:\n\n\n\nA healthcare dataset is given to predict a patient’s readmission probability with numerous correlated features. The aim is to build a model that predicts accurately, selects the most relevant features, and mitigates multicollinearity. Here, an elastic net is the preferred choice because it combines Ridge and Lasso regression, effectively handling multicollinearity while performing feature selection, making it ideal for this complex healthcare dataset.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "03_regularization.html#exercise-2-another-lesson-on-data-preparation-and-model-specification",
    "href": "03_regularization.html#exercise-2-another-lesson-on-data-preparation-and-model-specification",
    "title": "3  Regularization",
    "section": "4.2 Exercise 2: Another lesson on data preparation and model specification",
    "text": "4.2 Exercise 2: Another lesson on data preparation and model specification\nIn this exercise, we will revisit the rent dataset, but instead of looking at rent prices in Augsburg, we will now consider rent prices in Munich. We will briefly prepare the dataset and create a recipe similar to Exercise 2b ii on Sheet 02 but in a more sophisticated fashion.\n\ndata_muc &lt;- read.csv(\"data/data_muc_filtered.csv\")\n\n\n4.2.1 Exercise 2a: Data preparation\nExplain how we preprocess our data in the following code chunk.\n\ndata_muc_filtered &lt;- data_muc %&gt;%\n  select(!c(\"X\",\"serviceCharge\",\"heatingType\",\"picturecount\",\"totalRent\",\n            \"firingTypes\",\"typeOfFlat\",\"noRoomsRange\", \"petsAllowed\",\n            \"livingSpaceRange\",\"regio3\",\"heatingCosts\", \"floor\",\n            \"date\", \"pricetrend\")) %&gt;%\n  na.omit %&gt;%\n  mutate(\n    interiorQual = factor(\n      interiorQual,\n      levels = c(\"simple\", \"normal\", \"sophisticated\", \"luxury\"),\n      labels = c(\"simple\", \"normal\", \"sophisticated\", \"luxury\"),\n      ordered = TRUE\n    ),\n    condition = factor(\n      condition,\n      levels = c(\"need_of_renovation\", \"negotiable\",\"well_kept\",\"refurbished\",\n                 \"first_time_use_after_refurbishment\",\n                 \"modernized\", \"fully_renovated\", \"mint_condition\",\n                 \"first_time_use\"),\n      ordered = TRUE\n    ),\n    geo_plz = factor(geo_plz)\n  ) %&gt;%\n  mutate_if(is.logical, ~ as.numeric(.)) %&gt;%\n  filter(baseRent &lt;= 4000, livingSpace &lt;= 200)\n\n\n\n\n\n\n\nSolution:\n\n\n\n\nWe first select all the columns we want to use by removing the remaining columns using select(!c( … ).\nThen, we remove all the NA values.\nNext, we mutate our data as follows:\n\nWe convert the feature interiorQual to a factor (basically Rs encoding for ordinal features), which allows us for example, to value a luxury interiorQual higher than a simple interiorQual. We set the levels and labels to the same name and pass the argument TRUE to the ordered option to indicate that we have indeed an ordinal feature.\nThe same procedure is applied to the condition of the flat.\nInstead of setting specific levels for the zip codes, we simply convert them to factors in order define nominal values. Note, that in this case we also do not set the ordered parameter, since the sheer value of the zip code in terms of their numerical value shouldn’t influence the dependent variable totalRent.\n\nIn the second to last step we convert all logical values to numeric values. Here, TRUE corresponds to 1 and FALSE to 0 respectively.\nThe last step concerns some “quick and dirty” data cleaning where we remove any flat that costs more than 4000 EUR or is bigger than 200 \\(m^2\\).\n\n\n\n\n\n4.2.2 Exercise 2b: Setting up resampling\nSimilar to exercise Exercise 2a: i-iii on Sheet 02, create\n\nan initial split object called split with the data_muc_filtered tibble,\na training set called data_train and a test set called data_test using the training and testing functions respectively, and\nan instance folds of the vfold_cv class with the parameters data = data_train and v = 10.\n\n\nset.seed(1)\n  ###################\n  ## Continue here ##\n  ###################\n\n\nset.seed(1)\nsplit &lt;- initial_split(data_muc_filtered)\ndata_train &lt;- training(split)\ndata_test &lt;- testing(split)\nfolds &lt;- vfold_cv(data_train, v = 10)\n\n\n\n4.2.3 Exercise 2c: Setting up a recipe\nOn the last exercise sheet, we created a simple recipe, only containing the formula we wanted to use on our simple linear model. This process can be extended by adding a multitude of different steps for preprocessing the underlying training data.\nFor each of the following updates and steps, explain their purpose and what they aim to achieve.\n\nrec_lm &lt;- recipe(\n    formula = baseRent ~., \n    data = data_train\n  ) %&gt;%\n  update_role(scoutId, new_role = \"ID\") %&gt;%\n  step_ordinalscore(interiorQual, condition)%&gt;%\n  step_dummy(geo_plz)%&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_predictors())\n\n\n\n\n\n\n\nSolution:\n\n\n\n\nWe first set up the recipe by specifying the formula and data used in each step. Note, that by using the expression baseRent ~. we indicate that we want to fit every variable in the data_train dataset on the dependent variable baseRent.\nThe update_role function assigns the feature scoutId to a new role called ID. By doing so, the feature scoutId is no longer used as a predictor and will no longer influence our model. We will still keep it in the loop, however in case we want to access a specific listing that is only accessible using the unique scoutId.\nWe then convert the factors interiorQual and condition to ordinal scores by using the step_ordinalscore function. The translation uses a linear scale, i.e. for the feature interiorQual the level simple corresponds to the value 0 and luxury corresponds to the value 4.\nAfterward, we create dummy variables for each zip code. Here, every zip code in the data_train is treated as a new variable. Thus, we are basically replacing the feature geo_plz with 82 new features, each representing one of the 82 zip codes available in the training data.\nThe step_zv (zero variance filter) function removes all variables that contain only a single value. If, for example, a zip code does not occur in any of the entries of data_train, the whole column will be set to zero and effectively not affect our model. Thus it is in our best interest to remove those columns.\nIn the last step, we normalize all predictors, i.e., transform them in a way that they have a standard deviation of 1 and mean zero.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "03_regularization.html#exercise-3-regularizing-a-linear-model-using-lasso-ridge-and-mixed-models",
    "href": "03_regularization.html#exercise-3-regularizing-a-linear-model-using-lasso-ridge-and-mixed-models",
    "title": "3  Regularization",
    "section": "4.3 Exercise 3: Regularizing a linear model using lasso, ridge, and mixed models",
    "text": "4.3 Exercise 3: Regularizing a linear model using lasso, ridge, and mixed models\nIn this last exercise we will make use of the previously performed data preparation by modeling a workflow and selecting the best model based on some performance metrics we will specify later.\n\n4.3.1 Exercise 3a: Setting up and evaluating Lasso Regression\nThe approach is similar to Exercise 2c on Exercise Sheet 02, where I demonstrated how to train multiple models. So if you get stuck in some of the exercises you should revisit this sheet and try to reproduce the steps this way.\nWhile this sub-exercise will seem kind of lengthy again, the other sub-exercises can be solved a lot quicker, since we can recycle many of the objects we create throughout this sub-exercise.\n\n4.3.1.1 Exercise 3a i:\nFirst, create an instance of the linear_reg class called model_lasso with parameters penalty = tune() and mixture = 1.0. By setting the penalty parameter to tune(), we specify that the penalty is a tuning parameter that we want to optimize later. By setting the mixture parameter to 1.0 we specify that the model should be a pure lasso regression (setting it to 0.0 indicates that we are using a pure ridge regression). If calling the model model_lasso results in the same output as below, you solved the exercise correctly.\n\nmodel_lasso &lt;- linear_reg(penalty = tune(), mixture = 1.0)\nmodel_lasso\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: lm \n\n\n\n\n4.3.1.2 Exercise 3a ii:\nSince we have set the penalty value to tune() we have to specify a grid in which want to check for the best value. We can pass the grid values to our model by using the set_engine function.\n\npenalty &lt;- seq(0, 15, length.out = 100)\n\nUpdate the model model_lasso by completing the following Code snippet. Fill the gap by piping model_lassoto the set_engine function where you pass the parameters engine = \"glmnet\" and path_values = penalty. If you are interested in why we specifically have to use the path_values argument, you can check out this manual.\n\nmodel_lasso &lt;- model_lasso %&gt;% \n  ###################\n  ## Continue here ##\n  ###################\n\nYou can check whether you have successfully updated the model by calling model_lasso and comparing your output to the one below. If they coincide you have solved the exercise correctly.\n\nmodel_lasso &lt;- model_lasso %&gt;% \n  set_engine(engine = \"glmnet\", path_values = penalty)\nmodel_lasso\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nEngine-Specific Arguments:\n  path_values = penalty\n\nComputational engine: glmnet \n\n\n\n\n4.3.1.3 Exercise 3a iii:\nSimilarly to Exercise 2c i on Exercise Sheet 02, create a workflow called glmnet_wflow by creating an instance of the workflow class without passing any additional arguments, piping it to the add_model function with model_lasso as an argument, and finally piping it to the add_recipe function with rec_lm as an argument.\nYou can check whether you have successfully set up the workflow by calling glmnet_wflow and comparing your output to the one below. If they coincide you have solved the exercise correctly.\n\nglmnet_wflow &lt;- \n  workflow() %&gt;% \n  add_model(model_lasso) %&gt;% \n  add_recipe(rec_lm)\n\nglmnet_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_ordinalscore()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nEngine-Specific Arguments:\n  path_values = penalty\n\nComputational engine: glmnet \n\n\n\n\n4.3.1.4 Exercise 3a iv:\nGiven the following metric set and the previously created workflow, we can now finally train our lasso model.\n\nmulti_metric &lt;- metric_set(rsq,rmse)\n\nIn order to do so, create an object called glmnet_res (res stands for resampling in that context) by assigning the glmnet_wflow object to it and piping it to the tune_grid function. Recall, that this is the exact same process as in Exercise 2c i on Sheet 02. As arguments for the tune_grid function, you have to pass tibble(penalty), multi_metric, and folds.\nYou can check whether you have successfully trained the model by calling head(glmnet_res) and comparing your output to the one below. If they coincide you have solved the exercise correctly.\n\nglmnet_res &lt;- \n  glmnet_wflow %&gt;% \n  tune_grid(\n    grid = tibble(penalty),\n    metrics = multi_metric,\n    resamples = folds\n  )\n\nglmnet_res\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits             id     .metrics           .notes          \n   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;          \n 1 &lt;split [1471/164]&gt; Fold01 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [1471/164]&gt; Fold02 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [1471/164]&gt; Fold03 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [1471/164]&gt; Fold04 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [1471/164]&gt; Fold05 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [1472/163]&gt; Fold06 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [1472/163]&gt; Fold07 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [1472/163]&gt; Fold08 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [1472/163]&gt; Fold09 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [1472/163]&gt; Fold10 &lt;tibble [200 × 5]&gt; &lt;tibble [0 × 3]&gt;\n\n\n\n\n4.3.1.5 Exercise 3a v:\nConsider the following plot that depicts the mean out-of-sample RMSE for different penalty values. The dashed lines represent the optimal penalty and the largest value of the penalty such that the mean MSE is within one standard error of the optimum.\nDecide and present an argument for which line is the optimal penalty. Furthermore, explain why we would choose the non-optimal penalty in lasso regression.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\n\n\nThe red line depicts the optimal penalty, since it intersects the minimum of the RMSE. Especially in Lasso regression an optimal penalty parameter is often smaller than we desire. The effect of a smaller penalty parameter is, that we do not eliminate as many features as we anticipated. By increasing the penalty we can effectively overcome this problem as more features are eliminated. A disadvantage however is, that we sacrefice out-of-sample performance, as the newly chosen penalty is not optimal anymore.\n\n\n\n\n4.3.1.6 Exercise 3a vi:\nGiven all the different models created with the penalty grid we specified in Exercise 3a ii, how can we select the best model with respect to a given metric? It turns out that this is rather simple! Explain each of the steps below and recreate the result for the metric rsq.\n\nglm_res_best&lt;- glmnet_res %&gt;%\n  select_by_one_std_err(metric = \"rmse\", desc(penalty))\n#optionally, the method below can be chosen as well\n#  select_best(metric = \"rmse\")\n\nbest_penalty &lt;- glm_res_best$penalty\n\n\nlast_glm_model &lt;- linear_reg(penalty = best_penalty, mixture = 1.0) %&gt;%\n  set_engine(\"glmnet\")\n\nlast_glm_wflow &lt;- glmnet_wflow %&gt;% \n  update_model(last_glm_model)\n\nlast_glm_fit &lt;- \n  last_glm_wflow %&gt;% \n  last_fit(split)\n\n\n\n\n\n\n\nSolution:\n\n\n\n\nBy using the select_best function we can directly select the best model with respect to a passed metric (in this case rmse). The return value is a combination of the tuning parameter and the corresponding model that performed best.\nWe save the optimal penalty which we later want to use when creating our final best model.\nWe then create a new model called last_glm_model with the previously selected optimal penalty.\nThis last model can then be incorporated into the already existing workflow glmnet_wflow by simply using the update_model function, where we pass the newly created last_glm_model.\nAfter updating the workflow we then have to fit our final model. The last_fit function where we pass the whole training and testing split split, fits the model on the whole training split and automatically evaluates it on the test split. Note, that before this point, the model has never observed any sample of the test set.\n\n\n\n\nglm_res_best&lt;- glmnet_res %&gt;% \n  select_by_one_std_err(metric = \"rsq\", desc(penalty))\n#  select_best(metric = \"rsq\")\n\nbest_penalty &lt;- glm_res_best$penalty\n\nlast_glm_model &lt;- linear_reg(penalty = best_penalty, mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\nlast_glm_wflow &lt;- glmnet_wflow %&gt;% \n  update_model(last_glm_model)\n\nlast_glm_fit &lt;- \n  last_glm_wflow %&gt;% \n  last_fit(split)\n\nEt voilà, we have now created our final model last_glm_fit based on the best value for the penalty with respect to the rmse or rsq metric.\n\n\n4.3.1.7 Exercise 3a vii:\nA question that surely arises is, how we can see which of the the coefficients were set to 0 by the lasso regression.\nBy piping the last_glm_fit model to the extract_fit_parsnip function and piping the result to the tidy function, we can effectively extract the parameters for our final mode. Complete the following sequence of operations to filter for all the variables set to 0. Which variables were set to 0?\n\nlast_glm_fit %&gt;%\n  extract_fit_parsnip %&gt;%\n  tidy %&gt;%\n  #################\n  ##Continue Here##\n  #################\n\n\nlast_glm_fit %&gt;%\n  extract_fit_parsnip %&gt;%\n  tidy %&gt;% \n  filter(estimate &gt; 0) %&gt;%\n  arrange(desc(estimate))\n\n# A tibble: 31 × 3\n   term           estimate penalty\n   &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)      1648.     10.5\n 2 livingSpace       553.     10.5\n 3 interiorQual      120.     10.5\n 4 geo_plz_X80538     43.9    10.5\n 5 geo_plz_X80804     40.6    10.5\n 6 newlyConst         38.4    10.5\n 7 lift               38.2    10.5\n 8 condition          34.5    10.5\n 9 hasKitchen         34.5    10.5\n10 geo_plz_X80333     33.7    10.5\n# ℹ 21 more rows\n\nlast_glm_fit %&gt;%\n  extract_fit_parsnip %&gt;%\n  tidy %&gt;% \n  filter(estimate  == 0) %&gt;%\n  select(term)\n\n# A tibble: 35 × 1\n   term          \n   &lt;chr&gt;         \n 1 balcony       \n 2 noRooms       \n 3 garden        \n 4 geo_plz_X80339\n 5 geo_plz_X80635\n 6 geo_plz_X80636\n 7 geo_plz_X80637\n 8 geo_plz_X80639\n 9 geo_plz_X80643\n10 geo_plz_X80673\n# ℹ 25 more rows\n\n\n\n\n\n4.3.2 Exercise 3b: Ridge Regression\nIn this last exercise, we want to reap the fruits of our labor by easily training a ridge regression model.\n\n4.3.2.1 Exercise 3b i:\nConsider the following code snippet which is all we need for training a new model. Explain each of the following steps which are performed to train the new model.\n\nmodel_ridge &lt;- linear_reg(penalty = tune(), mixture = 0.0) %&gt;%\n  set_engine(engine = \"glmnet\", path_values = penalty)\n\nglmnet_wflow &lt;- glmnet_wflow %&gt;% \n  update_model(model_ridge)\n\nglmnet_res &lt;- \n  glmnet_wflow %&gt;% \n  tune_grid(\n    grid = tibble(penalty),\n    metrics = multi_metric,\n    resamples = folds\n  )\n\n\n\n\n\n\n\nSolution:\n\n\n\n\nWe first define a model called model_ridge, which is a simple linear model but instead of setting the mixture parameter to 1 as we did in the lasso regression, we now set it to 0 in order to indicate that we want to fit a ridge regression model. We then set the engine and path values for our grid to the same values as we did for the lasso regression model.\nBy using the update_model function, we can pass our new model to the workflow object previously created and make sure that this model is indeed fitted in the next step.\nIn the last step, we only have to pass out updated workflow glmnet_wflow to the tune_grid function, which fits the model on all folds of the cross-validation across the penalty grid. By specifying the metrics option, we also make sure that the specified metrics are saved.\n\n\n\n\n\n4.3.2.2 Exercise 3b i:\nGiven the tibble ridge_metrics, create two plots, where you display both, the mean rmse and mean rsq of the model across all folds for different penalty values. Additionally, mark the optimal penalty value.\nAn example of what one of those plots could look like is below.\n\nridge_metrics &lt;- glmnet_res %&gt;% collect_metrics\n\n\nridge_metrics %&gt;% filter(.metric == \"rsq\") %&gt;%\n  ggplot(aes(x=penalty, y = mean)) +\n  geom_line(linewidth = 1.2, color = \"red\") +\n  geom_vline(aes(xintercept = penalty[which.max(mean)]),\n             color = \"red\",\n             linetype = \"dotted\",\n             linewidth = 1.2) +\n  theme(\n    plot.title = element_markdown(),\n    axis.title.y = element_markdown()\n  )+\n  labs(\n    title = \"Average R&lt;sup&gt;2&lt;/sup&gt; across all hold-out samples for different penalty values\",\n    subtitle = \"The dotted red line indicates where the maximum value is attained.\"\n  )+\n  scale_y_continuous(\n    name = \" mean R&lt;sup&gt;2&lt;/sup&gt;\"\n  )",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "01_prerequisites.html",
    "href": "01_prerequisites.html",
    "title": "1  Important R concepts",
    "section": "",
    "text": "1.1 Installing R, RStudio, and getting started with Quarto\nBefore starting the revision, we need to ensure that R and RStudio are installed. The installation process for both R and RStudio is straightforward and user-friendly. While R (the programming language) comes with a preinstalled graphical user interface, we will use the integrated development environment RStudio instead, as it looks more appealing and provides some features RGui lacks. It is important to note that in order to get RStudio to work, R needs to be installed first.\nAfter successfully installing R and RStudio, starting the latter should open a window that somewhat looks like the following (cf. RStudio User Guide).\nThe Source pane displays a .R file named ggplot2.R. While .R files are the standard file format for R scripts used for programming in R, we will use Quarto documents (files ending with .qmd). Quarto labels itself as the\nmeaning that a Quarto document allows\nQuarto is preinstalled in RStudio, so creating such documents is relatively simple.\nThis should be enough to get you started with Quarto Documents. For further reading, I recommend the Quarto Guide.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "01_prerequisites.html#installing-r-rstudio-and-getting-started-with-quarto",
    "href": "01_prerequisites.html#installing-r-rstudio-and-getting-started-with-quarto",
    "title": "1  Important R concepts",
    "section": "",
    "text": "R (for Windows) can be downloaded here.\nRStudio can be downloaded here.\n\n\n\n\n\n\n\n\n\nnext-generation version of R Markdown,\n\n\n\nweaving together narrative text and code to produce elegantly formatted output as documents, web pages, books, and more.\n\n\n\nclick on New File -&gt; Quarto Document... in the File tab \nOnce the New Quarto Document window opens, you can modify the title and specify the output format. For the sake of simplicity, we will use the HTML output format. However, you could also choose PDF if you have a LaTeX installation on your device. Clicking on the create button will create a new Quarto Document. \nThe source pane now displays a sample Quarto document that can be modified. You might have to install rmarkdown (cf. mark 1). The Quarto document can then be modified by clicking on the respective sections. R Code cells can be executed by clicking on the green arrow (cf. mark 2). To insert new R code cells between paragraphs, click on the Insert tab and select Executable Cell -&gt; R (cf. mark 3).\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nWhile solving the (programming) exercises, you can create a new paragraph using # for a new chapter to make your document more readable. Additionally, you can simply create a section using ##.\nWriting down some details on how you approached the programming exercises in a paragraph above or below can help you understand your approach when repeating the exercises later.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "01_prerequisites.html#working-directories-and-projects",
    "href": "01_prerequisites.html#working-directories-and-projects",
    "title": "1  Important R concepts",
    "section": "1.2 Working directories and projects",
    "text": "1.2 Working directories and projects\nAfter opening RStudio, execute the getwd() command in the console pane, which returns the current working directory. The working directory displays the directory of the R process. For example, if the return value of the getwd() command is C:\\Users\\lachlinu\\Desktop\\ML, then R can access any file in the ML directory. One way to change the working directory is using the setwd() command, which changes the current working directory. Manually changing the directory in every .qmd document might become tedious after a while, so a more practical alternative is setting up a project. RStudio projects allow the creation of an individual working directory for multiple contexts. For example, you might use R not only for solving Machine Learning exercises but also for your master’s thesis. Then, setting up two different projects will help you organize working directories and workspaces for each project individually. To set up a project for this course\n\nGo to the File tab and select New Project....\n\n\n\n\n\nChoose Existing Directory and navigate to the directory in which you want to create the project in and click the Create Project button.\n\n\n\n\n\nYou can now open the project by double clicking on the icon which should open a new RStudio window.\n\n\n\n\n\n\nOnce the project is opened, running the getwd() command in the console pane returns its path. Any file in this directory can be directly accessed without specifying the preceding path.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "01_prerequisites.html#importing-data",
    "href": "01_prerequisites.html#importing-data",
    "title": "1  Important R concepts",
    "section": "3.1 Importing data",
    "text": "3.1 Importing data\nTo manipulate data, we first need to import it. R has quite a few preinstalled data sets; however, I prefer data sets that are either more complex, related to everyday life, or just more fun to explore. This course will provide most of the data in .csv or .txt files. Before we can start manipulating data, it’s essential to import it correctly. This ensures that the data is in the right format for further analysis.\nConsider the Netflix Movies and TV Shows data set, which can be downloaded from the data science community website Kaggle or directly below.\n\nDownload Netflix Data\n\nOnce you have downloaded the data and placed it in your current working directory, you can import it using the read.csv command:\n\ndata_netflix &lt;- read.csv(\"netflix_titles.csv\")\n\n\n\n\n\n\n\nPro Tip\n\n\n\nTo maintain structure in your project folder, it is advisable to create a separate directory for the data and import it from there. For example, if the project directory contains a data directory with the netflix_title.csv file inside, it can be imported using\n\ndata_netflix &lt;- read.csv(\"data/netflix_titles.csv\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "01_prerequisites.html#sec-essentials",
    "href": "01_prerequisites.html#sec-essentials",
    "title": "1  Important R concepts",
    "section": "3.2 Essential functions and libraries",
    "text": "3.2 Essential functions and libraries\nOne of the most versatile and essential collection of libraries in the context of data science with R is the {tidyverse} library, which includes\n\n{ggplot} for creating beautiful graphics,\n{dplyr} for data manipulation,\n{stringr} for working with strings, and\n{tibble} for storing data effectively.\n\nAn excellent in-depth introduction to the tidyverse called R for Data Science is freely available online if that piques your interest. This introduction focuses on a few core functions that will be useful throughout the course. As every other library, {tidyverse} can be attached using the library function once it has been installed:\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nOnce the library has been added, every function contained is available. For example, the glimpse function can be used on the data_netflix data set to get a short overview of the data types and contents:\n\nglimpse(data_netflix)\n\nRows: 8,807\nColumns: 12\n$ show_id      &lt;chr&gt; \"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"s6\", \"s7\", \"s8\", \"s9\", \"s1…\n$ type         &lt;chr&gt; \"Movie\", \"TV Show\", \"TV Show\", \"TV Show\", \"TV Show\", \"TV …\n$ title        &lt;chr&gt; \"Dick Johnson Is Dead\", \"Blood & Water\", \"Ganglands\", \"Ja…\n$ director     &lt;chr&gt; \"Kirsten Johnson\", \"\", \"Julien Leclercq\", \"\", \"\", \"Mike F…\n$ cast         &lt;chr&gt; \"\", \"Ama Qamata, Khosi Ngema, Gail Mabalane, Thabang Mola…\n$ country      &lt;chr&gt; \"United States\", \"South Africa\", \"\", \"\", \"India\", \"\", \"\",…\n$ date_added   &lt;chr&gt; \"September 25, 2021\", \"September 24, 2021\", \"September 24…\n$ release_year &lt;int&gt; 2020, 2021, 2021, 2021, 2021, 2021, 2021, 1993, 2021, 202…\n$ rating       &lt;chr&gt; \"PG-13\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"PG…\n$ duration     &lt;chr&gt; \"90 min\", \"2 Seasons\", \"1 Season\", \"1 Season\", \"2 Seasons…\n$ listed_in    &lt;chr&gt; \"Documentaries\", \"International TV Shows, TV Dramas, TV M…\n$ description  &lt;chr&gt; \"As her father nears the end of his life, filmmaker Kirst…\n\n\nRows: 8,807 means that the data set has 8,807 entries, and Columns: 12 means that the data set has 12 variables, respectively. The first column presents the variable names, their data types, and some initial values, providing a clear structure to the data set. We can already see that except for one variable (release_year), every other variable is of type chr, which stands for character or string.\n\n3.2.1 Filtering, grouping, and summarizing data sets\nFunctions frequently encountered while working with data are filter, group_by, and summarise. Let’s say we want to find out, according to the data set, how many movies and series were released in each year following 2010. Now, if we were to tackle this problem without the {tidyverse} framework, our code might look a little something like this:\n\nnetflix_filtered &lt;- data_netflix[data_netflix$release_year &gt; 2010, ]\nresult &lt;- aggregate(rep(1, nrow(netflix_filtered)), \n                    by = list(netflix_filtered$release_year), \n                    FUN = sum)\ncolnames(result) &lt;- c(\"release_year\", \"n\")\nresult\n\n   release_year    n\n1          2011  185\n2          2012  237\n3          2013  288\n4          2014  352\n5          2015  560\n6          2016  902\n7          2017 1032\n8          2018 1147\n9          2019 1030\n10         2020  953\n11         2021  592\n\n\nor this:\n\nnetflix_filtered &lt;- data_netflix[data_netflix$release_year &gt; 2010, ]\nresult &lt;- as.data.frame(table(netflix_filtered$release_year))\ncolnames(result) &lt;- c(\"release_year\", \"n\")\nresult\n\n   release_year    n\n1          2011  185\n2          2012  237\n3          2013  288\n4          2014  352\n5          2015  560\n6          2016  902\n7          2017 1032\n8          2018 1147\n9          2019 1030\n10         2020  953\n11         2021  592\n\n\nThe first code cell seems much more complicated than the second, yet it returns the same result. However, things can be simplified even more using the {dplyr} library that is contained in the {tidyverse}:\n\nnetflix_filtered &lt;- data_netflix %&gt;%\n  filter(release_year&gt;2010) %&gt;%\n  group_by(release_year) %&gt;%\n  summarise(n= n())\n\nLet us break down the code snippet above:\n\nIn line 1 we use the pipe operator %&gt;%. It is part of the {magrittr} package and forwards an object into a function of call expression. Figuratively, a pipe does precisely what is expected: channel an object from one and to another. In this case, the pipe operator %&gt;% passes the data_netflix data set into the filter function.\nIn line 2 the filter function selects a subset of a data set that satisfies a given condition. Here, the condition is that the movie or series’ release year should be after 2010, which is indicated by the &gt; condition.\n\n\n\n\n\n\nNote\n\n\n\nWithout the pipe operator, the first and second line can be merged into\n\nfilter(data_netflix,release_year&gt;2010)\n\nhowever, concatenating multiple functions causes the code to be unreadable and should thus be avoided.\n\n\nResults of the filtering procedure are then passed to the group_by function via the pipe operator again. The group_by function converts the underlying data set into a grouped one where operations can be performed group-wise. In the third line of the code cell, the group_by function is applied to the release_year variable, meaning that the data set now contains a group for every release year.\nThis can be seen as a pre-processing step for the summarise function applied in the following line. The summarise function creates a new data set based on the functions passed as arguments. These functions are applied to every group created by the previous step. In the example above, the function applied is n(), which returns the group size. Thus, setting n=n() as the argument creates a new column named n, which contains the number of samples within each group.\n\n\n\n3.2.2 Mutating data sets\nBesides filtering, grouping, and summarizing, another important concept is mutating the data, i.e., modifying the content of the data set.\nThe mutate function either creates new columns or modifies existing columns based on the passed column names and functions that are passed. It is helpful for modifying data and their types, creating new variables based on existing ones, and removing unwanted variables. In the example below, the mutate function is used to modify the variable date_added, create a new variable called is_show and delete the variable type.\n\ndata_netflix &lt;- data_netflix %&gt;% \n  mutate(date_added = mdy(date_added),\n         is_show = if_else(type==\"TV Show\",TRUE,FALSE),\n         type = NULL\n         )\n\n\nIn the first line of the code cell, the data set data_netflix is specified to be overwritten by its mutated version. Overwriting a data set is achieved by reassigning the data_netlifx object to the output of the pipe concatenation of the following code lines.\nThe original data_netflix data set is passed into the mutate function in the second line. Here, the variable date_written is overwritten by the output of the mdy function with argument date_added. The mdy function is a function in the {lubridate} library that transforms dates stored in strings to date objects that are easier to handle. Note that we can directly pass column names into functions as we have previously passed the data set into the mutate function using the %&gt;% operator.\nIn the third line, a new variable is_show is created, which takes the value TRUE, if the type of an entry in the data set is \"TV Show\" and FALSE if it is not. The if_else function achieves this.\nSetting the type variable to NULL effectively removes it from the data set.\n\n\n\n\n\n\n\nNote\n\n\n\nAssigning values in functions is achieved by using the = symbol. Assigning new variables outside of functions can also be done with the = symbol, but it is rarely used and except for some pathological cases there is no difference. However, most R users prefer assigning environment variables using &lt;- which does not work in function calls.\n\n\n\n\n\n\n\n\nPro Tip\n\n\n\nIn the previous code cell a line break was added after the %&gt;% and each argument in the mutate function for readability purposes. The code also works without adding the line breaks, but it can get messy fast:\n\ndata_netflix &lt;- data_netflix %&gt;% mutate(date_added = mdy(date_added), is_show = if_else(type==\"TV Show\",TRUE,FALSE), type = NULL)\n\n\n\n\n\n3.2.3 Factor Variables\nAn important data type that can handle both ordinal (data with some notion of order) and nominal data are so-called factor variables.\nConsider the following toy data set containing seven people with corresponding age groups and eye colors.\n\n\n# A tibble: 7 × 3\n  names   age_groups eye_color\n  &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;    \n1 Alice   18-25      Blue     \n2 Bob     &lt;18        Brown    \n3 Charlie 26-35      Green    \n4 Diana   36-45      Hazel    \n5 Eve     18-25      Brown    \n6 Frank   60+        Blue     \n7 Grace   26-35      Green    \n\n\nSince the variable age_group only specifies a range of ages, it does not make sense to encode them as integers rather than ordinal variables. The’ mutate’ function can encode age groups as ordinal variables. This involves setting the age_groups variable to a factor with levels and labels. Levels specify the order of the values, and labels can be used to rename these categories.\n\ndata_example &lt;- data_example %&gt;%\n  mutate( \n    age_groups = factor(\n      age_groups,\n      levels = c(\"&lt;18\", \"18-25\", \"26-35\", \"36-45\", \"60+\"),\n      labels = c(\"child\",\"adult\",\"adult\",\"adult\",\"senior\"),\n      ordered = TRUE\n    )\n  )\n\n\nSimilar to the previous example, we should specify that we overwrite the data_example data set with a mutated version.\nThe mutate function is applied to the age_groups variable.\nSetting age_groups = factor(age_groups, ...) converts the age_groups column into a (so far unordered) factor, allowing for specific levels (categories) and labels.\nlevels = c(\"&lt;18\", \"18-25\",...) specifies the predefined levels for the age groups.\nordered=TRUE specifies that the age groups are ordered according to the specified levels.\nLast but not least, labels = c(\"child\", \"adult\", ...) specifies the labels that replace the numeric age groups. For instance, &lt;18 is labeled as \"child\", the ranges 18-25, 26-35, and 36-45 are labeled as \"adult\", and 60+ is labeled as \"senior\".\n\nSimilarly, the variable eye_color can also be converted to a nominal factor variable:\n\ndata_example &lt;- data_example %&gt;%\n  mutate(\n    eye_color = factor(eye_color)\n  )\n\nTo confirm that the variable age_group is indeed ordered, calling the feature shows the ordered levels:\n\ndata_example$age_groups\n\n[1] adult  child  adult  adult  adult  senior adult \nLevels: child &lt; adult &lt; senior",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "01_prerequisites.html#visualizing-data-with-ggplot",
    "href": "01_prerequisites.html#visualizing-data-with-ggplot",
    "title": "1  Important R concepts",
    "section": "3.3 Visualizing data with ggplot",
    "text": "3.3 Visualizing data with ggplot\nAnother critical aspect of data science and machine learning is graphical storytelling. Describing an algorithm strictly using mathematical notation or exploring a data set using descriptive and inductive statistics alone can make it challenging to understand the message. While R offers some base functions for creating graphics, this course primarily uses the library {ggplot2}. A comprehensive introduction to {ggplot2} can be found in Hadley Wickham’s book Elegant Graphics for Data Analysis. A short summary can be found below.\nFor the following example, we will use the netflix_filtered data set (see Section 3.2.1)\nA graphic created with {ggplot2} consists of the following three base components:\n\nThe data itself.\n\nggplot(data = netflix_filtered)\n\n\n\n\n\n\n\n\nNote, that the plot does not show any axis, ticks, and variables.\nA set of aesthetics mappings that describe how variables in the data are mapped to visual properties.\n\nggplot(aes(x=release_year, y=n), data = netflix_filtered)\n\n\n\n\n\n\n\n\nUsing the aes function, we have specified that the release year should be mapped to the \\(x\\)-axis, and \\(n\\) to the \\(y\\)-axis.\nLastly, the geom-layer (component) describes how each observation in the data set is represented.\n\nggplot(aes(x=release_year, y=n), data = netflix_filtered)+\n  geom_col()+\n  geom_point()+\n  geom_line()\n\n\n\n\n\n\n\n\nCompared to the previous two code cells, a lot is going on here. So, let us break it down.\n\nThe plus at the end of line 1 is used to add another layer.\ngeom_col adds a column chart to the canvas, creating columns starting at 0 and ending at \\(n\\). Then, + indicates that another layer is added.\ngeom_point represents the data as points on the plane, i.e., an \\(x\\) and \\(y\\)-coordinate. The + indicates that yet another layer is added afterward.\nLastly, the geom_line function adds a line connecting each data point with the one following.\n\n\n\n\n\n\n\nPro Tips\n\n\n\n\nAs before, the data set can also directly be piped into the ggplot function:\n\nnetflix_filtered %&gt;%\nggplot(aes(x=release_year, y=n))+\n geom_col()+\n geom_point()+\n geom_line()\n\nBy changing the order of the layers, you can specify which layer should be added first and last. In this example, since geom_col was added first and every other layer is placed on top of the column plot.\n\n\n\n\nThere are a lot more functions and settings that can be applied to each function. A selection of those is discussed in the exercises.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "01_prerequisites.html#statistical-data-exploration-and-manipulation",
    "href": "01_prerequisites.html#statistical-data-exploration-and-manipulation",
    "title": "1  Important R concepts",
    "section": "4.1 Statistical Data Exploration and Manipulation",
    "text": "4.1 Statistical Data Exploration and Manipulation\nWe will start by getting a feeling for the data and performing some basic data manipulation steps.\n\nExercise 4.1 Import the data set and use the glimpse function to generate a summary of the data set.\n\n\nExercise 4.2 Assume that the data set has been imported and saved as an object called credit_info. Explain the following code snippet both syntactically and semantically. Hint: Use the help function for any function you do not know.\n\ncredit_info %&gt;%\n  select_if(is.character) %&gt;%\n  sapply(table)\n\n$Attrition_Flag\n\nAttrited Customer Existing Customer \n             1627              8500 \n\n$Gender\n\n   F    M \n5358 4769 \n\n$Education_Level\n\n      College     Doctorate      Graduate   High School Post-Graduate \n         1013           451          3128          2013           516 \n   Uneducated       Unknown \n         1487          1519 \n\n$Marital_Status\n\nDivorced  Married   Single  Unknown \n     748     4687     3943      749 \n\n$Income_Category\n\n       $120K +    $40K - $60K    $60K - $80K   $80K - $120K Less than $40K \n           727           1790           1402           1535           3561 \n       Unknown \n          1112 \n\n$Card_Category\n\n    Blue     Gold Platinum   Silver \n    9436      116       20      555 \n\n\n\n\nExercise 4.3 Overwrite the variables Income_Category and Education_Level into ordered factors. When setting the levels for each group, set \"Unknown\" as the lowest level. Use this cleaned data set for the remaining exercises.\n\n\nExercise 4.4 Group the data set by income category and find out each group’s mean and median credit limit.\n\n\nExercise 4.5 Which income group has the highest mean credit limit?\n\n\nExercise 4.6 Use the following code snippet to modify the data set by incorporating it into the mutate function. The snippet converts all \"Unknown\" values contained in character or factor columns into NA values, which are easier to handle.\n\nacross(where(~ is.character(.) | is.factor(.)),\n       ~na_if(.,\"Unknown\"))\n\n\n\nExercise 4.7 Apply the na.omit() function to the data set to remove all samples in the data set that contain NA values. How many samples have been removed in total?\n\nSometimes, we only want to infer results for specific subgroups. The Blue Credit Card is the most common type of credit card. Gaining insights for this particular group allows us to retrieve information that might be useful in later analyses.\n\nExercise 4.8 Find out how many customers have a Blue credit card.\n\n\nExercise 4.9 Create a new data set credit_info_blue containing all customers that hold a Blue credit card.\n\n\nExercise 4.10 Find the number of female customers holding the Blue Card who are, at most, 40 years old and have a credit limit above 10,000 USD.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "01_prerequisites.html#visual-data-exploration",
    "href": "01_prerequisites.html#visual-data-exploration",
    "title": "1  Important R concepts",
    "section": "4.2 Visual Data Exploration",
    "text": "4.2 Visual Data Exploration\n\nExercise 4.11 We now want to explore some of the demographics in our data set. Create a histogram for the age of the customers using the geom_histogram function. Note that only one variable is required for the aesthetics to create a histogram.\n\n\nExercise 4.12 Using the default parameters in the geom_histogram function, the message “stat_bin() using bins = 30. Pick better value with binwidth.” is displayed. Modify the code so that each age gets its own bin.\n\n\nExercise 4.13 Now that the histogram looks more organized, we want to add more information. For example, by setting the fill option to Gender, we can create two overlapping histograms showing the number of male and female customers within each age group.\n\n\nExercise 4.14 Instead of visualizing the Gender as in the plot above, we now want to analyze the continuous variable Credit_Limit. Therefore, instead of a histogram, use the geom_density function that plots an estimate of the underlying probability density.\n\n\nExercise 4.15 The histograms and density plots only provide limited insight into the demographics and customer status as it is relatively complex to figure out the proportions of each group. To take this one step further, consider the following histogram, which shows the Education_Level within every bin.\n\nggplot(data = credit_info_clean, aes(Customer_Age, fill = Education_Level))+\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nWe can use the geom_histogram function and the facet_wrap function, which generates a subplot for each group. Apply the facet_wrap function to create a subplot for each education level.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "01_prerequisites.html#loss-functions",
    "href": "01_prerequisites.html#loss-functions",
    "title": "1  Important R concepts",
    "section": "4.3 Loss functions",
    "text": "4.3 Loss functions\nIn future exercises, different loss functions will be deployed to measure how far some regression results deviate from actual values. This exercise, therefore, briefly discusses the advantages and disadvantages of some loss functions and introduces them in R.\nData-wise, we will consider the credit_info dataset and a simple linear model that is used to predict each customer’s credit limit.\nThe following Code snippet reads the unmodified data, removes the features Total_Revolving_Bal and Avg_Open_To_Buy and trains a linear model with target variable Credit_Limit on all the remaining features. It’s important to note that the model is intentionally kept simple for demonstrative purposes, making it easier for you to grasp and apply the concepts.\nCopy the snippet into your own notebook and run it. Hint: You might have to change the path in the read.csv function to your specified data path (Exercise 4.1) and install the libraries that are attached.\n\nlibrary(tidymodels)\nlibrary(yardstick)\n\ncredit_info &lt;- read.csv(\"data/BankChurners.csv\")\n\nmodel_linear_data &lt;- credit_info %&gt;%\n  select(-c(Total_Revolving_Bal,Avg_Open_To_Buy))\n\nmodel_linear_res &lt;- linear_reg() %&gt;%\n  fit(Credit_Limit ~., data = model_linear_data) %&gt;%\n  augment(model_linear_data)\n\nThe object model_linear_res now contains our model’s original data set and predictions. Do not worry if you do not understand every line in the snippet above. We will consider training models in future exercises more thoroughly.\n\n4.3.1 MAE Loss\nThe first loss function we explore is the Mean Absolute Error (MAE) loss defined as\n\\[\\begin{equation*}\n  \\mathrm{MAE} := \\mathrm{MAE}(y,\\hat{y}):=\\frac{1}{n}\\sum_{i=1}^n |y_i-\\hat{y_i}|,\n\\end{equation*}\\]\nwhere \\(y=(y_1,...,y_n)\\) are target values and \\(\\hat{y}=(\\hat{y_1},...,\\hat{y_n})\\) are estimates of the target values.\n\nExercise 4.16 Briefly explain how the MAE loss can be interpreted regarding the target features scale.\n\n\nExercise 4.17 The mae loss is a function in the {yardstick} library. If not already done, install the {yardstick} library and read the help function of the mae function. Then, apply it tot the model_linear_res data set and interpret the result.\n\n\n\n4.3.2 (R)MSE\nAnother widely used loss function is the (Root)MeanSquareError. It is defined as\n\\[\\begin{align*}\n  \\mathrm{RMSE} &:= \\mathrm{RMSE}(y,\\hat{y}) := \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2}\\\\\n  \\mathrm{MSE} &:= \\mathrm{MSE}(y,\\hat{y}) := \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2\n\\end{align*}\\]\n\nExercise 4.18 Repeat the exercise Exercise 4.16 and Exercise 4.17 for the RMSE and MSE.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "02_linear_models.html",
    "href": "02_linear_models.html",
    "title": "2  Linear Models",
    "section": "",
    "text": "3 Exercises\nlibrary(\"tidyverse\")\nlibrary(\"tidymodels\")\nlibrary(\"ggtext\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "02_linear_models.html#exercise-1-bias-variance-trade-off",
    "href": "02_linear_models.html#exercise-1-bias-variance-trade-off",
    "title": "2  Linear Models",
    "section": "3.1 Exercise 1: Bias-Variance Trade-Off",
    "text": "3.1 Exercise 1: Bias-Variance Trade-Off\nIn the lecture, we have already briefly discussed the issue of overfitting by considering a simple example. The goal of this exercise is to examine a real-world dataset and see how we can recreate the phenomenon of under and overfitting with a simple linear model.\n\n3.1.1 Exercise 1a: Data exploration and manipulation\n\ndata_aux_filtered &lt;- read_csv(\"data/rent_aux.csv\")\n\nThe rent_aux dataset is a preprocessed subset of the Apartment rental offers in Germany dataset. It contains 239 unique rental listings for flats in Augsburg. The data was sourced at three different dates in 2018 and 2019 and contains 28 different variables.\nInstead of focusing on a comprehensive data cleaning and manipulation process, we will simply use the two variables livingSpace measuring the area of living in \\(m^2\\) of a listing and totalRent in EUR, representing the total amount of rent for a month (base rent + utilities).\n\n3.1.1.1 Exercise 1a i:\nVisualize the relationship between the two variables livingSpace and totalRent. The output could look like the plot below.\n\ndata_aux_filtered %&gt;% \n  ggplot(aes(x = livingSpace, y = totalRent)) +\n  geom_point() +\n  labs(\n    x = \"living space\",\n    y = \"total rent\"\n  )\n\n\n\n\n\n\n\n\n\n\n3.1.1.2 Exercise 1a ii:\nWithout conducting a thorough outlier analysis we decide to remove every listing that either costs more than \\(2500\\) EUR or is bigger than \\(200\\: m^2\\). Use the filter function to remove those outliers.\n\ndata_aux_filtered &lt;- data_aux_filtered %&gt;%\n  filter(totalRent &lt;= 2500, livingSpace &lt;= 200)\n\n\n\n\n3.1.2 Exercise 1b: Training a simple model\nIn this exercise, we want to fit an overly simple model to demonstrate it’s low performance on both, a training and test set.\n\n3.1.2.1 Exercise 1b i:\nIn order to ensure consistent results, we set a seed. Furthermore, the vector sampled that is filled \\(70\\%\\) with TRUE values and \\(30 \\%\\) FALSE values is given. Its length matches the length of the data_aux_filtered dataset. The goal is to create a simple training and testing set by randomly splitting the data_aux_filtered dataset into two subsets.\n\nset.seed(2)\nsampled &lt;- sample(c(TRUE, FALSE),\n                 nrow(data_aux_filtered),\n                 replace=TRUE, prob=c(0.7, 0.3)\n                 )\nsampled\n\n  [1]  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n [13] FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n [37] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n [49]  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n [61] FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE\n[109]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE\n[121]  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[133]  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[145]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\n[157] FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n[169]  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n[181] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n[193] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE\n[205]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n[217]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n[229] FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n\n\n\n3.1.2.1.1 Exercise 1b i a:\nUse the vector sampled to create a tibble rent_train consisting of all samples in data_aux_filtered that match the position of TRUE values in the vector sample. Subsequently, create a list rent_test that consists of all samples in data_aux_filtered that match the position of FALSE values in the vector sampled.\n\nrent_train &lt;- data_aux_filtered[sampled, ]\nrent_test &lt;- data_aux_filtered[!sampled, ]\n\n\n\n3.1.2.1.2 Exercise 1b i b:\nUsing the if_else function and %in% operator, add a column called label to data_aux_filtered that consists of the strings \"train\" and \"test\" at the respective positions where a sample is either in rent_train or rent_test. You can use the variable scoutId to match the samples in rent_train and rent_test with samples in data_aux_filtered.\n\ndata_aux_filtered &lt;- data_aux_filtered %&gt;% mutate(\n  label = if_else(\n    scoutId %in% rent_train$scoutId, \n    \"train\",\n    \"test\"\n  )\n)\n\n\n\n\n3.1.2.2 Exercise 1b ii:\nCreate a model simple_model using the lm function and fit the variable totalRent on totalRent. The underlying data should be the previously created list rent_train. By following this kind of weird procedure, we create a model that simply returns the mean of totalRent as a predictor. There will most likely be a warning that we can (at least this time) safely ignore. Also, watch out that the output of lm() will show a rounded value as an intercept but in further calculations, the output simple_model contains the exact value.\n\nsimple_model &lt;- lm(\n  formula = totalRent ~ totalRent, \n  data = rent_train\n)\n\n\n\n3.1.2.3 Exercise 1b iii:\nCalculate the RMSE loss of the training set rent_train and the testing set rent_test by using the loss_rmse function below. The fitted values on the training set can be accessed by using simple_model$fitted.values. In order to predict the values of the rent_train set, familiarize yourself with the predict function. For reference, you can find some plausible values below.\n\nloss_rmse &lt;- function(y, yhat){\n  sqrt(mean((y - yhat)^2))\n}\n\n\nrmse_simple_model_train &lt;- loss_rmse(\n  rent_train$totalRent,\n  simple_model$fitted.values\n)\nrmse_simple_model_test &lt;- loss_rmse(\n  rent_test$totalRent,\n  predict(simple_model, rent_test)\n)\n\nglue::glue(\n\"Training error: {round(rmse_simple_model_train,2)}\\n\nTesting error: {round(rmse_simple_model_test,2)}\n\")\n\nTraining error: 419.43\n\nTesting error: 474.93\n\n\n\n\n3.1.2.4 Exercise 1b iv:\nComplete the following code snippet such that the plot looks like the one displayed below. What does the graph below show?\n\ndata_aux_filtered %&gt;% \n  ggplot(\n    aes(\n    ###############\n    ## Fill Here ##\n    ###############\n    )\n  ) +\n  geom_point(size = 3, alpha = 0.5)+\n  geom_hline(\n    aes(yintercept = simple_model$coefficients),\n    linewidth = 2\n  ) +\n  labs(\n    x = \"Living space (in sq.m)\",\n    y = \"Total rent (in Euro)\",\n    color = 'Data Set',\n    title = \"Our Model's fitted line doesn't fit particularly well\"\n  ) +\n  theme_minimal(base_size = 16)+\n  scale_color_brewer(palette = 'Set2')\n\n\ndata_aux_filtered %&gt;% \n  ggplot(\n    aes(\n      x=livingSpace,\n      y = totalRent,\n      colour = label\n    )\n  ) +\n  geom_point(size = 3, alpha = 0.5)+\n  geom_hline(\n    aes(yintercept = simple_model$coefficients),\n    linewidth = 2\n  ) +\n  labs(\n    x = \"Living space (in sq.m)\",\n    y = \"Total rent (in Euro)\",\n    color = 'Data Set',\n    title = \"Our Models's fitted line doesn't fit particularly well\"\n  ) +\n  theme_minimal(base_size = 16)+\n  scale_color_brewer(palette = 'Set2')\n\n\n\n\n\n\n\n\n\n\n\n3.1.3 Exercise 1c: Overfitting a model\nNow that we have seen how robust (low variance) a simple model is, we will consider a more complex linear model in this exercise.\n\n3.1.3.1 Exercise 1c i:\nCreate a model named model on the rent_train dataset, again, with totalRent as the response variable, but instead of simply using the livingSpace variable as a single predictor, use the poly(livingSpace,20) object instead.\n\nmodel &lt;- lm(\n  formula = totalRent ~ poly(livingSpace, 20), \n  data = rent_train\n)\n\n\n\n3.1.3.2 Exercise 1c ii:\nCalculate the RMSE. For reference, you can find some plausible values below.\n\nrmse_model_train&lt;- loss_rmse(\n  rent_train$totalRent,\n  model$fitted.values\n)\nrmse_model_test &lt;- loss_rmse(\n  rent_test$totalRent,\n  predict(model, rent_test)\n)\nglue::glue(\n\"training error: {round(rmse_model_train,2)}\\n\ntesting error: {round(rmse_model_test,2)}\"\n)\n\ntraining error: 125.43\n\ntesting error: 839.96\n\n\n\n\n3.1.3.3 Exercise 1c iii:\nExplain in your own words the phenomenon observed in the following plot.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "02_linear_models.html#intermezzo-good-practices-for-applied-machine-learning",
    "href": "02_linear_models.html#intermezzo-good-practices-for-applied-machine-learning",
    "title": "2  Linear Models",
    "section": "3.2 Intermezzo: Good practices for applied Machine Learning",
    "text": "3.2 Intermezzo: Good practices for applied Machine Learning\nIn previous courses, we mainly focused on how to fit a certain model to a given dataset. However, this process could be described as model specification, rather than model development. So, what’s the difference between specifying a model and actually building a model?\n\n3.2.1 Developing a model (What we have done so far!):\n\nThe given dataset has been cleaned, transformed, and manipulated using a multitude of different packages and libraries.\nResampling methods have been applied but training the model on each subset or newly generated dataset is usually performed by using a loop or similar methods.\nSimilar to applying resampling methods, hyperparameter tuning is applied by using a loop or similar methods.\n\nIn summary, we have only basically specified the model we want to train and used a rather arbitrary and inconsistent approach for everything else.\nOne of the biggest issues we face, however, is when switching the model. The approach we have been using so far emphasizes working with one selected model that we wish to keep using after data preprocessing.\n\n\n3.2.2 Developing a model (What we want to do moving forward!):\nThe main difference between the old approach and the new approach comes down to leveraging the advantages of the {tidyverse} and {tidymodels} frameworks. These frameworks allow for consistently preprocessing the data, setting model specifications, and performing steps like resampling and hyperparameter tuning all at once.\nAnother huge advantage is, that by following this procedure we can also swiftly switch between different ML models. For example, applying a random forest algorithm and switching to a neural network approach for the same data works is only a matter of changing a few lines of code as we will see in later exercises.\nSo where is the catch? At first, the process might seem a bit difficult or even “overkill” for the models we use. However, as the lecture progresses, our models will also (at least sometimes) become increasingly sophisticated. So we want to get used to this new process as early as possible in order to get used to it.\nThe biggest takeaways are:\n\nConsistency: Independent of what the dataset or desired model looks like, we can (almost) always use the same procedure when building a model.\nEffectiveness: Once we get used to this new approach, we can develop our models a lot more effectively than before.\nSafety: Developing an ML model has many pitfalls and potholes on the way and by design, {tidymodels} helps us to avoid those.\n\nWe will introduce and explore some of the aforementioned concepts in the next exercise and dive deeper in later sessions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "02_linear_models.html#exercise-2-cross-validation",
    "href": "02_linear_models.html#exercise-2-cross-validation",
    "title": "2  Linear Models",
    "section": "3.3 Exercise 2: Cross-Validation",
    "text": "3.3 Exercise 2: Cross-Validation\nOur first step down the {tidymodels} rabbit hole has already been broached in the lecture. By splitting the data into a training set and test set we can effectively evaluate our trained model on previously unseen data. However, how can we ensure that splitting the data once into a training set and test set yields a good model? A simple answer is: We can’t.\nThat is where cross-validation comes into play.\n\n3.3.1 Exercise 2a:\nSimilar to the lecture slides we want to build a simple model for predicting the response variable totalRent in Augsburg by using the single predictor livingSpace.\n\n3.3.1.1 Exercise 2a: i\nFamiliarize yourself with the initial_split function and create a split named split on the dataset data_aux_filtered.\n\nsplit &lt;- initial_split(data_aux_filtered)\n\n\n\n3.3.1.2 Exercise 2a ii:\nUtilizing the newly created split, create the objects data_train and data_test by using the training and testing functions respectively. Note, that we are basically doing the same thing as in Exercise 1b, but instead of using the base R function sample, we’re now working with the {rsample} package that is part of the {tidyverse} framework.\n\ndata_train &lt;- training(split)\ndata_test &lt;- testing(split)\n\n\n\n3.3.1.3 Exercise 2a iii:\nFamiliarize yourself with the functionality of the vfold_cv function and create an instance named folds with the dataset data_train and the parameter v = 10.\n\nfolds &lt;- vfold_cv(data_train, v = 10)\n\nThe function vfold_cv randomly splits the data into v roughly equally sized subsets. One resample then consists of v-1 of those subsets. The picture below depicts how that process works for v = 5.\n\n\n\n5-fold Cross Validation\n\n\n\n\n3.3.1.4 Exercise 2a iv:\nEach split in the fold can be accessed as a variable by using the $ operator. In order to access split i, we have to use the notation folds$splits[[i]]. By using the analysis function we can then access the v-1 subsets of the original data set that are returned as a data frame. The assessment function returns the hold-out subset of the split.\nFamiliarize yourself with the functionality of the analysis and assessment function. For the first split, create a scatter plot for both of the subsets subsets. An example of what such plots could look like can be found below.\n\nfolds$splits[[1]] %&gt;% \n  analysis %&gt;%\n  ggplot(aes(x = livingSpace, y = totalRent)) +\n  geom_point() +\n  labs(\n    x = \"living space\",\n    y = \"total rent\",\n    title = \"Training sample in split 1\"\n  )\n\n\n\n\n\n\n\n\n\nfolds$splits[[1]] %&gt;% \n  assessment %&gt;%\n  ggplot(aes(x = livingSpace, y = totalRent)) +\n  geom_point()+\n  labs(\n    x = \"living space\",\n    y = \"total rent\",\n    title = \"Testing sample in split 1\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 Exercise 2b:\nNow that we have set up our data set for cross-validation, we can continue developing our model. The next step is to assemble the model that we wish to use. In this case, we will choose a simple linear model once again.\n\n3.3.2.1 Exercise 2b i:\nInstead of using a model created with the lm function of the {stats} package as in Exercise 1b ii, we will continue working with the {tidymodels} framework. The {parsnip} package offers a standardized interface for fitting models as well as the return values. A linear model can be created with the linear_reg function. Create a linear model called lm_mod using the linear_reg function. Note, that you do not have to pass any arguments to the function linear_reg at this point.\n\nlm_mod &lt;- linear_reg()\n\nIf calling the lm_mod object returns the same output as the one you can find below, you have solved this exercise correctly!\n\nlm_mod \n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n3.3.2.2 Exercise 2b ii:\nAs a next step, we want to create a so-called recipe. According to the R Documentation, a recipe is a description of the steps to be applied to a data set in order to prepare it for data analysis. So a recipe can be thought of figuratively as a sequential procedure to cook our model!\nIn this exercise, our recipe is rather simple. Create a recipe called lm_recipe by using the recipe function. As arguments for the recipe function, you can pass the following formula. The second argument concerns the data that is given by the data_train dataset in our case.\n\nformula &lt;- totalRent ~ livingSpace\n\nIf calling the lm_recipe object returns the same output as the one you can find below, you have solved this exercise correctly!\n\nlm_recipe &lt;- recipe(formula, data = data_train)\n\nlm_recipe %&gt;% prep %&gt;% bake(data_train)\n\n# A tibble: 179 × 2\n   livingSpace totalRent\n         &lt;dbl&gt;     &lt;dbl&gt;\n 1        55         710\n 2        40         620\n 3        53         666\n 4        87        1282\n 5        75         950\n 6       121        1470\n 7        60.8      1060\n 8       105        1285\n 9        58.1       974\n10        59         825\n# ℹ 169 more rows\n\n\n\n\n3.3.2.3 Exercise 2b iii:\nAfter creating our recipe, we create a so-called workflow. The workflow object aggregates information required to fit and predict from a model. In this case, our model lm_mod and the recipe lm_recipe. This whole procedure might seem quite complicated by now. However, later on, it will prove much more efficient and consistent this way.\nGiven the following code chunk, complete the workflow by adding the model lm_mod using the add_model function and the lm_recipe using the function add_recipe.\n\nlm_wf &lt;- \n  workflow() %&gt;%\n  ###################\n  ## Continue here ##\n  ###################\n\nAs above, you can check whether your workflow is correct by calling it and comparing it to the following output.\n\nlm_wf &lt;- \n  workflow() %&gt;%\n  add_model(lm_mod) %&gt;%\n  add_recipe(lm_recipe)\n\nlm_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n3.3.2.4 Exercise 2b iii:\nAs a next step, we will specify the metrics we want to use for evaluating our model. The metric_set function allows us to combine multiple metric functions such as the RMSE, MAE, and Huber Loss.\nCreate an instance called multi_metric using the metric_set function. As arguments, you can pass rmse,mae, and, huber_loss.\n\nmulti_metric &lt;- metric_set(rmse, mae, huber_loss)\n\nAs above, you can check whether your metric set multi_metric is correct by calling it and comparing it to the following output.\n\nmulti_metric\n\nA metric set, consisting of:\n- `rmse()`, a numeric metric       | direction: minimize\n- `mae()`, a numeric metric        | direction: minimize\n- `huber_loss()`, a numeric metric | direction: minimize\n\n\n\n\n3.3.2.5 Exercise 2b iv:\nNow that we have fully specified our model development process we can fit the model by piping the workflow lm_wf to the fit_resamples function. As the name suggests, the fit_resamples function fits the linear model on each of the folds provided. We simply have to specify the vfold_cv object and the metric_set in order to train the model and calculate the metrics.\n\nlm_fit_rs &lt;-\n  lm_wf %&gt;% \n  fit_resamples(folds, metrics = multi_metric)\n\nFamiliarize yourself with the collect_metrics function and apply it to the lm_fit_rs object. The function returns the average for the respective metrics over all hold-out folds of all the splits (i.e. the average of all the blue cells in the illustration in Exercise 2a iii) and a few more key figures such as the standard deviation of the metric across all folds and the number of folds. A possible output can be found below.\n\nlm_fit_rs %&gt;% collect_metrics()\n\n# A tibble: 3 × 6\n  .metric    .estimator  mean     n std_err .config             \n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 huber_loss standard    130.    10    9.44 Preprocessor1_Model1\n2 mae        standard    131.    10    9.44 Preprocessor1_Model1\n3 rmse       standard    160.    10    9.62 Preprocessor1_Model1\n\n\n\n\n\n3.3.3 Exercise 2c:\nIn this last exercise, we will first look at a demonstration of how to properly train multiple models within the {tidymodels} framework. The idea is to fit multiple polynomial regressions using cross-validation. Subsequently, it is your task to extract different hold-out sample losses and visualize them properly.\n\n3.3.3.1 Exercise 2c i:\nCarefully read the following paragraph and insert the code chunks into your own worksheet in order to solve the subsequent exercises.\n\nWe first set the degree values for the polynomials we wish to fit. Similar to Exercise 2 b ii, we then create a recipe, by setting a formula and the data we want to train on. The recipe rec_poly is then passed to the step_poly function that basically creates new columns that are basis expansions of the livinSpace variable. Note, that as for the degree of the step_poly function, we did not pass degree_values rather than the {hardhat} function tune(). tune() is a placeholder for the grid degree_values. Why this is important will be elaborated in the explanation of the next cell (🐻 with me). The raw = TRUE parameter is passed because, in this specific example, we want to use the original (or in this case raw) values of the variable livingSpace.\n\ndegree_values &lt;- 2:10\n\nrec_poly &lt;- lm_recipe %&gt;% \n    step_poly(\n      livingSpace, \n      degree = tune(), \n      options = list(raw = TRUE)\n    )\n\nSimilarly to Exercise 2b iii, we then create a workflow where we pass the model lm_mod we created in Exercise 2b i.\nThe real magic then happens when passing the workflow poly_wf to the tune_grid function. The tune_grid function basically iterates along the degree_values, trains the model on all the folds sequentially, and even saves the previously specified metrics in the multi_metric vector to the tune_poly object.\n\npoly_wf &lt;- workflow() %&gt;%\n    add_model(lm_mod) %&gt;%\n    add_recipe(rec_poly)\n\ntune_poly &lt;- poly_wf %&gt;%\n  tune_grid(\n    grid = tibble(degree = degree_values),\n    metrics = multi_metric,\n    resamples = folds\n  )\n\nSimilarly to Exercise 2b iv, we can collect the metrics for the average of all hold-out samples by using the collect_metrics function. However, as we have trained polynomials up to degree n = 10, the function returns the metrics for all polynomials with degree n = 2,…,10.\n\n\n\n3.3.3.2 Exercise 2c ii:\nGiven the tune_poly tibble from the paragraph above, we now want to evaluate the metrics graphically. To do so, create a tibble metrics_cv by applying the collect_metrics() to the tune_poly tibble. The result should look similar to the output below.\n\nmetrics_cv &lt;- tune_poly %&gt;%\n  collect_metrics()\n\n\nhead(metrics_cv)\n\n# A tibble: 6 × 7\n  degree .metric    .estimator  mean     n std_err .config             \n   &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1      2 huber_loss standard    124.    10    9.69 Preprocessor1_Model1\n2      2 mae        standard    124.    10    9.69 Preprocessor1_Model1\n3      2 rmse       standard    158.    10   11.1  Preprocessor1_Model1\n4      3 huber_loss standard    124.    10    9.55 Preprocessor2_Model1\n5      3 mae        standard    125.    10    9.55 Preprocessor2_Model1\n6      3 rmse       standard    157.    10   11.0  Preprocessor2_Model1\n\n\n\n\n3.3.3.3 Exercise 2c iii:\nUsing the metrics_cv tibble, create a line plot that displays different mean losses for each polynomial model. The plot should look similar to the one shown below.\n\nmetrics_cv %&gt;%\n  ggplot(aes(x = degree, y = mean, color = .metric)) +\n  geom_line(linewidth = 1) +\n  ylab(\"Mean loss of hold-out sample\") +\n  xlab(\"Degree of fitted polonomial\") +\n  labs(\n    title = 'Mean Metrics across Folds',\n    col = 'Metric',\n  )",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "04_regression_trees.html",
    "href": "04_regression_trees.html",
    "title": "4  Regression Trees",
    "section": "",
    "text": "5 Exercises\nSome of the packages we will use throughout the session.\nlibrary(\"tidyverse\")\nlibrary(\"tidymodels\")\nWarning: package 'ggtext' was built under R version 4.4.1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression Trees</span>"
    ]
  },
  {
    "objectID": "04_regression_trees.html#exercise-1-theoretical-aspects-of-regression-trees",
    "href": "04_regression_trees.html#exercise-1-theoretical-aspects-of-regression-trees",
    "title": "4  Regression Trees",
    "section": "5.1 Exercise 1: Theoretical aspects of regression trees",
    "text": "5.1 Exercise 1: Theoretical aspects of regression trees\nIn this exercise, we want to gain intuition for the theoretical aspects of regression trees.\n\n5.1.1 Exercise 1a: Representations and modelling decisions\nBefore diving into the process of building and evaluating a tree rigorously, we first consider different representations of binary trees, check their validity, and decide for simple datasets, whether they are suitable for regression trees.\n\n5.1.1.1 Exercise 1a i:\nConsider the following two splits of the feature space generated by two features \\(X_1\\) and \\(X_2\\). Argue, which one of the splits was generated by a binary splitting tree!\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSplit 1 can’t be produced by a binary tree, because the bottom-center rectangle is overlapping the left-most rectangle.\n\n\n\n\n5.1.1.2 Exercise 1a ii:\nConsider the following split generated by a binary tree. \\(t_1,…,t_4\\) denote the splitting conditions, \\(R_1,…,R_4\\) the final regions, and \\(X_1,X_2\\) the variables used for evaluating the splitting conditions.\nDraw a binary tree that corresponds to the split given below.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n5.1.1.3 Exercise 1a iii:\nFor the following scatterplots, decide whether a simple linear model ( \\(y=\\hat \\beta_1x+\\hat \\beta_0\\) ) or a regression tree should be chosen for modeling the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nFor the data in the first plot, we should use a simple linear model, as the data seems to follow a simple linear trend.\nA linear model is most likely not suitable for modelling this data, as the shape of the cloud of points looks more like a parabola instead of a line.\nAs the third plot consists of points that can be assigned to four (almost) distinct regions, a regression tree seems to be more suitable than a linear model.\nAt first, the data in the fourth plot seems to be too messy to make a decision. However, upon closer inspection, there are several indicators that a linear model might perform better:\n\nThe points in the center seem to follow a positive linear trend.\nThe deviations of points around this linear trend seem to be distributed in a way, that there are more points towards the line than further away. So the residuals could be assumed to be normally distributed.\n\n\n\n\n\n\n\n5.1.2 Exercise 1b: Modelling a regression tree\nNow, that we have considered some visual examples of trees and gained an intuition of situations where trees might be a suitable model, we now want to focus on the process of building a tree.\n\n5.1.2.1 Exercise 1b i:\nConsider the following dataset. Calculate the first optimal splitting point with respect to \\(x\\).\n\ndata &lt;- tibble(\n  x = c(1,0.5,2.0,5.5,4.5),\n  y = c(10,7,8,3,4)\n)\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n1.0\n10\n\n\n0.5\n7\n\n\n2.0\n8\n\n\n5.5\n3\n\n\n4.5\n4\n\n\n\n\n\n\n\nIn order to do so, you have to proceed as follows:\n\nDerive the order statistics of \\(\\{x_1,...,x_n\\}\\)\nDerive the set \\(S := \\left\\{\\frac{1}{2}(x_{(r)}+x_{(r+1)}):r=1,...,n-1\\right\\}\\) of all potential splitting points.\nFor each potential splitting point, derive the regions \\(R_1\\) and \\(R_2\\) and calculate the estimate \\(\\hat y_1\\) and \\(\\hat y_2\\) for the respective regions.\nCalculate the loss \\(\\mathcal{L}(y,\\hat y) := \\sum_{i:x_i\\in R_1} (y_i-\\hat y)^2 + \\sum_{i:x_i\\in R_2}(y_i-\\hat y)^2\\).\nDerive the optimal splitting point by settling for the splitting point leading to the smallest loss \\(\\mathcal{L}\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\nloss_x&lt;- function(data,r) {\n  xr = sort(data$x)[r]\n  y1 &lt;- mean(data$y[data$x&lt;=xr])\n  y2 &lt;- mean(data$y[data$x&gt;xr])\n\n  loss&lt;-sum((data$y[data$x&lt;=xr]-y1)^2) + sum((data$y[data$x&gt;xr]-y2)^2)\n  \n  return(loss)\n}\n\nSince we are interested in finding the optimal split with respect to \\(x_1=x\\), consider the sets of all possible splits \\[\nS := \\left\\{\\frac{1}{2}(x_{(r)}+x_{(r+1)}):r=1,...,n-1\\right\\} = \\{0.75,1.5,3.25,5\\}.\n\\]\nHere, \\(\\{x_{(r)},\\, r = 1,...,n\\} = \\{0.5,1,2,4.5,5.5\\}\\) denotes the order statistic of \\(x\\).\nFor \\(r=1\\) we have \\(s = 0.75\\) and \\[\n\\begin{align*}\nR_1(1,0.75) &= \\{x: x \\leq 0.75\\} = \\{0.5\\},\\\\\nR_2(1,0.75) &= \\{x: x &gt; 0.75\\} = \\{1.0,2.5,5.5,4.5\\}.\n\\end{align*}\n\\] Then, \\[\n\\begin{align*}\n\\hat y_1 &= \\frac{1}{|R_1|}\\sum_{i:x_{i}\\in R_1} y_i = \\frac{1}{1}\\cdot 7 = 7,\\\\\n\\hat y_2 &= \\frac{1}{|R_2|}\\sum_{i:x_{i}\\in R_2} y_i = \\frac{1}{4}(10+8+3+4) = 6.25.\n\\end{align*}\n\\] Given the above, we can calculate the Loss with respect to \\(s = 0.75\\), which is given by\n\\[\n\\begin{align*}\n\\mathcal{L}(y,\\hat y) &= \\sum_{i:x_{i}\\in R_1} (y_i-\\hat y_{R_1})^2 + \\sum_{i:x_{i}\\in R_2} (y_i-\\hat y_{R_2})^2 \\\\\n&= (7 -7)^2 + (10-6.25)^2 + (8-6.25)^2 + (3-6.25)^2 + (4-6.25)^2\\\\\n&= 32.75\n\\end{align*}\n\\]\n\nloss_x(data,1)\n\n[1] 32.75\n\n\nFor \\(r=2\\) we have \\(s = 1.5\\) and \\[\n\\begin{align*}\nR_1(1,1.5) &= \\{x: x \\leq 1.5\\} = \\{0.5,1.0\\},\\\\\nR_2(1,1.5) &= \\{x: x &gt; 1.5\\} = \\{2.5,5.5,4.5\\}.\n\\end{align*}\n\\] Then, \\[\n\\begin{align*}\n\\hat y_1 &= \\frac{1}{|R_1|}\\sum_{i:x_{i}\\in R_1} y_i = \\frac{1}{2}\\cdot (7+10) = 8.5,\\\\\n\\hat y_2 &= \\frac{1}{|R_2|}\\sum_{i:x_{i}\\in R_2} y_i = \\frac{1}{3}(8+3+4) = 5.\n\\end{align*}\n\\] Given the above, we can calculate the Loss with respect to \\(s = 1.5\\), which is given by\n\\[\n\\begin{align*}\n\\mathcal{L}(y,\\hat y) &= \\sum_{i:x_{i}\\in R_1} (y_i-\\hat y_{R_1})^2 + \\sum_{i:x_{i}\\in R_2} (y_i-\\hat y_{R_2})^2 \\\\\n&= (7-8.5)^2 + (10-8.5)^2  + (8-5)^2 + (3-5)^2 + (4-5)^2\\\\\n&= 18.5\n\\end{align*}\n\\]\n\nloss_x(data,2)\n\n[1] 18.5\n\n\nFor \\(r=3\\) we have \\(s = 3.25\\) and \\[\n\\begin{align*}\nR_1(1,3.25) &= \\{x: x \\leq 3.25\\} = \\{0.5,1.0, 2.5 \\},\\\\\nR_2(1,3.25) &= \\{x: x &gt; 3.25\\} = \\{5.5,4.5\\}.\n\\end{align*}\n\\] Then, \\[\n\\begin{align*}\n\\hat y_1 &= \\frac{1}{|R_1|}\\sum_{i:x_{i}\\in R_1} y_i = \\frac{1}{3}\\cdot (7+10+8) = 8.333,\\\\\n\\hat y_2 &= \\frac{1}{|R_2|}\\sum_{i:x_{i}\\in R_2} y_i = \\frac{1}{2}(3+4) = 3.5.\n\\end{align*}\n\\] Given the above, we can calculate the Loss with respect to \\(s = 4\\), which is given by\n\\[\n\\begin{align*}\n\\mathcal{L}(y,\\hat y) &= \\sum_{i:x_{i}\\in R_1} (y_i-\\hat y_{R_1})^2 + \\sum_{i:x_{i}\\in R_2} (y_i-\\hat y_{R_2})^2 \\\\\n&= (7-8.333)^2 + (10-8.333)^2  + (8-8.333)^2 + (3-3.5)^2 + (4-3.5)^2\\\\\n&= 5.167\n\\end{align*}\n\\]\n\nloss_x(data,3)\n\n[1] 5.166667\n\n\nFor \\(r=4\\) we have \\(s = 5\\) and \\[\n\\begin{align*}\nR_1(1,5) &= \\{x: x \\leq 5\\} = \\{0.5,1.0, 2.5,4.5 \\},\\\\\nR_2(1,5) &= \\{x: x &gt; 5\\} = \\{5.5\\}.\n\\end{align*}\n\\] Then, \\[\n\\begin{align*}\n\\hat y_1 &= \\frac{1}{|R_1|}\\sum_{i:x_{i}\\in R_1} y_i = \\frac{1}{4}\\cdot (7+10+8+4) = 7.25,\\\\\n\\hat y_2 &= \\frac{1}{|R_2|}\\sum_{i:x_{i}\\in R_2} y_i = \\frac{1}{1}\\cdot 3 = 3.\n\\end{align*}\n\\] Given the above, we can calculate the Loss with respect to \\(s = 5\\), which is given by\n\\[\n\\begin{align*}\n\\mathcal{L}(y,\\hat y) &= \\sum_{i:x_{i}\\in R_1} (y_i-\\hat y_{R_1})^2 + \\sum_{i:x_{i}\\in R_2} (y_i-\\hat y_{R_2})^2 \\\\\n&= (7-7.25)^2 + (10-7.25)^2  + (8-7.25)^2 + (4-7.25)^2 + (3-3)^2\\\\\n&= 18.75\n\\end{align*}\n\\]\n\nloss_x(data,4)\n\n[1] 18.75\n\n\nSince \\(\\mathcal{L}(y,\\hat y)\\) is the lowest for \\(r=3\\), i.e., \\(\\mathcal{L}(y,\\hat y) = 5.167\\), \\(s = 3.25\\) is the optimal splitting point with respect to \\(x\\).\n\n\n\n\n5.1.2.2 Exercise 1b ii:\nGiven the tibble data, create a simple scatter plot and add a dashed line indicating the initial splitting point. An example of what such a plot could look like can be found below.\n\ntitle_text = \"Scatterplot showing the &lt;span style='color:red'&gt;optimal threshold&lt;/span&gt; for an initial split with respect to x\"\n\ndata %&gt;% ggplot(aes(x,y))+\n  geom_point(size = 3, alpha = 0.7) +\n  geom_vline(xintercept = 3.25, linetype = \"dashed\", color = \"red\") +\n  theme_minimal()+\n  theme(\n    plot.title = element_markdown()\n  )+\n  labs( x = \"x\",\n        title = title_text)\n\n\n\n\n\n\n\n\n\n\n5.1.2.3 Exercise 1b iii:\nCalculate the improvement of the given split. Recall, that the improvement of a split is given by\n\\[\n\\frac{\\mathrm{MSE}_1 \\cdot n_1 - (\\mathrm{MSE}_2 \\cdot n_2 +  \\mathrm{MSE}_3 \\cdot n_3)}{\\mathrm{MSE_1}\\cdot n_1},\n\\]\nwhere \\(\\mathrm{MSE}_1\\) denotes the mean squared error of the region before the split and \\(\\mathrm{MSE_2}\\) and \\(\\mathrm{MSE_3}\\) are the mean square errors of the respective regions after the split. \\(n_i,\\, i=1,2,3\\) denotes the number of samples in the respective region.\n\n\n\n\n\n\nSolution\n\n\n\nThe improvement is given by the following term.\n\\[\n\\frac{\\mathrm{MSE}_1 \\cdot n_1 - (\\mathrm{MSE}_2 \\cdot n_2 +  \\mathrm{MSE}_3 \\cdot n_3)}{\\mathrm{MSE_1}\\cdot n_1}\n\\]\nCalculating \\(MSE_i\\) for \\(i=1,2,3\\) yields\n\\[\n\\begin{align}\nn_1 \\cdot \\mathrm{MSE}_1 &= (10-6.4)^2 + (7-6.4)^2 + (8-6.4)^2 + (3-6.4)^2 + (4-6.4)^2 = 33.2, \\\\\nn_2 \\cdot \\mathrm{MSE}_2 &= (7-8.333)^2 + (10-8.333)^2  + (8-8.333)^2 = 4.667, \\\\\nn_3 \\cdot \\mathrm{MSE}_3 &= (3-3.5)^2 + (4-3.5)^2 = 0.5\\ .\n\\end{align}\n\\]\nThe improvement for this split is therefore\n\\[\n\\frac{\\mathrm{MSE}_1 \\cdot n_1 - (\\mathrm{MSE}_2 \\cdot n_2 +  \\mathrm{MSE}_3 \\cdot n_3)}{\\mathrm{MSE_1}\\cdot n_1} = \\frac{33.2 - (4.667 + 0.5)}{33.2} = 0.8444\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression Trees</span>"
    ]
  },
  {
    "objectID": "04_regression_trees.html#exercise-2-modelling-and-tuning-a-regression-tree",
    "href": "04_regression_trees.html#exercise-2-modelling-and-tuning-a-regression-tree",
    "title": "4  Regression Trees",
    "section": "5.2 Exercise 2: Modelling and tuning a regression tree",
    "text": "5.2 Exercise 2: Modelling and tuning a regression tree\nIn this exercise, we want to apply our theoretical knowledge to training a tree-based model on the Apartment rental offers in Germany dataset. As in Session 03 we will be using the rental offers in Munich to build a predictive model for the base rent.\n\n5.2.1 Preprocessing and splitting\nGiven the data_muc_filtered dataset, we apply the usual transformations.\n\ndata_muc &lt;- read.csv(\"data/data_muc_filtered.csv\")\n\ndata_muc_filtered &lt;- data_muc %&gt;%\n  select(!c(\"X\",\"serviceCharge\",\"heatingType\",\"picturecount\",\"totalRent\",\n            \"firingTypes\",\"typeOfFlat\",\"noRoomsRange\", \"petsAllowed\",\n            \"livingSpaceRange\",\"regio3\",\"heatingCosts\", \"floor\",\n            \"date\", \"pricetrend\")) %&gt;%\n  na.omit %&gt;%\n  mutate(\n    interiorQual = factor(\n      interiorQual,\n      levels = c(\"simple\", \"normal\", \"sophisticated\", \"luxury\"),\n      labels = c(\"simple\", \"normal\", \"sophisticated\", \"luxury\"),\n      ordered = TRUE\n    ),\n    condition = factor(\n      condition,\n      levels = c(\"need_of_renovation\", \"negotiable\",\"well_kept\",\"refurbished\",\n                 \"first_time_use_after_refurbishment\",\n                 \"modernized\", \"fully_renovated\", \"mint_condition\",\n                 \"first_time_use\"),\n      ordered = TRUE\n    ),\n    geo_plz = factor(geo_plz)\n  ) %&gt;%\n  mutate_if(is.logical, ~ as.numeric(.)) %&gt;%\n  filter(baseRent &lt;= 4000, livingSpace &lt;= 200)\n\n\nset.seed(1)\nsplit &lt;- initial_split(data_muc_filtered)\ndata_train &lt;- training(split)\ndata_test &lt;- testing(split)\nfolds &lt;- vfold_cv(data_train, v = 10)\n\n\n\n5.2.2 Exercise 2a:\n\n5.2.2.1 Exercise 2a i: Creating a recipe\nCreate a recipe rec_rt with the following properties:\n\nAs parameters, directly pass a formula where you fit every feature on the variable base_rent and as data the data_train dataset.\nUpdate the feature scoutId to a new role called ID.\nCreate ordinal scores for the features interiorQual and condition.\nCreate dummy variables for the feature geo_plz.\nRemove all zero variance features, i.e. all features that only contain a single value.\n\nYou can check whether you successfully created the recipe by calling rec_rt and comparing it to the output below.\n\nrec_rt &lt;- recipe(\n    formula = baseRent ~., \n    data = data_train\n  ) %&gt;%\n  update_role(scoutId, new_role = \"ID\") %&gt;%\n  step_ordinalscore(interiorQual, condition)%&gt;%\n  step_dummy(geo_plz)%&gt;%\n  step_zv(all_predictors())\n\n\n\n5.2.2.2 Exercise 2a ii: Specifying the model\nCreate an instance of the decision_tree class called model_rt_tune with the parameters min_n = tune(),tree_depth = tune(), and cost_complexity = tune(). Set the mode to \"regression\" and the engine to \"rpart\". If calling model_rt_tune returns the same output as the one below, you have successfully solved this exercise.\n\nmodel_rt_tune &lt;-  \n  decision_tree(\n    min_n = tune(),\n    tree_depth = tune(),\n    cost_complexity = tune()\n    ) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"rpart\")\nmodel_rt_tune\n\nDecision Tree Model Specification (regression)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = tune()\n\nComputational engine: rpart \n\n\n\n\n5.2.2.3 Exercise 2a iii: Creating a workflow and specifying metrics\nCreate a metric set called multi_metric with rmse and rsq as arguments and create a workflow called rt_wflow to which you add the model_rt_tune model and the rec_rt recipe. If calling rt_wflow and multi_metric returns the same output as the one below, you have successfully solved this exercise.\n\nrt_wflow &lt;- \n  workflow() %&gt;%\n  add_model(model_rt_tune) %&gt;%\n  add_recipe(rec_rt)\nrt_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_ordinalscore()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (regression)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = tune()\n\nComputational engine: rpart \n\nmulti_metric &lt;- metric_set(rmse,rsq)\nmulti_metric\n\nA metric set, consisting of:\n- `rmse()`, a numeric metric | direction: minimize\n- `rsq()`, a numeric metric  | direction: maximize\n\n\n\n\n5.2.2.4 Exercise 2a iv: Setting up a tune-grid\nNow, that we have set up the model, specified the recipe, and created a workflow, we need to define a grid for the grid search of the tuning parameters min_n,tree_depth, and cost_complexity. Instead of defining one manually, we can use the grid_regular function of the {dials} package instead. The grid_regular function creates a tibble with one column for each parameter and a row for every parameter combination. One advantage of this function is, that we can pass our tuning parameters directly into it and the grid_regular function creates a range of valid values. Consider the following example: The min_n parameter, which determines the minimum number of data points in a node that is required for the node to be split further, takes values between \\(0\\) and \\(40\\). Calling the grid_regular function with min_n() and levels = 5, creates a grid with five values ranging from \\(0\\) to \\(40\\), as you can see in the code chunk below.\n\ngrid_regular(min_n(),levels = 5)\n\n# A tibble: 5 × 1\n  min_n\n  &lt;int&gt;\n1     2\n2    11\n3    21\n4    30\n5    40\n\n\nInstead of passing the tuning parameters manually as we have done in the example above, we can also simply use the extract_parameter_set_dials function, which returns all the parameters we have specified for the tuning process.\nCreate an instance of the grid_regular object with 4 levels, by passing the rt_wflow workflow to the extract_parameter_set_dials function and passing this to the grid_regular function with levels = 4 as an additional input parameter. If calling head(tree_grid) returns the same output as the one below, you have successfully solved this exercise.\n\ntree_grid &lt;- rt_wflow %&gt;% \n  extract_parameter_set_dials %&gt;% \n  grid_regular(levels = 4)\nhead(tree_grid)\n\n# A tibble: 6 × 3\n  cost_complexity tree_depth min_n\n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt;\n1    0.0000000001          1     2\n2    0.0000001             1     2\n3    0.0001                1     2\n4    0.1                   1     2\n5    0.0000000001          5     2\n6    0.0000001             5     2\n\n\n\n\n\n5.2.3 Exercise 2b: Training the model and evaluating the results\n\n5.2.3.1 Exercise 2b i: Model training\nGiven our workflow, the grid for hyperparameter tuning, the metric set, and resampling specification, we can finally train our model using the tune_grid function.\nCreate an instance called rt_res of the tune_grid function, by piping the rt_wflow object to the tune_grid function. As parameters for the tune_grid function, use grid = tree_grid, metrics = multi_metric, and resamples = folds.\n\nWarning: Training the model might take a few minutes, as we are effectively training 64 tree models.\n\n\nrt_res &lt;- \n  rt_wflow %&gt;% \n  tune_grid(\n    grid = tree_grid,\n    metrics = multi_metric,\n    resamples = folds\n  )\n\n\n\n5.2.3.2 Exercise 2b ii: Interpreting the results\nGiven the following plot. What can you say about the relationship between the tree parameters Tree Depth, Minimal Node Size, and Cost-Complexity Parameter with respect to the RMSE?\n\nautoplot(rt_res) + theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nIf the tree depth parameter is set to \\(1\\), the other two parameters do not seem to have an impact on the RMSE and it therefore remains constant at around \\(500\\) EUR. So, from now on, we will only consider the case where tree depth &gt; 1.\nAn increasing complexity parameter (CP) generally leads to a higher RMSE, except for the case where minimal node size = 2.\nIf CP\\(\\leq 10^{-4}\\), the CP does not seem to influence model performance. Only if the Cost-Complexity Parameter \\(\\approx 0.1\\), the RMSE increases noticeably.\nA point can be made, that the best performing model has a minimal node size of either \\(14\\) or \\(27\\), however, the difference in RMSE is only marginal for these two cases.\n\n\n\n\n5.2.3.3 Exercise 2b iii: Selecting the best model\nGiven the rt_res tibble, use the select_best function to select the best model based on the metric rmse and save it under the name rt_res_best.\n\nrt_res_best&lt;- rt_res %&gt;%\n  select_best(metric = \"rmse\")\n\n\n\n\n5.2.4 Exercise 2c: Evaluating the best model\nNow, that we have successfully selected the best model based on the cross-validation approach, the next step is to evaluate it on our test set data_test.\n\n5.2.4.1 Exercise 2c i:\nCreate a model last_rt_fit by piping the rt_wflow to the finalize_worflow function, where you pass the previously created rt_res_best as an argument and pipe this finalized workflow to the last_split function, where you pass the split object as an argument.\nOnce you have fit the last model, utilize the collect_metrics function to obtain the metrics of the model on the dataset data_test.\n\nlast_rt_fit &lt;- \n  rt_wflow %&gt;%\n  finalize_workflow(rt_res_best) %&gt;%\n  last_fit(split)\n\nlast_rt_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     376.    Preprocessor1_Model1\n2 rsq     standard       0.717 Preprocessor1_Model1\n\n\n\n\n5.2.4.2 Exercise 2c ii:\nIt is usually easier to get a feeling for model performance by visualizing the results. One way to do that would be to plot the predicted values of our model against the actual values. By adding a simple line through the origin with slope one, we can then evaluate the estimates as follows:\nPoints that are closely scattered around this line have been well predicted, whereas points further away from this line indicate that the model performed badly.\nThe goal of this exercise is for you to rebuild the plot that is depicted below.\n\ntitle_text &lt;- \"Predictions of the test set plotted against the actual values\"\nlast_rt_fit %&gt;%\n  collect_predictions() %&gt;%\n  ggplot(aes(baseRent, .pred)) +\n  geom_abline(slope = 1, lty = 2, color = \"red\", alpha = 1) +\n  geom_point(alpha = 0.6, color = \"green\")+\n  labs(\n    x = \"True base rent\",\n    y = \"Estimated base rent\",\n    title = title_text\n  )+\n  coord_fixed()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression Trees</span>"
    ]
  }
]