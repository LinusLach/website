[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Supplementary Material",
    "section": "",
    "text": "Preface\nThis exercise manuscript supplements the lecture notes provided for Prof. Dr. Yarema Okhrin’s lecture Machine Learning at the University of Augsburg. Please note that this is still a work in progress and subject to change.\nThe idea to develop such a manuscript stems from Albert Rapp who also helped me starting this project.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-this-manuscript-is",
    "href": "index.html#what-this-manuscript-is",
    "title": "Machine Learning Supplementary Material",
    "section": "What this manuscript is",
    "text": "What this manuscript is\nThe manuscript intends to provide more context to different areas usually neglected in lecture and exercise sessions. The exercise sessions can especially suffer from an imbalance between repeating the theoretical aspects of the lecture and applying the concepts thoroughly. Moreover, this manuscript is comprehensive, containing every exercise and solution presented in the exercise sessions. The solutions will be more detailed than the ones presented in the in-person sessions, which can help if the goal is to partake in the exam.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-this-manuscript-isnt",
    "href": "index.html#what-this-manuscript-isnt",
    "title": "Machine Learning Supplementary Material",
    "section": "What this manuscript isn’t",
    "text": "What this manuscript isn’t\nThis manuscript generally lacks is the interaction between the students and lecturers which I believe is an important aspect of the in-person exercise sessions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_prerequisites.html",
    "href": "01_prerequisites.html",
    "title": "1  Important R concepts",
    "section": "",
    "text": "1.1 Setting up R\nMost exercises involve working with the statistical software R. Since a comprehensive recap of R would go beyond the scope of this lecture, we only review libraries and concepts we frequently need throughout the exercises. For a comprehensive introduction to R, see YARDS by Dr. Albert Rapp.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "01_prerequisites.html#setting-up-r",
    "href": "01_prerequisites.html#setting-up-r",
    "title": "1  Important R concepts",
    "section": "",
    "text": "1.1.1 Installing R, RStudio, and getting started with Quarto\nBefore starting the revision, we need to ensure that R and RStudio are installed. The installation process for both R and RStudio is straightforward and user-friendly. While R (the programming language) comes with a preinstalled graphical user interface, we will use the integrated development environment RStudio instead, as it looks more appealing and provides some features RGui lacks. It is important to note that in order to get RStudio to work, R needs to be installed first.\n\nR for Windows can be downloaded here.\nR for MacOS/Unix based systems can be downloaded here\nRStudio can be downloaded here.\n\nAfter successfully installing R and RStudio, starting the latter should open a window that somewhat looks like the following (cf. RStudio User Guide).\n\n\n\n\n\nThe Source pane displays a .R file named ggplot2.R. While .R files are the standard file format for R scripts used for programming in R, we will use Quarto documents (files ending with .qmd). Quarto labels itself as the\n\nnext-generation version of R Markdown,\n\nmeaning that a Quarto document allows\n\nweaving together narrative text and code to produce elegantly formatted output as documents, web pages, books, and more.\n\nQuarto is preinstalled in RStudio, so creating such documents is relatively simple.\n\nclick on New File -&gt; Quarto Document... in the File tab \nOnce the New Quarto Document window opens, you can modify the title and specify the output format. For the sake of simplicity, we will use the HTML output format. However, you could also choose PDF if you have a LaTeX installation on your device. Clicking on the create button will create a new Quarto Document. \nThe source pane now displays a sample Quarto document that can be modified. You might have to install rmarkdown (cf. mark 1). The Quarto document can then be modified by clicking on the respective sections. R Code cells can be executed by clicking on the green arrow (cf. mark 2). To insert new R code cells between paragraphs, click on the Insert tab and select Executable Cell -&gt; R (cf. mark 3).\n\n\n\n\n\n\nThis should be enough to get you started with Quarto Documents. For further reading, I recommend the Quarto Guide.\n\n\n\n\n\n\nTip\n\n\n\n\nWhile solving the (programming) exercises, you can create a new paragraph using # for a new chapter to make your document more readable. Additionally, you can simply create a section using ##.\nWriting down some details on how you approached the programming exercises in a paragraph above or below can help you understand your approach when repeating the exercises later.\n\n\n\n\n\n1.1.2 Working directories and projects\nAfter opening RStudio, execute the getwd() command in the console pane, which returns the current working directory. The working directory displays the directory of the R process. For example, if the return value of the getwd() command is C:\\Users\\lachlinu\\Desktop\\ML, then R can access any file in the ML directory. One way to change the working directory is using the setwd() command, which changes the current working directory. Manually changing the directory in every .qmd document might become tedious after a while, so a more practical alternative is setting up a project. RStudio projects allow the creation of an individual working directory for multiple contexts. For example, you might use R not only for solving Machine Learning exercises but also for your master’s thesis. Then, setting up two different projects will help you organize working directories and workspaces for each project individually. To set up a project for this course\n\nGo to the File tab and select New Project....\n\n\n\n\n\nChoose Existing Directory and navigate to the directory in which you want to create the project in and click the Create Project button.\n\n\n\n\n\nYou can now open the project by double clicking on the icon which should open a new RStudio window.\n\n\n\n\n\n\nOnce the project is opened, running the getwd() command in the console pane returns its path. Any file in this directory can be directly accessed without specifying the preceding path.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "01_prerequisites.html#help-my-code-doesnt-do-what-i-expect-it-to-do",
    "href": "01_prerequisites.html#help-my-code-doesnt-do-what-i-expect-it-to-do",
    "title": "1  Important R concepts",
    "section": "1.2 Help, my code doesn’t do what I expect it to do!",
    "text": "1.2 Help, my code doesn’t do what I expect it to do!\nDebugging, also known as finding out why code does not behave as expected, plays a substantial role in programming. Even after years of practice, occasional bugs will occur. There are several more or less effective approaches to resolve problems that might occur in the context of this course, with debugging being a key focus.\n\nA simple Google search can do wonders. Most questions that arise in the context of this lecture have probably been asked on boards like Stack Overflow. Alternatively, the search results might contain websites like DataCamp or GeeksforGeeks, which will most likely answer your question as well.\nUsing a large language model like ChatGPT. When it comes to debugging or coding questions in general, large language models are helpful. Enter your question as a prompt and the answer will likely help you. However, when using less popular libraries, the answers might contain hallucinations that make it much more difficult to resolve your problem.\nThe internal help function, while it might be considered old-school, is a highly effective troubleshooting approach. It can provide valuable insights when a function doesn’t behave as expected. If a function doesn’t behave as expected, typing ?function_name in the console pane opens the respective help page. The Arguments section of the help page explains the different input parameters of a function. The Value section describes what the function intends to return. Examples of how to use a function are provided in the last section of the help.\nIf each of the steps above fails, you can also ask questions via mail. When reporting a problem via mail, it’s crucial to provide some context and the steps you’ve tried to solve it yourself. This information is invaluable in understanding the issue and providing an effective solution. Also, as part of the learning process, try solving the problems first since that is one of the most essential skill sets to develop.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "01_prerequisites.html#working-with-datasets",
    "href": "01_prerequisites.html#working-with-datasets",
    "title": "1  Important R concepts",
    "section": "1.3 Working with datasets",
    "text": "1.3 Working with datasets\nIn the context of machine learning, data builds the foundation. Whether you want to predict the weather, future stock prices, or the base rent of a potential rental apartment, without high-quality data, even the most advanced models fail. However, the reality is that data we gather from the web, servers, or spreadsheets is often far from pristine. For example, missing values could be encoded as NA (Not Available), NaN (Not a Number), NULL, or simply by an empty string \" \". That is why knowing your way around basic data manipulation is essential.\n\n1.3.1 Importing data\nTo manipulate data, we first need to import it. R has quite a few preinstalled data sets; however, I prefer data sets that are either more complex, related to everyday life, or just more fun to explore. This course will provide most of the data in .csv or .txt files. Before we can start manipulating data, it’s essential to import it correctly. This ensures that the data is in the right format for further analysis.\nConsider the Netflix Movies and TV Shows data set, which can be downloaded from the data science community website Kaggle or directly below.\n\nDownload Netflix Data\n\nOnce you have downloaded the data and placed it in your current working directory, you can import it using the read.csv command:\n\ndata_netflix &lt;- read.csv(\"netflix_titles.csv\")\n\n\n\n\n\n\n\nTipPro Tip\n\n\n\nTo maintain structure in your project folder, it is advisable to create a separate directory for the data and import it from there. For example, if the project directory contains a data directory with the netflix_title.csv file inside, it can be imported using\n\ndata_netflix &lt;- read.csv(\"data/netflix_titles.csv\")\n\n\n\n\n\n1.3.2 Essential functions and libraries\nOne of the most versatile and essential collection of libraries in the context of data science with R is the {tidyverse} library, which includes\n\n{ggplot} for creating beautiful graphics,\n{dplyr} for data manipulation,\n{stringr} for working with strings, and\n{tibble} for storing data effectively.\n\nAn excellent in-depth introduction to the tidyverse called R for Data Science is freely available online if that piques your interest. This introduction focuses on a few core functions that will be useful throughout the course. As every other library, {tidyverse} can be attached using the library function once it has been installed:\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nOnce the library has been added, every function contained is available. For example, the glimpse function can be used on the data_netflix data set to get a short overview of the data types and contents:\n\nglimpse(data_netflix)\n\nRows: 8,807\nColumns: 12\n$ show_id      &lt;chr&gt; \"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"s6\", \"s7\", \"s8\", \"s9\", \"s1…\n$ type         &lt;chr&gt; \"Movie\", \"TV Show\", \"TV Show\", \"TV Show\", \"TV Show\", \"TV …\n$ title        &lt;chr&gt; \"Dick Johnson Is Dead\", \"Blood & Water\", \"Ganglands\", \"Ja…\n$ director     &lt;chr&gt; \"Kirsten Johnson\", \"\", \"Julien Leclercq\", \"\", \"\", \"Mike F…\n$ cast         &lt;chr&gt; \"\", \"Ama Qamata, Khosi Ngema, Gail Mabalane, Thabang Mola…\n$ country      &lt;chr&gt; \"United States\", \"South Africa\", \"\", \"\", \"India\", \"\", \"\",…\n$ date_added   &lt;chr&gt; \"September 25, 2021\", \"September 24, 2021\", \"September 24…\n$ release_year &lt;int&gt; 2020, 2021, 2021, 2021, 2021, 2021, 2021, 1993, 2021, 202…\n$ rating       &lt;chr&gt; \"PG-13\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"PG…\n$ duration     &lt;chr&gt; \"90 min\", \"2 Seasons\", \"1 Season\", \"1 Season\", \"2 Seasons…\n$ listed_in    &lt;chr&gt; \"Documentaries\", \"International TV Shows, TV Dramas, TV M…\n$ description  &lt;chr&gt; \"As her father nears the end of his life, filmmaker Kirst…\n\n\nRows: 8,807 means that the data set has 8,807 entries, and Columns: 12 means that the data set has 12 variables, respectively. The first column presents the variable names, their data types, and some initial values, providing a clear structure to the data set. We can already see that except for one variable (release_year), every other variable is of type chr, which stands for character or string.\n\n1.3.2.1 Filtering, grouping, and summarizing data sets\nFunctions frequently encountered while working with data are filter, group_by, and summarise. Let’s say we want to find out, according to the data set, how many movies and series were released in each year following 2010. Now, if we were to tackle this problem without the {tidyverse} framework, our code might look a little something like this:\n\nnetflix_filtered &lt;- data_netflix[data_netflix$release_year &gt; 2010, ]\nresult &lt;- aggregate(rep(1, nrow(netflix_filtered)), \n                    by = list(netflix_filtered$release_year), \n                    FUN = sum)\ncolnames(result) &lt;- c(\"release_year\", \"n\")\nresult\n\n   release_year    n\n1          2011  185\n2          2012  237\n3          2013  288\n4          2014  352\n5          2015  560\n6          2016  902\n7          2017 1032\n8          2018 1147\n9          2019 1030\n10         2020  953\n11         2021  592\n\n\nor this:\n\nnetflix_filtered &lt;- data_netflix[data_netflix$release_year &gt; 2010, ]\nresult &lt;- as.data.frame(table(netflix_filtered$release_year))\ncolnames(result) &lt;- c(\"release_year\", \"n\")\nresult\n\n   release_year    n\n1          2011  185\n2          2012  237\n3          2013  288\n4          2014  352\n5          2015  560\n6          2016  902\n7          2017 1032\n8          2018 1147\n9          2019 1030\n10         2020  953\n11         2021  592\n\n\nThe first code cell seems much more complicated than the second, yet it returns the same result. However, things can be simplified even more using the {dplyr} library that is contained in the {tidyverse}:\n\nnetflix_filtered &lt;- data_netflix %&gt;%\n  filter(release_year&gt;2010) %&gt;%\n  group_by(release_year) %&gt;%\n  summarise(n= n())\n\nLet us break down the code snippet above:\n\nIn line 1 we use the pipe operator %&gt;%. It is part of the {magrittr} package and forwards an object into a function of call expression. Figuratively, a pipe does precisely what is expected: channel an object from one and to another. In this case, the pipe operator %&gt;% passes the data_netflix data set into the filter function.\nIn line 2 the filter function selects a subset of a data set that satisfies a given condition. Here, the condition is that the movie or series’ release year should be after 2010, which is indicated by the &gt; condition.\n\n\n\n\n\n\nNote\n\n\n\nWithout the pipe operator, the first and second line can be merged into\n\nfilter(data_netflix,release_year&gt;2010)\n\nhowever, concatenating multiple functions causes the code to be unreadable and should thus be avoided.\n\n\nResults of the filtering procedure are then passed to the group_by function via the pipe operator again. The group_by function converts the underlying data set into a grouped one where operations can be performed group-wise. In the third line of the code cell, the group_by function is applied to the release_year variable, meaning that the data set now contains a group for every release year.\nThis can be seen as a pre-processing step for the summarise function applied in the following line. The summarise function creates a new data set based on the functions passed as arguments. These functions are applied to every group created by the previous step. In the example above, the function applied is n(), which returns the group size. Thus, setting n=n() as the argument creates a new column named n, which contains the number of samples within each group.\n\n\n\n\n1.3.3 Mutating data sets\nBesides filtering, grouping, and summarizing, another important concept is mutating the data, i.e., modifying the content of the data set.\nThe mutate function either creates new columns or modifies existing columns based on the passed column names and functions that are passed. It is helpful for modifying data and their types, creating new variables based on existing ones, and removing unwanted variables. In the example below, the mutate function is used to modify the variable date_added, create a new variable called is_show and delete the variable type.\n\ndata_netflix &lt;- data_netflix %&gt;% \n  mutate(date_added = mdy(date_added),\n         is_show = if_else(type==\"TV Show\",TRUE,FALSE),\n         type = NULL\n         )\n\n\nIn the first line of the code cell, the data set data_netflix is specified to be overwritten by its mutated version. Overwriting a data set is achieved by reassigning the data_netlifx object to the output of the pipe concatenation of the following code lines.\nThe original data_netflix data set is passed into the mutate function in the second line. Here, the variable date_written is overwritten by the output of the mdy function with argument date_added. The mdy function is a function in the {lubridate} library that transforms dates stored in strings to date objects that are easier to handle. Note that we can directly pass column names into functions as we have previously passed the data set into the mutate function using the %&gt;% operator.\nIn the third line, a new variable is_show is created, which takes the value TRUE, if the type of an entry in the data set is \"TV Show\" and FALSE if it is not. The if_else function achieves this.\nSetting the type variable to NULL effectively removes it from the data set.\n\n\n\n\n\n\n\nNote\n\n\n\nAssigning values in functions is achieved by using the = symbol. Assigning new variables outside of functions can also be done with the = symbol, but it is rarely used and except for some pathological cases there is no difference. However, most R users prefer assigning environment variables using &lt;- which does not work in function calls.\n\n\n\n\n\n\n\n\nTipPro Tip\n\n\n\nIn the previous code cell a line break was added after the %&gt;% and each argument in the mutate function for readability purposes. The code also works without adding the line breaks, but it can get messy fast:\n\ndata_netflix &lt;- data_netflix %&gt;% mutate(date_added = mdy(date_added),...)\n\n\n\n\n\n1.3.4 Factor Variables\nAn important data type that can handle both ordinal (data with some notion of order) and nominal data are so-called factor variables.\nConsider the following toy data set containing seven people with corresponding age groups and eye colors.\n\ndata_example &lt;- tibble(\n names = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\", \"Frank\", \"Grace\"),\n age_groups = c(\"18-25\", \"&lt;18\", \"26-35\", \"36-45\", \"18-25\", \"60+\", \"26-35\"),\n eye_color = c(\"Blue\", \"Brown\", \"Green\", \"Hazel\", \"Brown\", \"Blue\", \"Green\")\n)\n\ndata_example \n\n# A tibble: 7 × 3\n  names   age_groups eye_color\n  &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;    \n1 Alice   18-25      Blue     \n2 Bob     &lt;18        Brown    \n3 Charlie 26-35      Green    \n4 Diana   36-45      Hazel    \n5 Eve     18-25      Brown    \n6 Frank   60+        Blue     \n7 Grace   26-35      Green    \n\n\nSince the variable age_group only specifies a range of ages, it does not make sense to encode them as integers rather than ordinal variables. The’ mutate’ function can encode age groups as ordinal variables. This involves setting the age_groups variable to a factor with levels and labels. Levels specify the order of the values, and labels can be used to rename these categories.\n\ndata_example &lt;- data_example %&gt;%\n  mutate( \n    age_groups = factor(\n      age_groups,\n      levels = c(\"&lt;18\", \"18-25\", \"26-35\", \"36-45\", \"60+\"),\n      labels = c(\"child\",\"adult\",\"adult\",\"adult\",\"senior\"),\n      ordered = TRUE\n    )\n  )\n\n\nSimilar to the previous example, we should specify that we overwrite the data_example data set with a mutated version.\nThe mutate function is applied to the age_groups variable.\nSetting age_groups = factor(age_groups, ...) converts the age_groups column into a (so far unordered) factor, allowing for specific levels (categories) and labels.\nlevels = c(\"&lt;18\", \"18-25\",...) specifies the predefined levels for the age groups.\nordered=TRUE specifies that the age groups are ordered according to the specified levels.\nLast but not least, labels = c(\"child\", \"adult\", ...) specifies the labels that replace the numeric age groups. For instance, &lt;18 is labeled as \"child\", the ranges 18-25, 26-35, and 36-45 are labeled as \"adult\", and 60+ is labeled as \"senior\".\n\nSimilarly, the variable eye_color can also be converted to a nominal factor variable:\n\ndata_example &lt;- data_example %&gt;%\n  mutate(\n    eye_color = factor(eye_color)\n  )\n\nTo confirm that the variable age_group is indeed ordered, we can print the levels in ascending. The levels-function does exactly that:\n\nlevels(data_example$age_groups)\n\n[1] \"child\"  \"adult\"  \"senior\"\n\n\nNote, that we used the $ selector since the levels-function can’t extract the levels from a list of factors:\n\ndata_example %&gt;%\n  select(age_groups) %&gt;%\n  levels()\n\nNULL\n\n\nBy checking the types of the respective objects, we can confirm our suspicion:\n\ndata_example %&gt;%\n  select(age_groups) %&gt;%\n  typeof()\n\n[1] \"list\"\n\ndata_example$age_groups %&gt;%\n  typeof()\n\n[1] \"integer\"\n\n\nTo extract the raw values from the list, we can use the pluck()-function:\n\ndata_example %&gt;%\n  select(age_groups) %&gt;%\n  pluck(\"age_groups\") %&gt;%\n  typeof()\n\n[1] \"integer\"\n\n\nBy passing the column name, either as a string or the position as an integer (2 in this case), we can extract the raw values. Checking the data type subsequently confirms that we can now apply the levels() function:\n\ndata_example %&gt;%\n  select(age_groups) %&gt;%\n  pluck(\"age_groups\") %&gt;%\n  levels()\n\n[1] \"child\"  \"adult\"  \"senior\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "01_prerequisites.html#visualizing-data-with-ggplot",
    "href": "01_prerequisites.html#visualizing-data-with-ggplot",
    "title": "1  Important R concepts",
    "section": "1.4 Visualizing data with ggplot",
    "text": "1.4 Visualizing data with ggplot\nAnother critical aspect of data science and machine learning is graphical storytelling. Describing an algorithm strictly using mathematical notation or exploring a data set using descriptive and inductive statistics alone can make it challenging to understand the message. While R offers some base functions for creating graphics, this course primarily uses the library {ggplot2}. A comprehensive introduction to {ggplot2} can be found in Hadley Wickham’s book Elegant Graphics for Data Analysis. A short summary can be found below.\nFor the following example, we will use the netflix_filtered data set (see Section 1.3.2.1)\nA graphic created with {ggplot2} consists of the following three base components:\n\nThe data itself.\n\nggplot(data = netflix_filtered)\n\n\n\n\n\n\n\n\nNote, that the plot does not show any axis, ticks, and variables.\nA set of aesthetics mappings that describe how variables in the data are mapped to visual properties.\n\nggplot(aes(x=release_year, y=n), data = netflix_filtered)\n\n\n\n\n\n\n\n\nUsing the aes function, we have specified that the release year should be mapped to the \\(x\\)-axis, and \\(n\\) to the \\(y\\)-axis.\nLastly, the geom-layer (component) describes how each observation in the data set is represented.\n\nggplot(aes(x=release_year, y=n), data = netflix_filtered)+\n  geom_col()+\n  geom_point()+\n  geom_line()\n\n\n\n\n\n\n\n\nCompared to the previous two code cells, a lot is going on here. So, let us break it down.\n\nThe plus at the end of line 1 is used to add another layer.\ngeom_col adds a column chart to the canvas, creating columns starting at 0 and ending at \\(n\\). Then, + indicates that another layer is added.\ngeom_point represents the data as points on the plane, i.e., an \\(x\\) and \\(y\\)-coordinate. The + indicates that yet another layer is added afterward.\nLastly, the geom_line function adds a line connecting each data point with the one following.\n\n\n\n\n\n\n\nTipPro Tips\n\n\n\n\nAs before, the data set can also directly be piped into the ggplot function:\n\nnetflix_filtered %&gt;%\nggplot(aes(x=release_year, y=n))+\n geom_col()+\n geom_point()+\n geom_line()\n\nBy changing the order of the layers, you can specify which layer should be added first and last. In this example, since geom_col was added first and every other layer is placed on top of the column plot.\n\n\n\n\nThere are a lot more functions and settings that can be applied to each function. A selection of those is discussed in the exercises.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "01_prerequisites.html#exercises",
    "href": "01_prerequisites.html#exercises",
    "title": "1  Important R concepts",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\nThroughout the exercises, we will work with the Credit Card Customers data set that can either be downloaded using the provided link or the button below.\n\nDownload BankChurners\n\nThe data set consists of 10,127 entries that represent individual customers of a bank including but not limited to their age, salary, credit card limit, and credit card category.\n\n1.5.1 Statistical Data Exploration and Manipulation\nWe will start by getting a feeling for the data and performing some basic data manipulation steps.\n\nExercise 1.1 Import the data set and use the glimpse function to generate a summary of the data set.\n\n\nExercise 1.2 Assume that the data set has been imported and saved as an object called credit_info. Explain the following code snippet both syntactically and semantically. Hint: Use the help function for any function you do not know.\n\ncredit_info %&gt;%\n  select_if(is.character) %&gt;%\n  sapply(table)\n\n$Attrition_Flag\n\nAttrited Customer Existing Customer \n             1627              8500 \n\n$Gender\n\n   F    M \n5358 4769 \n\n$Education_Level\n\n      College     Doctorate      Graduate   High School Post-Graduate \n         1013           451          3128          2013           516 \n   Uneducated       Unknown \n         1487          1519 \n\n$Marital_Status\n\nDivorced  Married   Single  Unknown \n     748     4687     3943      749 \n\n$Income_Category\n\n       $120K +    $40K - $60K    $60K - $80K   $80K - $120K Less than $40K \n           727           1790           1402           1535           3561 \n       Unknown \n          1112 \n\n$Card_Category\n\n    Blue     Gold Platinum   Silver \n    9436      116       20      555 \n\n\n\n\nExercise 1.3 Overwrite the variables Income_Category and Education_Level into ordered factors. When setting the levels for each group, set \"Unknown\" as the lowest level. Use this cleaned data set for the remaining exercises.\n\n\nExercise 1.4 Group the data set by income category and find out each group’s mean and median credit limit.\n\n\nExercise 1.5 Which income group has the highest mean credit limit?\n\n\nExercise 1.6 Use the following code snippet to modify the data set by incorporating it into the mutate function. The snippet converts all \"Unknown\" values contained in character or factor columns into NA values, which are easier to handle.\n\nacross(where(~ is.character(.) | is.factor(.)),\n       ~na_if(.,\"Unknown\"))\n\n\n\nExercise 1.7 Apply the na.omit() function to the data set to remove all samples in the data set that contain NA values. How many samples have been removed in total?\n\nSometimes, we only want to infer results for specific subgroups. The Blue Credit Card is the most common type of credit card. Gaining insights for this particular group allows us to retrieve information that might be useful in later analyses.\n\nExercise 1.8 Find out how many customers have a Blue credit card.\n\n\nExercise 1.9 Create a new data set credit_info_blue containing all customers that hold a Blue credit card.\n\n\nExercise 1.10 Find the number of female customers holding the Blue Card who are, at most, 40 years old and have a credit limit above 10,000 USD.\n\n\n\n1.5.2 Visual Data Exploration\n\nExercise 1.11 We now want to explore some of the demographics in our data set. Create a histogram for the age of the customers using the geom_histogram function. Note that only one variable is required for the aesthetics to create a histogram.\n\n\nExercise 1.12 Using the default parameters in the geom_histogram function, the message “stat_bin() using bins = 30. Pick better value with binwidth.” is displayed. Modify the code so that each age gets its own bin.\n\n\nExercise 1.13 Now that the histogram looks more organized, we want to add more information. For example, by setting the fill option to Gender, we can create two overlapping histograms showing the number of male and female customers within each age group.\n\n\nExercise 1.14 Instead of visualizing the Gender as in the plot above, we now want to analyze the continuous variable Credit_Limit. Therefore, instead of a histogram, use the geom_density function that plots an estimate of the underlying probability density.\n\n\nExercise 1.15 The histograms and density plots only provide limited insight into the demographics and customer status as it is relatively complex to figure out the proportions of each group. To take this one step further, consider the following histogram, which shows the Education_Level within every bin.\n\nggplot(data = credit_info_clean, aes(Customer_Age, fill = Education_Level))+\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nWe can use the geom_histogram function and the facet_wrap function, which generates a subplot for each group. Apply the facet_wrap function to create a subplot for each education level.\n\n\n\n1.5.3 Loss functions\nIn future exercises, different loss functions will be deployed to measure how far some regression results deviate from actual values. This exercise, therefore, briefly discusses the advantages and disadvantages of some loss functions and introduces them in R.\nData-wise, we will consider the credit_info dataset and a simple linear model that is used to predict each customer’s credit limit.\nThe following Code snippet reads the unmodified data, removes the features Total_Revolving_Bal and Avg_Open_To_Buy and trains a linear model with target variable Credit_Limit on all the remaining features. It’s important to note that the model is intentionally kept simple for demonstrative purposes, making it easier for you to grasp and apply the concepts.\nCopy the snippet into your own notebook and run it. Hint: You might have to change the path in the read.csv function to your specified data path (Exercise 2.1) and install the libraries that are attached.\n\nlibrary(tidymodels)\nlibrary(yardstick)\n\ncredit_info &lt;- read.csv(\"data/BankChurners.csv\")\n\nmodel_linear_data &lt;- credit_info %&gt;%\n  select(-c(Total_Revolving_Bal,Avg_Open_To_Buy))\n\nmodel_linear_res &lt;- linear_reg() %&gt;%\n  fit(Credit_Limit ~., data = model_linear_data) %&gt;%\n  augment(model_linear_data)\n\nThe object model_linear_res now contains our model’s original data set and predictions. Do not worry if you do not understand every line in the snippet above. We will consider training models in future exercises more thoroughly.\n\n1.5.3.1 MAE Loss\nThe first loss function we explore is the Mean Absolute Error (MAE) loss defined as\n\\[\\begin{equation*}\n  \\mathrm{MAE} := \\mathrm{MAE}(y,\\hat{y}):=\\frac{1}{n}\\sum_{i=1}^n |y_i-\\hat{y_i}|,\n\\end{equation*}\\]\nwhere \\(y=(y_1,...,y_n)\\) are target values and \\(\\hat{y}=(\\hat{y_1},...,\\hat{y_n})\\) are estimates of the target values.\n\nExercise 1.16 Briefly explain how the MAE loss can be interpreted regarding the target features scale.\n\n\nExercise 1.17 The mae loss is a function in the {yardstick} library. If not already done, install the {yardstick} library and read the help function of the mae function. Then, apply it tot the model_linear_res data set and interpret the result.\n\n\n\n1.5.3.2 (R)MSE\nAnother widely used loss function is the (Root)MeanSquareError. It is defined as\n\\[\\begin{align*}\n  \\mathrm{RMSE} &:= \\mathrm{RMSE}(y,\\hat{y}) := \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2}\\\\\n  \\mathrm{MSE} &:= \\mathrm{MSE}(y,\\hat{y}) := \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2\n\\end{align*}\\]\n\nExercise 1.18 Repeat the exercise Exercise 1.16 and Exercise 1.17 for the RMSE and MSE.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "01_prerequisites.html#solutions",
    "href": "01_prerequisites.html#solutions",
    "title": "1  Important R concepts",
    "section": "1.6 Solutions",
    "text": "1.6 Solutions\n\nSolution 1.1 (Exercise 2.1). \n\ncredit_info &lt;- read.csv(\"data/BankChurners.csv\")\n\n\n\nSolution 1.2 (Exercise 2.2). \n\ncredit_info %&gt;%\n  select_if(is.character) %&gt;%\n  sapply(table)\n\n$Attrition_Flag\n\nAttrited Customer Existing Customer \n             1627              8500 \n\n$Gender\n\n   F    M \n5358 4769 \n\n$Education_Level\n\n      College     Doctorate      Graduate   High School Post-Graduate \n         1013           451          3128          2013           516 \n   Uneducated       Unknown \n         1487          1519 \n\n$Marital_Status\n\nDivorced  Married   Single  Unknown \n     748     4687     3943      749 \n\n$Income_Category\n\n       $120K +    $40K - $60K    $60K - $80K   $80K - $120K Less than $40K \n           727           1790           1402           1535           3561 \n       Unknown \n          1112 \n\n$Card_Category\n\n    Blue     Gold Platinum   Silver \n    9436      116       20      555 \n\n\n\nIn the first line, the data set credit_info is passed to the following line.\nThe credit_info data set is passed into the select_if function that selects columns of the data set based on some condition passed in the arguments. In this case, the condition is the is.character function, that checks, whether a column is of type chr. The results are then piped into the following line.\nIn the third line, the selected columns are passed into the sapply function, that applies a given function column wise to a data set and returns the resulting data set. Here, the table function is applied generating a contingency table of the counts for each column.\n\n\n\nSolution 1.3 (Exercise 1.3). \n\ncredit_info_clean &lt;-credit_info %&gt;%\n  mutate(Income_Category = factor(Income_Category,\n                                  levels = c(\"Unknown\",\"Less than $40K\",\n                                            \"$40K - $60K\",\"$60K - $80K\",\n                                            \"$80K - $120K\",\"$120K +\"),\n                                  ordered = TRUE),\n         Education_Level = factor(Education_Level,\n                                  levels = c(\"Unknown\",\"Uneducated\",\n                                             \"High School\",\"College\",\n                                             \"Graduate\", \"Post-Graduate\",\n                                             \"Doctorate\")\n                                  )\n         )\n\n\n\nSolution 1.4 (Exercise 1.4). \n\ncredit_info_clean %&gt;%\n  group_by(Income_Category) %&gt;%\n  summarise(\n    meanlim = mean(Credit_Limit),\n    medlim = median(Credit_Limit)\n  )\n\n# A tibble: 6 × 3\n  Income_Category meanlim medlim\n  &lt;ord&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n1 Unknown           9517.   6380\n2 Less than $40K    3754.   2766\n3 $40K - $60K       5462.   3682\n4 $60K - $80K      10759.   7660\n5 $80K - $120K     15810.  12830\n6 $120K +          19717.  18442\n\n\n\n\nSolution 1.5 (Exercise 1.5). \n\ncredit_info_clean %&gt;%\n  group_by(Income_Category) %&gt;%\n  summarise(\n    mean_group = mean(Credit_Limit)\n  )\n\n# A tibble: 6 × 2\n  Income_Category mean_group\n  &lt;ord&gt;                &lt;dbl&gt;\n1 Unknown              9517.\n2 Less than $40K       3754.\n3 $40K - $60K          5462.\n4 $60K - $80K         10759.\n5 $80K - $120K        15810.\n6 $120K +             19717.\n\n\nUnsurprisingly, the highest income category also has the highest mean credit limit (19,717 USD).\n\n\nSolution 1.6 (Exercise 1.6). \n\ncredit_info_clean &lt;- credit_info_clean %&gt;%\n  mutate(across(\n    where(~ is.character(.) | is.factor(.)),\n    ~ na_if(., \"Unknown\")\n  ))\n\n\n\nSolution 1.7 (Exercise 1.7). \n\nnrow_old &lt;- nrow(credit_info_clean)\n\ncredit_info_clean &lt;- credit_info_clean %&gt;%\n  na.omit()\n\nglue::glue(\"{nrow_old-nrow(credit_info_clean)} samples were removed.\")\n\n3046 samples were removed.\n\n\n\n\nSolution 1.8 (Exercise 1.8). \n\ncredit_info_clean %&gt;%\n  group_by(Card_Category) %&gt;%\n  summarise(n=n())\n\n# A tibble: 4 × 2\n  Card_Category     n\n  &lt;chr&gt;         &lt;int&gt;\n1 Blue           6598\n2 Gold             81\n3 Platinum         11\n4 Silver          391\n\n\n\n\nSolution 1.9 (Exercise 1.9). \n\ncredit_info_blue &lt;- credit_info_clean %&gt;%\n  filter(Card_Category == \"Blue\")\n\n\n\nSolution 1.10 (Exercise 1.10). \n\ncredit_info_blue %&gt;%\n  filter(Gender == \"F\" &\n           Customer_Age &lt;= 40 &\n           Credit_Limit &gt; 10000) %&gt;%\n  count()\n\n  n\n1 7\n\n\n\n\nSolution 1.11 (Exercise 1.11). \n\nggplot(data = credit_info_clean, aes(Customer_Age))+\n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\nSolution 1.12 (Exercise 1.12). \n\nggplot(data = credit_info_clean, aes(Customer_Age))+\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\n\n\nSolution 1.13 (Exercise 1.13). \n\nggplot(data = credit_info_clean, aes(Customer_Age, fill = Gender))+\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\n\n\nSolution 1.14 (Exercise 1.14). \n\nggplot(data = credit_info_clean, aes(Credit_Limit))+\n  geom_density()\n\n\n\n\n\n\n\n\n\n\nSolution 1.15 (Exercise 1.15). \n\nggplot(data = credit_info_clean,\n       aes(Customer_Age, fill = Education_Level)\n       )+\n  geom_histogram(binwidth = 1) +\n  facet_wrap(\"Education_Level\")\n\n\n\n\n\n\n\n\n\n\nSolution 1.16 (Exercise 1.16). The Mean Absolute Error (MAE) loss can be interpreted in terms of the scale of the target features because it directly measures the average absolute difference between predicted and actual target values. Thus, if the target variable is on a large scale (e.g., thousands), MAE will also be large. Conversely, for small target values, the MAE will be correspondingly smaller. This makes MAE sensitive to the scale of the target features, and it is essential to normalize or scale data if different features or targets are on very different scales to ensure the MAE provides meaningful comparisons across models or data sets.\n\n\nSolution 1.17 (Exercise 1.17). The model_linear_res data set contains the .pred column, where predictions of the linear model are saved. We can use the predictions and the outcome variable Credit_Limit to calculate the MAE.\n\nmodel_linear_res %&gt;% mae(.pred,Credit_Limit)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard       4114.\n\n\nAn MAE of 4114 indicates that on average, the predicted credit limit of a customer deviates 4114 USD.\n\n\nSolution 1.18 (Exercise 1.18). \n\nSimilar to the MAE loss, the RMSE can be interpreted in terms of the scale of the target features. It also measures the average difference between the observed and predicted values, but its unique feature is that it emphasizes outliers more. Greater distances are weighted more heavily due to the square term, thereby enhancing prediction accuracy.\n\nmodel_linear_res %&gt;% yardstick::rmse(.pred,Credit_Limit)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       5628.\n\n\nAn RMSE of 5628 indicates that, on average, a customer’s predicted credit limit deviates 5628 USD.\nAs for the MSE, the error units are expressed as squared terms. Therefore, the scales can not be interpreted directly. MSE is usually deployed in practice since it has some nice properties like differentiability at \\(0\\), which the MAE lacks. Moreover, MSE is easier to compute, thanks to the absence of a square root, which reduces computational time.\n\nmodel_linear_rmse &lt;- model_linear_res %&gt;% rmse(.pred,Credit_Limit) %&gt;%\n  pluck(\".estimate\")\nmodel_linear_rmse^2\n\n[1] 31676421",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Important R concepts</span>"
    ]
  },
  {
    "objectID": "02_linear_models.html",
    "href": "02_linear_models.html",
    "title": "2  Linear Models",
    "section": "",
    "text": "2.1 Overview\nIn this exercise session, we will review linear regression and polynomial regression. We will also learn how to efficiently split our data into training and test data, how to perform cross-validation, and why that is important. Before we dive into the details, we will discuss developing statistical models in R.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "02_linear_models.html#good-practices-for-applied-machine-learning",
    "href": "02_linear_models.html#good-practices-for-applied-machine-learning",
    "title": "2  Linear Models",
    "section": "2.2 Good practices for applied Machine Learning",
    "text": "2.2 Good practices for applied Machine Learning\nIn previous courses, we mainly focused on fitting a certain model to a given dataset. However, this process could be described as model specification rather than model development. So, what is the difference between specifying and building a model?\n\n2.2.1 Developing a model (What we have done so far!):\n\nThe given dataset has been cleaned, transformed, and manipulated using various packages and libraries.\nResampling methods (like the ones we consider today) have been applied, but training the model on each subset or newly generated dataset is usually performed using a for-loop or similar methods. Loops should mostly be avoided in programming languages like R since they can be slow compared to optimized methods specifically written for higher performance.\nSimilar to applying resampling methods, hyperparameter tuning is usually performed in the same fashion.\n\nIn summary, we have only specified the model we want to train and used a somewhat arbitrary and inconsistent approach for everything else.\nOne of the most significant issues we face, however, is when switching the model. The approach we have been using so far emphasizes working with one selected model that we wish to keep using after data preprocessing.\n\n\n2.2.2 Developing a model (What we want to do moving forward!):\nThe main difference between the old and new approaches is leveraging the advantages of the {tidyverse} and {tidymodels} frameworks. These frameworks allow for consistently preprocessing the data, setting model specifications, and performing steps like resampling and hyperparameter tuning simultaneously.\nAnother huge advantage is that we can swiftly switch between different ML models by following this procedure. For example, applying a random forest algorithm and switching to a neural network approach for the same data is only a matter of changing a few lines of code, as we will see in later exercises.\nSo, where is the catch? At first, the process might seem complicated or even “overkill” for the models we use. However, as the lecture progresses, our models will also (at least sometimes) become increasingly sophisticated. We want to get used to this new process as early as possible, as it will be useful once we consider more sophisticated models.\nThe biggest takeaways are:\n\nConsistency: Independent of what the dataset or desired model looks like, we can (almost) always use the same procedure when building a model.\nEffectiveness: Once we get used to this new approach, we can develop our models more effectively.\nSafety: Developing an ML model has many pitfalls and potholes on the way, and by design, {tidymodels} helps us to avoid those.\n\nWe will introduce and explore some of the concepts above in this session’s exercises and dive deeper in later sessions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "02_linear_models.html#introduction-to-model-development-with-tidymodels",
    "href": "02_linear_models.html#introduction-to-model-development-with-tidymodels",
    "title": "2  Linear Models",
    "section": "2.3 Introduction to model development with {tidymodels}",
    "text": "2.3 Introduction to model development with {tidymodels}\nThis section briefly introduces the most important concepts when working with the {tidymodels} framework.\nThe data set for this introduction is called “Wine Quality White,” and it contains roughly 5,000 different white wines tested for their physicochemical properties, such as citric acid, pH value, and density. After assessing these properties, experts rated the wine quality and assigned a score between \\(0\\) and \\(10\\), where \\(0\\) is the lowest, and \\(10\\) is the highest score a wine can achieve.\nThe data set can be downloaded directly from the UC Irvine Machine Learning Repository or by clicking the button below.\n\nDownload Wine Data\n\n\n2.3.1 Data Exploration\nSince the data set is relatively nice in that we do not have to do much cleaning, we will keep this section relatively short.\n\nlibrary(\"tidyverse\")\nlibrary(\"tidymodels\")\n\n\ndata_wine &lt;- read.csv(\"data/winequality-white.csv\")\ndata_wine %&gt;% glimpse()\n\nRows: 4,898\nColumns: 12\n$ fixed.acidity        &lt;dbl&gt; 7.0, 6.3, 8.1, 7.2, 7.2, 8.1, 6.2, 7.0, 6.3, 8.1,…\n$ volatile.acidity     &lt;dbl&gt; 0.27, 0.30, 0.28, 0.23, 0.23, 0.28, 0.32, 0.27, 0…\n$ citric.acid          &lt;dbl&gt; 0.36, 0.34, 0.40, 0.32, 0.32, 0.40, 0.16, 0.36, 0…\n$ residual.sugar       &lt;dbl&gt; 20.70, 1.60, 6.90, 8.50, 8.50, 6.90, 7.00, 20.70,…\n$ chlorides            &lt;dbl&gt; 0.045, 0.049, 0.050, 0.058, 0.058, 0.050, 0.045, …\n$ free.sulfur.dioxide  &lt;dbl&gt; 45, 14, 30, 47, 47, 30, 30, 45, 14, 28, 11, 17, 1…\n$ total.sulfur.dioxide &lt;dbl&gt; 170, 132, 97, 186, 186, 97, 136, 170, 132, 129, 6…\n$ density              &lt;dbl&gt; 1.0010, 0.9940, 0.9951, 0.9956, 0.9956, 0.9951, 0…\n$ pH                   &lt;dbl&gt; 3.00, 3.30, 3.26, 3.19, 3.19, 3.26, 3.18, 3.00, 3…\n$ sulphates            &lt;dbl&gt; 0.45, 0.49, 0.44, 0.40, 0.40, 0.44, 0.47, 0.45, 0…\n$ alcohol              &lt;dbl&gt; 8.8, 9.5, 10.1, 9.9, 9.9, 10.1, 9.6, 8.8, 9.5, 11…\n$ quality              &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 7, 5, 7, 6…\n\n\nExcept for the quality variable, every other variable is of type double.\n\n\n2.3.2 Training a simple linear model\nFor this example we build a linear model to predict the alcohol content of the wines. Recall the model equation \\[\\begin{equation}\n  y = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n + \\varepsilon,\n\\end{equation}\\]\nwhere \\(x_1,...,x_n\\in\\mathbb{R}^k\\) denotes \\(n\\) different features with \\(k\\) samples, \\(\\varepsilon \\sim \\mathcal{N}(0,1)\\) a \\(k\\)-dimensional error term and \\(\\beta_0,...,\\beta_n\\) the \\(n+1\\) model parameters. In our example, \\(y\\) denotes the variable alcohol, \\(k= 4898\\), and \\(n = 11\\).\nThe {parsnip} packages which is part of {tidymodels} contains the function linear_reg which creates a linear model when called.\n\nlm_mod &lt;- linear_reg()\nlm_mod\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nOnly calling the function name does not help much when trying to model the alcohol contents of the wine, since we haven’t specified the input variables, output variable, and data used for the regression.\nTo add these, we can use the %&gt;% pipe operator and fit function. As discussed in the previous exercise, the pipe operator passes the output of one operation in the other. Therefore, passing the lm_mod object into the fit function, specifies that the fit function fits a linear model. Fitting in that context refers to estimating the parameters \\(\\beta_0,...,\\beta_n\\). Besides the model specification, arguments for the fit function contain the formula which specifies the independent and dependent variables and the data argument which specifies the data that is used for training the model.\nThe formula in the code cell below specifies that we want to regress alcohol on every other variable indicated by the . after ~. ~ (tilde) is used to separate the left- and right-hand sides in the model formula.\nAs data we simply pass the whole data_wine data set.\n\nlm_mod &lt;- linear_reg() %&gt;% fit(\n formula = alcohol ~.,\n data = data_wine\n)\nlm_mod\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = alcohol ~ ., data = data)\n\nCoefficients:\n         (Intercept)         fixed.acidity      volatile.acidity  \n           6.719e+02             5.099e-01             9.636e-01  \n         citric.acid        residual.sugar             chlorides  \n           3.658e-01             2.341e-01            -1.832e-01  \n free.sulfur.dioxide  total.sulfur.dioxide               density  \n          -3.665e-03             6.579e-04            -6.793e+02  \n                  pH             sulphates               quality  \n           2.383e+00             9.669e-01             6.663e-02  \n\n\nThe return value is a parsnip model object that prints the model coefficients \\(\\beta_0,...,\\beta_n\\) when called.\nInstead of fitting the linear model on all parameters using ., we can also specify the relationship between the independent variables using arithmetic notation:\n\nlm_mod &lt;- linear_reg() %&gt;% fit(\n formula = alcohol ~ fixed.acidity+volatile.acidity+citric.acid+\n   residual.sugar+chlorides+free.sulfur.dioxide+\n   total.sulfur.dioxide+density+pH+\n   sulphates+quality,\n data = data_wine\n)\n\nNote, that this notation does not fall under best practices of model development as it is more advisable to create a separate data set for training and fit the model on every variable contained using the . notation.\n\n\n2.3.3 Evaluating a model\n\n2.3.3.1 Creating a summary of the model parameters\nUsing the tidy function on the fitted model returns an overview of the model parameters including \\(p\\)-values and the \\(t\\)-statistic.\n\nlm_mod %&gt;% tidy()\n\n# A tibble: 12 × 5\n   term                    estimate std.error statistic  p.value\n   &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)           672.        5.56       121.    0       \n 2 fixed.acidity           0.510     0.00986     51.7   0       \n 3 volatile.acidity        0.964     0.0672      14.3   1.00e-45\n 4 citric.acid             0.366     0.0560       6.54  6.88e-11\n 5 residual.sugar          0.234     0.00296     79.1   0       \n 6 chlorides              -0.183     0.321       -0.571 5.68e- 1\n 7 free.sulfur.dioxide    -0.00366   0.000494    -7.42  1.33e-13\n 8 total.sulfur.dioxide    0.000658  0.000222     2.97  3.01e- 3\n 9 density              -679.        5.70      -119.    0       \n10 pH                      2.38      0.0519      45.9   0       \n11 sulphates               0.967     0.0575      16.8   1.02e-61\n12 quality                 0.0666    0.00834      7.99  1.70e-15\n\n\nRecall that the statistic column refers to the \\(t\\)-statistic which corresponds to the following hypotheses for any \\(\\hat{\\beta}_i,\\, i=1,...,12\\): \\[\\begin{equation*}\nH_0:\\, \\hat{\\beta}_i= 0\\qquad \\mathrm{vs.}\\qquad H_1:\\, \\hat{\\beta}_i \\neq 0.\n\\end{equation*}\\] If the \\(p\\)–value regarding this test is low, we can confidently reject the null hypothesis.\nSimilar to the tidy function, the summary function can be called on the fit attribute of the model, which also returns a summary which contains a few more details, such as the \\(F\\)-statistic, \\(R^2\\), and residual standard error.\n\nlm_mod$fit %&gt;% summary()\n\n\nCall:\nstats::lm(formula = alcohol ~ fixed.acidity + volatile.acidity + \n    citric.acid + residual.sugar + chlorides + free.sulfur.dioxide + \n    total.sulfur.dioxide + density + pH + sulphates + quality, \n    data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3343 -0.2553 -0.0255  0.2214 15.7789 \n\nCoefficients:\n                       Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)           6.719e+02  5.563e+00  120.790  &lt; 2e-16 ***\nfixed.acidity         5.099e-01  9.855e-03   51.745  &lt; 2e-16 ***\nvolatile.acidity      9.636e-01  6.718e-02   14.342  &lt; 2e-16 ***\ncitric.acid           3.658e-01  5.596e-02    6.538 6.88e-11 ***\nresidual.sugar        2.341e-01  2.960e-03   79.112  &lt; 2e-16 ***\nchlorides            -1.832e-01  3.207e-01   -0.571  0.56785    \nfree.sulfur.dioxide  -3.665e-03  4.936e-04   -7.425 1.33e-13 ***\ntotal.sulfur.dioxide  6.579e-04  2.217e-04    2.968  0.00301 ** \ndensity              -6.793e+02  5.696e+00 -119.259  &lt; 2e-16 ***\npH                    2.383e+00  5.191e-02   45.916  &lt; 2e-16 ***\nsulphates             9.669e-01  5.751e-02   16.814  &lt; 2e-16 ***\nquality               6.663e-02  8.341e-03    7.988 1.70e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4409 on 4886 degrees of freedom\nMultiple R-squared:  0.8719,    Adjusted R-squared:  0.8716 \nF-statistic:  3024 on 11 and 4886 DF,  p-value: &lt; 2.2e-16\n\n\nTo extract these statistics in tidy fashion, the {yardsticks} library (see Exercise Session 01) can help.\n\n\n2.3.3.2 Using metric sets and glance\nInstead of using the summary function on the fit attribute to extract certain metrics, we can also use the metric_set function. First, define a metric set by passing different metrics such as rsq, rmse, and mae into the metric_set function. Then, pass the predictions of the model into this newly defined metric set which returns the specified metrics.\n\nmulti_metric &lt;- metric_set(rsq,rmse,mae)\n\nlm_mod %&gt;%\n  augment(data_wine) %&gt;%\n  multi_metric(.pred,alcohol)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.872\n2 rmse    standard       0.440\n3 mae     standard       0.294\n\n\nA general metric set can be created using the glance function which returns a comprehensive list of metrics for the underlying model.\n\nglance(lm_mod)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.872         0.872 0.441     3024.       0    11 -2933. 5892. 5976.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\n2.3.4 Training and test split\nSplitting the data into three different subsets, called training, validation, and testing data, is a crucial aspect in Machine Learning.\n\n\n\n\n\n\nFigure 2.1\n\n\n\nThe basic idea is to train the model on the training data, validate the results on the validation data, and finally test the performance on the testing data that has previously not been observed.\nWithout this separation the model might become prone to overfitting meaning that the model does not perform well on previously unseen data.\nThe general procedure for training and testing a model is depicted in the following figure.\n\n\n\n\n\n\nFigure 2.2: Based on this Google Developers Figure\n\n\n\nHere, the training and validation data set are used in the training procedure and the testing data set in the testing procedure.\n\n\n\n\n\n\nNote\n\n\n\nThe reevaluation during training only makes sense when changing parameters can lead to an improvement. For a simple linear regression this is not the case, since once the model parameters \\(\\beta_0,...,\\beta_n\\) are estimated, they are optimal (cf. Gauss-Markov theorem).\n\n\nA simple training and test split with \\(80\\%\\) training data and \\(20\\%\\) test data can be generated with the initial_split, training, and testing function. Before splitting the data (which is done randomly) we can set a seed. Setting a seed allows us to reproduce the outcome of our code, even when randomness is involved.\n\nset.seed(123)\ntt_split&lt;-initial_split(data_wine, 0.8)\ndata_train &lt;- tt_split %&gt;% training() \ndata_test &lt;- tt_split %&gt;% testing()\n\nTraining the linear model on the training data and evaluating it on the test data then yields\n\nlm_mod &lt;- linear_reg() %&gt;%\n  fit(\n   formula = alcohol ~.,\n   data = data_train\n  )\n\nlm_mod %&gt;%\n  augment(data_test) %&gt;%\n  multi_metric(.pred,alcohol)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.887\n2 rmse    standard       0.408\n3 mae     standard       0.307\n\n\n\n\n2.3.5 Cross validation\nA popular approach we will use in most exercises extends the procedure described in Figure 2.1 and Figure 2.2. Instead of using a single training and validation data set, we create multiple instances of training and validation data by randomly assigning a data point to the training or validation set. More formally, \\(v\\)-fold cross validation (CV) randomly splits the training and validation data into \\(v\\) equally sized subsets. In each iteration, one of these subsets is set aside to be the validation data, while the other \\(v-1\\) subsets are used for training. The figure below depicts how that process works for v = 5.\n\n\n\n5-fold Cross Validation\n\n\nA part of the whole data set is left as testing data. The blue boxes in the lower part of the figure contain the validation (sub)sets for each split while the training data is contained in the remaining (sub)sets.\n\n2.3.5.1 Creating a cross validation set in R\nCross validation in R can be performed using the {resample} library that is also part of the {tidymodels} framework.\nThe vfold_cv function creates a nested data frame, meaning that each entry in the data frame contains another data frame. v denotes the number of created folds and data_train specifies that the folds are created from the data_train data set.\n\nset.seed(123)\nfolds &lt;- vfold_cv(data_train, v = 5)\n\nNote, that since the folds are also created stochastically, setting a seed ensures that the results are reproducible.\nThe folds tibble currently contains two columns \"splits\" and \"id\". The column \"splits\" contains lists with the respective splits, while the column \"id\" contains a unique identifier for the respective splits. We can access split \\(i\\) in a fold by applying the pluck()-function iteratively. By using pluck(1), we can extract the list of splits:\n\nfolds %&gt;% \n  pluck(1)\n\n[[1]]\n&lt;Analysis/Assess/Total&gt;\n&lt;3134/784/3918&gt;\n\n[[2]]\n&lt;Analysis/Assess/Total&gt;\n&lt;3134/784/3918&gt;\n\n[[3]]\n&lt;Analysis/Assess/Total&gt;\n&lt;3134/784/3918&gt;\n\n[[4]]\n&lt;Analysis/Assess/Total&gt;\n&lt;3135/783/3918&gt;\n\n[[5]]\n&lt;Analysis/Assess/Total&gt;\n&lt;3135/783/3918&gt;\n\n\nSay, we want to take a closer look at the second split, then calling pluck(2) allows accessing it.\n\nfolds %&gt;% \n  pluck(1)%&gt;%\n  pluck(2)\n\n&lt;Analysis/Assess/Total&gt;\n&lt;3134/784/3918&gt;\n\n\nIn our example, each split contains 5 folds, of which are comprised of four parts training data and one part validation data. To access the respective parts, we can use the analysis()- and assessment()-functions:\n\nfolds %&gt;% \n  pluck(1)%&gt;%\n  pluck(2) %&gt;%\n  analysis() %&gt;%\n  glimpse()\n\nRows: 3,134\nColumns: 12\n$ fixed.acidity        &lt;dbl&gt; 8.3, 6.2, 5.9, 5.7, 6.7, 6.0, 6.6, 6.6, 9.0, 8.6,…\n$ volatile.acidity     &lt;dbl&gt; 0.25, 0.28, 0.32, 0.26, 0.18, 0.29, 0.15, 0.17, 0…\n$ citric.acid          &lt;dbl&gt; 0.33, 0.45, 0.39, 0.24, 0.28, 0.25, 0.32, 0.26, 0…\n$ residual.sugar       &lt;dbl&gt; 2.5, 7.5, 3.3, 17.8, 10.2, 1.4, 6.0, 7.4, 10.4, 1…\n$ chlorides            &lt;dbl&gt; 0.053, 0.045, 0.114, 0.059, 0.039, 0.033, 0.033, …\n$ free.sulfur.dioxide  &lt;dbl&gt; 12, 46, 24, 23, 29, 30, 59, 45, 52, 42, 14, 14, 3…\n$ total.sulfur.dioxide &lt;dbl&gt; 72, 203, 140, 124, 115, 114, 128, 128, 195, 240, …\n$ density              &lt;dbl&gt; 0.99404, 0.99573, 0.99340, 0.99773, 0.99469, 0.98…\n$ pH                   &lt;dbl&gt; 2.89, 3.26, 3.09, 3.30, 3.11, 3.08, 3.19, 3.16, 3…\n$ sulphates            &lt;dbl&gt; 0.48, 0.46, 0.45, 0.50, 0.45, 0.43, 0.71, 0.37, 0…\n$ alcohol              &lt;dbl&gt; 9.5, 9.2, 9.2, 10.1, 10.9, 13.2, 12.1, 10.0, 10.2…\n$ quality              &lt;int&gt; 5, 6, 6, 5, 7, 6, 8, 6, 6, 6, 6, 5, 5, 7, 6, 7, 6…\n\nfolds %&gt;%\n  pluck(1) %&gt;%\n  pluck(2) %&gt;%\n  assessment() %&gt;%\n  glimpse()\n\nRows: 784\nColumns: 12\n$ fixed.acidity        &lt;dbl&gt; 7.7, 7.2, 9.3, 6.8, 6.4, 6.7, 5.9, 7.0, 6.1, 6.3,…\n$ volatile.acidity     &lt;dbl&gt; 0.28, 0.24, 0.34, 0.37, 0.20, 0.36, 0.28, 0.24, 0…\n$ citric.acid          &lt;dbl&gt; 0.58, 0.29, 0.49, 0.47, 0.28, 0.26, 0.14, 0.24, 0…\n$ residual.sugar       &lt;dbl&gt; 12.10, 2.20, 7.30, 11.20, 2.50, 7.90, 8.60, 9.00,…\n$ chlorides            &lt;dbl&gt; 0.046, 0.037, 0.052, 0.071, 0.032, 0.034, 0.032, …\n$ free.sulfur.dioxide  &lt;dbl&gt; 60, 37, 30, 44, 24, 39, 30, 42, 37, 24, 26, 42, 3…\n$ total.sulfur.dioxide &lt;dbl&gt; 177, 102, 146, 136, 84, 123, 142, 219, 136, 108, …\n$ density              &lt;dbl&gt; 0.99830, 0.99200, 0.99800, 0.99680, 0.99168, 0.99…\n$ pH                   &lt;dbl&gt; 3.08, 3.27, 3.17, 2.98, 3.31, 2.99, 3.28, 3.47, 3…\n$ sulphates            &lt;dbl&gt; 0.46, 0.64, 0.61, 0.88, 0.55, 0.30, 0.44, 0.46, 0…\n$ alcohol              &lt;dbl&gt; 8.9, 11.0, 10.2, 9.2, 11.5, 12.2, 9.5, 10.2, 10.8…\n$ quality              &lt;int&gt; 5, 7, 5, 5, 5, 7, 6, 6, 7, 6, 7, 5, 5, 5, 6, 6, 7…\n\n\nSince we will be using cross validation in R throughout the manuscript many different times, it is essential to understand its components and construction.\n\n\n2.3.5.2 Training a model using cross validation\nTo train the linear model on each split, we can use the fit_resamples function. Training the model on each fold is the as simple as training he model without resamples: We simply pass the model specification, formula, and additionally the cross validation object into the fit_resamples formula. Additionally, we can also pass the metric set multi_metric to specify which metrics we want to use for model evaluation.\n\nlm_mod_resampling &lt;- linear_reg()\nlm_mod_resampling_res &lt;- fit_resamples(lm_mod_resampling,\n                            alcohol ~.,\n                            folds,\n                            metrics = multi_metric)\n\nThe return value of the fit_resamples function is a data frame containing \\(5\\) linear models (since we specified v=5 when creating the folds object).\n\nlm_mod_resampling_res\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits             id    .metrics         .notes          \n  &lt;list&gt;             &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [3134/784]&gt; Fold1 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 4]&gt;\n2 &lt;split [3134/784]&gt; Fold2 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 4]&gt;\n3 &lt;split [3134/784]&gt; Fold3 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 4]&gt;\n4 &lt;split [3135/783]&gt; Fold4 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 4]&gt;\n5 &lt;split [3135/783]&gt; Fold5 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 4]&gt;\n\n\nWe are mainly interested in the cross validation error (CV-RMSE) defined as\n\\[\\begin{equation*}\n  \\mathrm{CV-RMSE} = \\frac{1}{v}\\sum_{i=1}^v \\mathrm{RMSE}_i,\n\\end{equation*}\\]\nwhere \\(\\mathrm{RMSE}_i\\) stands for the RMSE of the \\(i\\)-th. hold-out sample.\nWe can collect this metric by applying the collect_metrics function:\n\nlm_mod_resampling_res %&gt;% collect_metrics()\n\n# A tibble: 3 × 6\n  .metric .estimator  mean     n std_err .config        \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1 mae     standard   0.300     5 0.00460 pre0_mod0_post0\n2 rmse    standard   0.451     5 0.0710  pre0_mod0_post0\n3 rsq     standard   0.866     5 0.0414  pre0_mod0_post0\n\n\nThe third column mean depicts the mean rmse and rsq across all the splits. Comparing the CV-RMSE (\\(0.451\\)) to the true out of sample (OOS) RMSE of the test set (\\(0.408\\)) reveals that the performance of the linear model seems stable, meaning that it is not prone to overfitting. Furthermore, the CV-RMSE seems to overestimate the true OOS RMSE, since \\(0.451&gt;0.408\\). Using this information we can make prediction about the alcohol contents of a wine with the estimated model parameters on the training data set.\n\n\n\n2.3.6 Polynomial regression and the bias variance trade off\nPolynomial regression is a special case of multiple linear regression where the model is still linear in its coefficients but the dependent variable \\(y\\) is modeled as polynomial in \\(x\\). For an \\(n\\)-th degree polynomial model the model equation is therefore given by\n\\[\\begin{equation*}\n  y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + ... + \\beta_n x^n.\n\\end{equation*}\\]\n\n\n\n\n\n\nNote\n\n\n\nFor this specific model we only use one independent variable \\(x\\) instead of \\(n\\) different independent variables \\(x_1\\),…,\\(x_n\\).\n\n\nSay, we want to model the alcohol contents of wine with a MLR model using the wine density as the only predictor.\nConsider the following synthetic dataset consisting of \\(12\\) observations.\n\nset.seed(123)\ndata_synth &lt;- tibble(\n  x = seq(0,1,1/11),\n  y = x+rnorm(12,0,0.1)\n)\n\nsplit_synth &lt;- initial_split(data_synth,0.8)\ndata_train_synth &lt;- split_synth %&gt;% training()\ndata_test_synth &lt;- split_synth %&gt;% testing()\n\ndata_synth\n\n# A tibble: 12 × 2\n        x       y\n    &lt;dbl&gt;   &lt;dbl&gt;\n 1 0      -0.0560\n 2 0.0909  0.0679\n 3 0.182   0.338 \n 4 0.273   0.280 \n 5 0.364   0.377 \n 6 0.455   0.626 \n 7 0.545   0.592 \n 8 0.636   0.510 \n 9 0.727   0.659 \n10 0.818   0.774 \n11 0.909   1.03  \n12 1       1.04  \n\n\nFrom the figure below, it is immediately evident, that a polynomial model perfectly fits the training data, but severely fails to estimate the rightmost point of the testing data. While the linear model does not fit the training data\n\n\n\n\n\n\n\n\n\nTo verify this, we can also compare the training and test error for different metrics of each model.\n\nlm_poly_mod &lt;- linear_reg() %&gt;% \n  fit(formula = y ~ poly(x, 8),\n      data = data_train_synth\n      )\n\nlm_lin_mod &lt;- linear_reg() %&gt;% \n  fit(formula = y ~ x,\n      data = data_train_synth\n      )\n\nVisualizing the difference in train and test error then yields\n\n\n\n\n\n\n\n\n\nThe example above demonstrates the phenomenon of bias-variance tradeoff. A low variance and high bias can be observed in the linear model, since there are only few (two) model parameters and a small discrepancy between training and test error. The polynomial model exhibits a large discrepancy between training and test error and since there are many (nine) parameters, indicating that it has a high variance but low bias.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "02_linear_models.html#exercises",
    "href": "02_linear_models.html#exercises",
    "title": "2  Linear Models",
    "section": "2.4 Exercises",
    "text": "2.4 Exercises\nThroughout the exercises, we will work with a subset of the Apartment rental offers in Germany data set that can be downloaded using the button below. It contains 239 unique rental listings for flats in Augsburg which were sourced at three different dates in 2018 and 2019 and contains 28 different variables.\n\nDownload AugsburgRental\n\n\nExercise 2.1 Instead of focusing on a comprehensive data cleaning and manipulation process, we will simply use the two variables livingSpace measuring the area of living in \\(m^2\\) of a listing and baseRent in EUR, representing the monthly base rent.\nImport the data and visualize the relationship between the two variables livingSpace and baseRent with a scatter plot. Rename the axis of the plot such that they display the words base rent and living space (seperated by a whitespace).\n\n\nExercise 2.2 Without conducting a thorough outlier analysis, remove every listing that either costs more than \\(2500\\) EUR or is bigger than \\(200\\: m^2\\).\n\n\nExercise 2.3 Create a training (\\(80\\%\\)) and test data set using the filtered data. Use set.seed(2) to generate reproducible results.\n\n\nExercise 2.4 Train a simple linear model on the training data.\n\n\nExercise 2.5 Generate a model summary and interpret the coefficients. Is the independent variable statistically significant?\n\n\nExercise 2.6 Evaluate the model on the test data by considering the adjusted \\(R^2\\) and MAE. On average, how far off is the estimated base rent?\n\n\nExercise 2.7 Create a \\(10\\)-fold cross validation split of the training data using the same seed as before and retrain the simple linear model. Compare the cross validation MAE to the OOS MAE and interpret the result.\n\n\nExercise 2.8 Repeat Exercises Exercise 2.6 and Exercise 2.8 for a polynomial model of degree 20. Compare the test and cross-validation MAE of the linear model with the polynomial model.\n\n\nExercise 2.9 Create a scatter plot of the training data and insert both fitted curves using the geom_smooth function.\n\n\nExercise 2.10 Assume we have fitted a simple linear model \\[\\begin{equation*}\n  \\hat{y} = \\hat{\\beta_0}+\\hat{\\beta_1}x.\n\\end{equation*}\\]\nFor \\(\\hat\\beta_1\\) the \\(t\\)-statistic has a \\(p\\)-value of \\(1e-16\\) (\\(=1\\cdot 10^{-16}\\)). Describe the null hypothesis of the underlying test and explain what concnulision can be drawn based on this \\(p\\)-value.\n\n\nExercise 2.11 Assume we have a data set with \\(k = 400\\) containing a single independent variable \\(x\\) and a real valued dependent variable \\(y\\). We fit a simple regression model and a polynomial regression with degree \\(5\\) on \\(75\\%\\) of the data and leave the remaining \\(25\\%\\) for testing .\n\nAssume that the true relationship between \\(x\\) and \\(y\\) is linear. Consider the training RMSE for the linear regression, and also the training RMSE for the polynomial regression. Choose the most appropriate answer and justify your choice.\n1.1 The polynomial model has more flexibility due to the additional terms, so it will fit the data better than the linear model, resulting in a lower training RMSE.\n1.2 The linear model is simpler and less prone to overfitting, so it will produce a lower training RMSE compared to the polynomial model.\n1.3 Since both models are trying to explain the same data, their training RMSE should be approximately the same.\n1.4 Without knowing the true underlying relationship between the predictor and response, we cannot definitively predict the behavior of the training RMSE for either model.\nRepeat the previous exercise but instead of the training RMSE, consider the test RMSE.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "02_linear_models.html#solutions",
    "href": "02_linear_models.html#solutions",
    "title": "2  Linear Models",
    "section": "2.5 Solutions",
    "text": "2.5 Solutions\n\nSolution 2.1 (Exercise 2.1). The data set can be imported using the read_csv function.\n\ndata_aux &lt;- read_csv(\"data/rent_aux.csv\")\n\nUsing the ggplot function, we can create a simple scatter plot. The labs function allows to define new axis labels by specifying the axis and assigning the names to the respective axis. \\(x\\) corresponds to the horizontal axis and \\(y\\) to the vertical axis.\n\ndata_aux %&gt;% \n  ggplot(aes(x = livingSpace, y = baseRent)) +\n  geom_point() +\n  labs(\n    x = \"living space\",\n    y = \"base rent\"\n  )\n\n\n\n\n\n\n\n\n\n\nSolution 2.2 (Exercise 2.2). To select all listings with base rent lower than \\(2500\\) EUR and living space less than \\(200\\) sqm, we can use the filter function and overwrite the old data set.\n\ndata_aux &lt;- data_aux %&gt;%\n  filter(baseRent &lt;= 2500, livingSpace &lt;= 200)\n\n\n\nSolution 2.3 (Exercise 2.3). By setting set.seed(2) we can make sure that the following results are reproducible. Similar to Section 2.3.4, we can define a split object using the initial_split function and select the training and test portion using the training and testing functions respectively.\n\nset.seed(2)\nsplit &lt;-initial_split(data_aux,0.8)\ndata_train &lt;- split %&gt;% training(split)\ndata_test &lt;- split %&gt;% testing(split)\n\n\n\nSolution 2.4 (Exercise 2.4). \n\nlm_mod &lt;- linear_reg() %&gt;%\n  fit(baseRent ~ livingSpace, data_train)\n\n\n\nSolution 2.5 (Exercise 2.5). A simple model summary can be created by passing the trained model into the tidy function.\n\nlm_mod %&gt;% tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     87.3    25.2        3.46 6.58e- 4\n2 livingSpace     10.4     0.322     32.2  5.49e-79\n\n\nThe estimated parameter \\(\\hat{\\beta_1} = 10.4\\) indicates that according to the linear model, we expect the base rent to rise \\(10.4\\) EUR for every additional square meter of living space.\n\n\nSolution 2.6 (Exercise 2.6). We can calculate the adjusted \\(R^2\\) and \\(MAE\\) using the metric_set function.\n\nmulti_metric&lt;- metric_set(rsq,mae)\nlm_mod %&gt;%\n  augment(data_test) %&gt;%\n  multi_metric(.pred,baseRent)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.883\n2 mae     standard     108.   \n\n\nMAE\\(=108\\) indicates that on average our estimated base rent is off by \\(108\\) EUR.\n\n\nSolution 2.8 (Exercise 2.8). \n\nset.seed(2)\nfolds &lt;- vfold_cv(data_train)\n\nlm_mod_resampling_res &lt;- linear_reg() %&gt;% \n  fit_resamples(baseRent ~ livingSpace,\n                folds,\n                metrics = multi_metric)\n\nlm_mod_resampling_res %&gt;% collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config        \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1 mae     standard   116.       10  5.12   pre0_mod0_post0\n2 rsq     standard     0.869    10  0.0153 pre0_mod0_post0\n\n\nThe cross-validation MAE of \\(116\\) EUR compared to the test MAE of \\(108\\) EUR indicates that the cross-validation error is overestimating the true test error. This is in fact favorable, since it is preferable to have a pessimistic bias towards an estimated error. In other words, overestimating an error is always better than underestimating an error.\n\n\nSolution 2.8 (Exercise 2.8). \n\nlm_poly_mod &lt;- linear_reg() %&gt;% fit(\n  formula = baseRent ~ poly(livingSpace, 20), \n  data = data_train\n)\n\nlm_poly_mod %&gt;%\n  augment(data_test) %&gt;%\n  multi_metric(.pred,baseRent)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.349\n2 mae     standard     153.   \n\nlm_poly_mod_resampling_res &lt;- linear_reg() %&gt;% \n  fit_resamples(baseRent ~ poly(livingSpace,20),\n                folds,\n                metrics = multi_metric)\n\nlm_poly_mod_resampling_res %&gt;% collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator     mean     n   std_err .config        \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;          \n1 mae     standard   1541.       10 1205.     pre0_mod0_post0\n2 rsq     standard      0.624    10    0.0937 pre0_mod0_post0\n\n\nCompared to the simple linear model, the polynomial model performs a lot worse on the test data as it severely ovefits on the training data. The CV-MAE of \\(1541\\) compared to the test MAE of \\(153\\) is another redflag we should consider when evaluating model performance.\n\n\nSolution 2.9 (Exercise 2.9). We can directly pipe the training data data_train into the ggplot function to create the plot. The color argument in the geom_point function changes the color of the points and the size argument toggles the point sizes in the figure. When using geom_smooth have to express the formula in terms of x and y instead of using livingSpace and baseRent. Setting se=FALSE removes the confidence band of the estimated lines. color = \"black\", linetype=2, and linewidth=1.5 specifies the visual characteristics of the line. As with the previous plot, we can also change the axis labels using labs.\n\ndata_train %&gt;% ggplot(aes(x=livingSpace,y=baseRent))+\n  geom_point(color=\"#f07167\",\n             size = 2)+\n  geom_smooth(formula = y ~ poly(x, 20),\n              method = \"lm\",\n              color = \"black\",\n              se = FALSE,\n              linetype= 2,\n              linewidth = 1.5)+\n  geom_smooth(formula = y ~ x,\n              method = \"lm\",\n              color = \"black\",\n              se = FALSE,\n              linetype= 5,\n              linewidth = 1.5)+\n  labs(x=\"Living space in sqm\",\n       y=\"base rent in EUR\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nSolution 2.10 (Exercise 2.10). A \\(p\\)-value of \\(1e-16\\) (\\(=1\\cdot 10^{-16}\\)) indicates that we can reject the null hypotheses \\(H_0: \\beta_1 = 0\\) to the significance level \\(1-(10^{-16})\\).\n\n\nSolution 2.11 (Exercise 2.11). \n\nSince the polynomial model has more flexibility due to the additional terms, it will likely fit the training data better than the linear model, resulting in a lower training RMSE.\nThe linear model is simpler and less prone to overfitting, so it will likely produce a lower test RMSE compared to the polynomial model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "03_regularization.html",
    "href": "03_regularization.html",
    "title": "3  Regularization",
    "section": "",
    "text": "3.1 Introduction\nThis cahpter aims to review multiple linear regression (MLR) and different regularization techniques in theory and practice. Regularization techniques in linear regression involve hyperparameters, namely the penalty and mixture. The models we consider in this session are, ridge, lasso and elastic net regression models.\nAs for new R concepts, we will consider recipes and workflows, which are helpful for keeping track of the preprocessing and model training process.\nA recipe aggregates steps applied to a data set before specifying a model. Therefore, recipes include the formula, i.e., the specification for the target and features, and most of the preprocessing steps we have applied manually before.\nWorkflows serve as a container for recipes and model specifications. They streamline the training process and are especially helpful whenever many different models are involved.\nWe use the same white wine data set as in Session 02 for the introduction.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "03_regularization.html#introduction",
    "href": "03_regularization.html#introduction",
    "title": "3  Regularization",
    "section": "",
    "text": "3.1.1 Recipes and Workflows\nWe start out by importing the wine data set again. For detailed description of each parameter see Cortez et al.\n\nlibrary(\"tidyverse\")\nlibrary(\"tidymodels\")\nlibrary(\"glmnet\")\n\n\ndata_wine &lt;- read.csv(\"data/winequality-white.csv\")\ndata_wine %&gt;% glimpse()\n\nRows: 4,898\nColumns: 12\n$ fixed.acidity        &lt;dbl&gt; 7.0, 6.3, 8.1, 7.2, 7.2, 8.1, 6.2, 7.0, 6.3, 8.1,…\n$ volatile.acidity     &lt;dbl&gt; 0.27, 0.30, 0.28, 0.23, 0.23, 0.28, 0.32, 0.27, 0…\n$ citric.acid          &lt;dbl&gt; 0.36, 0.34, 0.40, 0.32, 0.32, 0.40, 0.16, 0.36, 0…\n$ residual.sugar       &lt;dbl&gt; 20.70, 1.60, 6.90, 8.50, 8.50, 6.90, 7.00, 20.70,…\n$ chlorides            &lt;dbl&gt; 0.045, 0.049, 0.050, 0.058, 0.058, 0.050, 0.045, …\n$ free.sulfur.dioxide  &lt;dbl&gt; 45, 14, 30, 47, 47, 30, 30, 45, 14, 28, 11, 17, 1…\n$ total.sulfur.dioxide &lt;dbl&gt; 170, 132, 97, 186, 186, 97, 136, 170, 132, 129, 6…\n$ density              &lt;dbl&gt; 1.0010, 0.9940, 0.9951, 0.9956, 0.9956, 0.9951, 0…\n$ pH                   &lt;dbl&gt; 3.00, 3.30, 3.26, 3.19, 3.19, 3.26, 3.18, 3.00, 3…\n$ sulphates            &lt;dbl&gt; 0.45, 0.49, 0.44, 0.40, 0.40, 0.44, 0.47, 0.45, 0…\n$ alcohol              &lt;dbl&gt; 8.8, 9.5, 10.1, 9.9, 9.9, 10.1, 9.6, 8.8, 9.5, 11…\n$ quality              &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 7, 5, 7, 6…\n\n\nAgain, the goal is to estimate the alcohol contents (measured in alc/vol), but instead of using multiple linear regression, we use a penalized linear regression. First, we create a data split and cross-validation object:\n\nset.seed(123)\n\nsplit_wine &lt;- initial_split(data_wine)\ndata_train_wine &lt;- training(split_wine)\ndata_test_wine &lt;- testing(split_wine)\n\n\n3.1.1.1 Recipes\nInstead of directly specifying a linear model, we describe how we want to fit any model to the data and which preprocessing steps we want to add. Compared to the formula method inside the fit function, the recipe can perform more preprocessing steps via step_*() functions without executing them directly. Independent of the type of model we fit (or train, for that matter), we can reuse the following recipe:\n\nrec_wine &lt;- recipe(\n  alcohol ~.,\n  data_train_wine\n) %&gt;%\n  step_normalize(all_predictors())\n\nIn the first four lines, the recipe function specifies the formula (alcohol ~.) and template data_train_wine. The template initializes the recipe but can be changed later (e.g., when we apply these preprocessing steps to the test data). The step_normalize function in the fifth line normalizes the data passed to the recipe. Normalizing coefficients allows a comparison between features invariant of the original scale. In other words, a large absolute value of a standardized coefficient strongly influences the model.\nCalling the recipe then creates a summary of the input and operations that will be performed when training a model based on this recipe:\n\nrec_wine\n\nWe can also use the recipe to preprocess directly. By passing the rec_wine recipe into the prep() and bake functions, the specified steps are applied to the provided data set.1\n\nrec_wine %&gt;%\n  prep() %&gt;%\n  bake(data_train_wine) %&gt;%\n  glimpse()\n\nRows: 3,673\nColumns: 12\n$ fixed.acidity        &lt;dbl&gt; 1.74676532, -0.77276698, 1.02689895, -1.13270017,…\n$ volatile.acidity     &lt;dbl&gt; -0.28262884, 0.01405471, 0.01405471, 0.40963277, …\n$ citric.acid          &lt;dbl&gt; -0.02709580, 0.96236127, 2.03427309, 0.46763273, …\n$ residual.sugar       &lt;dbl&gt; -0.75671284, 0.23276240, 1.14307962, -0.59839680,…\n$ chlorides            &lt;dbl&gt; 0.326331917, -0.035460154, 0.009763855, 3.0849964…\n$ free.sulfur.dioxide  &lt;dbl&gt; -1.35630369, 0.61777490, 1.43063080, -0.65957007,…\n$ total.sulfur.dioxide &lt;dbl&gt; -1.56230134, 1.52596077, 0.91302325, 0.04076601, …\n$ density              &lt;dbl&gt; 0.01579382, 0.57999326, 1.43797700, -0.19786750, …\n$ pH                   &lt;dbl&gt; -1.96826221, 0.46863813, -0.71688096, -0.65101879…\n$ sulphates            &lt;dbl&gt; -0.093372770, -0.267595877, -0.267595877, -0.3547…\n$ quality              &lt;dbl&gt; -0.9943613, 0.1353291, -0.9943613, 0.1353291, -0.…\n$ alcohol              &lt;dbl&gt; 9.5, 9.2, 8.9, 9.2, 10.1, 10.9, 11.0, 10.2, 13.2,…\n\n\nNote that by applying the prep and bake functions, the new data set now contains the variable quality as an ordinal feature.\n\n\n3.1.1.2 Workflows\nWorkflows combine model specifications and recipes. A workflow object can be defined using the workflow function. Adding a recipe is as simple as calling the function add_recipe and specifying the desired recipe. Calling the workflow object shows that a recipe is present, but a model is still missing.\n\nwf_wine &lt;- workflow() %&gt;%\n  add_recipe(rec_wine)\n\nwf_wine\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: None\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_normalize()\n\n\nSpecifying an MLR model and adding it to the workflow is achieved by calling the add_model function:\n\nlm_spec&lt;- linear_reg()\n\nwf_wine &lt;- wf_wine %&gt;%\n  add_model(lm_spec)\n\nWe can then finally train the model using the fit function again:\n\nlm_fit_res &lt;- wf_wine %&gt;%\n  fit(data_train_wine)\n\nThe whole process can be visualized as follows:\n\n\n\nComponents of a workflow object\n\n\nThe circling arrows symbolize that the model specification can be swapped, meaning that we can simply specify a different model and replace it in the workflow.\n\n\n\n3.1.2 Tuning an elastic net regression model\n\n3.1.2.1 Training an elastic net regression model\nSimilar to specifying a unpenalized linear regression model, we can specify an elastic net model with the linear_reg()-function as well:\n\nlm_elnet_spec &lt;- linear_reg(\n  penalty = 0.05,\n  mixture = 0.6\n  ) %&gt;%\n  set_engine(\"glmnet\")\n\nHere, the mixture argument \\(\\alpha\\in [0,1]\\) specifies the ratio of ridge and lasso penalty in the elastic net. The hyperparameter \\(\\lambda\\) specifies the penalty in both terms of the elastic net loss function:\n\\[\\begin{equation}\n\\mathcal{L}_{\\mathrm{el\\_net}}(\\beta,\\lambda,\\alpha) = \\sum_{n=1}^{N} (y_n - \\hat{y}_n)^2 + \\lambda\\left(\\alpha \\sum_{i=1}^{k} \\beta_i^2 +(1-\\alpha) \\sum_{i=1}^{k} \\|\\beta_i\\|\\right)\n\\end{equation}\\]\nNote, that we do not penalize the coefficient \\(\\beta_0\\)!\nSetting mixture = 0 indicates the model is a ridge regression, while mixture=1 corresponds to lasso regression.\nIn the specification above, \\(\\alpha=0.6\\) and \\(\\lambda = 0.05\\).\nUsing the set_engine(\"glmnet\") command, we specify that the elastic net regression is performed using the {glmnet} package. Besides the {glmnet} library, there are around 13 other packages for fitting linear models. However, it is far from the scope of this manuscript to discuss every other library in detail.\nTo fit the elastic net regression model, swap the model specification lm_spec with the model specification of the elastic net regression lm_elnet_spec in the workflow using update_model and pass the output to the fit function.\n\nlm_elnet_fit_res &lt;- wf_wine %&gt;%\n  update_model(lm_elnet_spec) %&gt;%\n  fit(data_train_wine)\n\nThe estimated coefficients can then be viewed using the tidy() function:\n\nlm_elnet_fit_res %&gt;%\n  tidy()\n\n# A tibble: 12 × 3\n   term                 estimate penalty\n   &lt;chr&gt;                   &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)           10.5       0.05\n 2 fixed.acidity          0.230     0.05\n 3 volatile.acidity       0.113     0.05\n 4 citric.acid            0.0178    0.05\n 5 residual.sugar         0.630     0.05\n 6 chlorides             -0.0745    0.05\n 7 free.sulfur.dioxide   -0.0158    0.05\n 8 total.sulfur.dioxide  -0.0229    0.05\n 9 density               -1.40      0.05\n10 pH                     0.194     0.05\n11 sulphates              0.0496    0.05\n12 quality                0.143     0.05\n\n\nComparing the model coefficients of the penalized model with the unpenalized model yields:\n\n\n\n\n\n\n\n\n\nJudging by the figure above, the variables density and residual_sugar are affected most by the penalisation.\n\n\n3.1.2.2 Determining an optimal value for \\(\\lambda\\)\nThe penalty value 0.05 and mixing parameter 0.6 were chosen arbitrarily. Depending on each of the parameters, the performance of the penalized model might increase or decrease. It is, therefore, essential to find optimal hyperparameters!\nHyperparameter tuning aims to streamline this approach. Instead of randomly choosing a penalty and mixture value, we can search for an optimal one by trying out a range of different penalties and choosing the optimal one within this range. Usually, one deploys an equidistant grid of candidate values. For example, if the penalty can take any value in the interval \\([0,1]\\), then try \\(\\lambda \\in \\{0,\\frac{1}{n},\\frac{2}{n},...,\\frac{n}{n}\\}\\), where \\(n \\in \\mathbb{N}\\). However, this approach comes with challenges as well. Consider the following hypothetical example where an equidistant grid with \\(n=8\\) is chosen.\n\n\n\nHypothetical example of the model performance depending on the penalty value\n\n\nThe chosen grid (blue dots) does not cover the optimal penalty value (green dot). A solution could be increasing the grid size. However, more complex models require more computing time, so testing many different grid values eventually becomes unfeasible\nIt is, therefore, important to balance grid size and compute time to obtain satisfactory results.\n\n\n3.1.2.3 Tuning for an optimal penalty value\nWhen computational resources are available, it is encouraged to perform hyperparameter tuning via cross-validation instead of a single train/validation/test split.\n\nfolds_wine &lt;- vfold_cv(data_train_wine,10)\n\nUsing the {tidymodels} framework makes hyperparameter tuning fairly straightforward, even when using cross-validation. By setting a hyperparameters in the model specification to tune(), we indicate that a parameter should later be tuned.\nFor the specific example of elastic net regression, we can achieve that using the following code snippet:\n\nlm_elnet_spec_tune &lt;- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n  ) %&gt;%\n  set_engine(\"glmnet\")\n\nBy updating the workflow and subsequently training the model for different hyperparameters on each split using the tune_grid function, we obtain the results of hyperparameter tuning.\n\nlm_elnet_tune_fit_res &lt;- wf_wine %&gt;%\n  update_model(lm_elnet_spec_tune) %&gt;%\n  tune_grid(\n    resamples = folds_wine,\n    grid = 20\n  )\n\nSetting grid = 20 automatically generates \\(20\\) different candidate hyperparameters. We can analyze the results using the autoplot()-function:\n\nlm_elnet_tune_fit_res %&gt;% autoplot()\n\n\n\n\n\n\n\n\nIt seems like an increasing regularisation generally leads to worse performance on the validation sets. On the other hand, there does not seem to be a clear influence of the mixture parameter on the model performance based on this limited sample. We should, therefore, check if increasing the hyperparameter combinations yields some more insights.\nBy setting a grid manually, we can specify the parameters and values to be tuned explicitly:\n\nhyper_grid &lt;- expand.grid(\n  penalty = seq(0,1,length.out=15),\n  mixture = seq(0,1,length.out = 15)\n  ) %&gt;%\n  as_tibble()\n\nNote, that we used the expand.grid()-function prior to converting the data into a tibble, to ensure that we generate evey possible combination of penatly and mixture values. In the example above, we created a parameter grid of size \\(15^2=225\\). While this approach is fairly ineffecitve it sufficses for the purpose of this simple experiment.\n\nlm_elnet_tune_fit_res &lt;- wf_wine %&gt;%\n  update_model(lm_elnet_spec_tune) %&gt;%\n  tune_grid(\n    resamples = folds_wine,\n    grid = hyper_grid,\n    metrics = metric_set(mae)\n  )\n\nBy setting metrics = metric_set(mae), we specify that the metric for evaluating the model is the mean absolute error.\nCalling the results of model tuning returns a nested data frame consisting of \\(10\\) rows and \\(4\\) columns. Each row contains information information like the metric about the underlying split for each tested hyperparameter.\n\nlm_elnet_tune_fit_res\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits             id     .metrics           .notes          \n   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;          \n 1 &lt;split [3305/368]&gt; Fold01 &lt;tibble [225 × 6]&gt; &lt;tibble [0 × 4]&gt;\n 2 &lt;split [3305/368]&gt; Fold02 &lt;tibble [225 × 6]&gt; &lt;tibble [0 × 4]&gt;\n 3 &lt;split [3305/368]&gt; Fold03 &lt;tibble [225 × 6]&gt; &lt;tibble [0 × 4]&gt;\n 4 &lt;split [3306/367]&gt; Fold04 &lt;tibble [225 × 6]&gt; &lt;tibble [0 × 4]&gt;\n 5 &lt;split [3306/367]&gt; Fold05 &lt;tibble [225 × 6]&gt; &lt;tibble [0 × 4]&gt;\n 6 &lt;split [3306/367]&gt; Fold06 &lt;tibble [225 × 6]&gt; &lt;tibble [0 × 4]&gt;\n 7 &lt;split [3306/367]&gt; Fold07 &lt;tibble [225 × 6]&gt; &lt;tibble [0 × 4]&gt;\n 8 &lt;split [3306/367]&gt; Fold08 &lt;tibble [225 × 6]&gt; &lt;tibble [0 × 4]&gt;\n 9 &lt;split [3306/367]&gt; Fold09 &lt;tibble [225 × 6]&gt; &lt;tibble [0 × 4]&gt;\n10 &lt;split [3306/367]&gt; Fold10 &lt;tibble [225 × 6]&gt; &lt;tibble [0 × 4]&gt;\n\n\nAgain, using the autoplot()-function we can visualize some finding right away:\n\nlm_elnet_tune_fit_res %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\nConsidering the figure above, the cross-validation MAE (CV-MAE) increases for each parameter combination different from \\((0,0)\\). Furthermore, the increase in CV-MAE seems to be more profound as the lasso proportion increases.\nThis is an example where penalizing model parameters solely for model performance does not seem to be beneficial. However, if we wish to extract the optimal hyperparameter values, we can proceed as follows:\n\n(hyper_opt &lt;- select_best(lm_elnet_tune_fit_res,\n                            metric = \"mae\")\n )\n\n# A tibble: 1 × 3\n  penalty mixture .config          \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1       0       1 pre0_mod015_post0\n\n\nUnsurprisingly, the optimal penalty is \\(\\lambda = 0\\). Since the CV-MAE is indifferent of the mixture parameter when \\(\\lambda=0\\), the mixture parameter is set to \\(1\\).\nWe can also choose \\(\\lambda\\) according to the “one-standard error” rule:\n\n(hyper_opt_osr &lt;- select_by_one_std_err(lm_elnet_tune_fit_res,\n                            metric = \"mae\",\n                            desc(penalty))\n )\n\n# A tibble: 1 × 3\n  penalty mixture .config          \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1       0  0.0714 pre0_mod002_post0\n\n\nThe optimal penalty according to the “one-standard error” rule, which selects the most simple model within one standard error of the numerically optimal results, is different from \\(0\\).\n\n\n\n\n\n\nTipPro Tip\n\n\n\nYou can use the pull()-function in a similar fashion as pluck to extract the raw values of a column. While the pluck()-function requires passing a string or the column index, the pull()-function works fine with passing the column name verbatim.\n\nhyper_opt %&gt;%\n  pull(penalty)\n\n[1] 0\n\nhyper_opt_osr %&gt;%\n  pull(penalty)\n\n[1] 0\n\n\n\n\n\n\n3.1.2.4 Training a final model with the chosen hyperparameter values\nAfter determining the best hyperparameter, a final model can be trained on the whole training data and evaluated on the testing data.\nWe can use the finalize_workflow()-function to set the hyperparameters specified in hyper_opt_osr for a final model that is trained on the entire training data. By passing this final workflow to the last_fit-function, we fit this final model.\n\nlm_elnet_spec_final_fit &lt;- finalize_workflow(\n  wf_wine,\n  hyper_opt_osr\n  ) %&gt;%\n  last_fit(\n    split = split_wine,\n    metrics = metric_set(mae)\n    )\n\nThe metrics can then be extracted using the collect_metrics function.\n\nlm_elnet_spec_final_fit %&gt;% \n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 mae     standard       0.302 pre0_mod0_post0\n\n\nA visualization of the steps for effectively tuning the penalty value in a ridge regression setting can be found below.\n\n\n\nTuning procedure for the penalty in ridge regression",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "03_regularization.html#exercises",
    "href": "03_regularization.html#exercises",
    "title": "3  Regularization",
    "section": "3.2 Exercises",
    "text": "3.2 Exercises\n\n3.2.1 Theoretical Exercises\n\nExercise 3.1 The goal of this initial exercise is to review some theoretical aspects of OLS, ridge, and lasso regression.\nIn Statistics I/II, we learned that OLS is the cornerstone of linear regression analysis. It allows us to explore and quantify the relationship between the response variable and the regressors in a relatively simple but meaningful way. We can extend the idea of a simple linear regression by adding a penalty term to the loss function we want to minimize. This process is called regularization and has been introduced in the lecture regarding ridge and lasso regression.\nConsider a simple linear model with a quantitative response variable \\(Y\\) and a single predictor \\(X\\). The simple linear model then assumes (among other things) that there is approximately a linear relationship between \\(Y\\) and \\(X\\), i.e.,\n\\[ Y \\approx \\beta_0 + \\beta_1 X. \\]\nwith unknown coefficients \\(\\beta_0,\\beta_1\\). To obtain the best estimate \\(\\beta_0\\) and \\(\\beta_1\\) for a given sample we can minimize the MSE\n\\[\\begin{equation}\n  \\min_{\\beta_0,\\beta_1} MSE = \\frac{1}{N}\\sum_{n=1}^{N} (y_n - (\\beta_0 + \\beta_1 x_n))^2\n\\end{equation}\\]\nwhere \\(N = \\mathrm{length(Y)}\\), \\(y_1,…,y_N\\) is a realized sample of \\(Y\\), and \\(x_1,…,x_N\\) is a realized sample of \\(X\\).\nShow, that\n\\[\\begin{align*}\n  \\hat \\beta_1 &= \\frac{\\sum_{n=1}^N (x_n - \\bar x)(y_n-\\bar y)}{\\sum_{n=1}^{N}(x_n-\\bar x)^2}\\\\\n  \\hat\\beta_0 &= \\bar y - \\hat\\beta_1\\bar x.\n\\end{align*}\\]\nwith \\(\\bar x = \\frac{1}{N}\\sum_{n=1}^{N}x_n\\) and \\(\\bar y = \\frac{1}{N}\\sum_{n=1}^{N}y_n\\) minimizes the minimization problem above. You can assume that the critical points calculated using the partial derivatives are in fact minima and that \\(\\sum_{n=1}^{N}(x_n-\\bar x)^2\\neq 0\\).\n\n\n\nExercise 3.2 Explain the meaning of the following meme in relation to ridge regression and OLS regression:\n\n\n\nSource\n\n\n\n\nExercise 3.3 Consider the following statements and decide whether ridge or lasso regression should be applied.\n\nYou are building a predictive model for stock price prediction, and you have a large number of potential predictors. Some of these predictors might be highly correlated with each other.\nYou are modeling housing prices, and you want to prevent the model from overfitting to the training data.\nYou are working on a marketing project where you have a dataset with a mix of numerical and categorical features. You need to build a regression model to predict customer lifetime value.\n\n\n\nExercise 3.4 Come up with a scenario where a mixed model, i.e. an elastic net might be a good choice.\n\n\n\n3.2.2 Programming Exercises\nIn the following exercises, we will revisit the ImmoScout data set Apartment rental offers in Germany, but instead of considering rent prices in Augsburg, we will now consider rent prices in Munich. The data set can be downloaded using the button below.\n\nDownload MunichRental\n\nIt contains 4383 unique rental listings for flats in Munich, sourced on three dates in 2018 and 2019, and 28 different variables. We will briefly prepare the data set, create a recipe, and create a workflow before training and tuning different models.\n\nExercise 3.5 Import the data set and apply the following steps to the data:\n\nRemove the following columns from the data set\n\nc(\"serviceCharge\",\"heatingType\",\"picturecount\",\"totalRent\",\n        \"firingTypes\",\"typeOfFlat\",\"noRoomsRange\", \"petsAllowed\",\n        \"livingSpaceRange\",\"regio3\",\"heatingCosts\", \"floor\",\n        \"date\", \"pricetrend\")\n\nRemove all the NA values.\nNext, mutate the data as follows:\n\nConvert the feature interiorQual to an ordered factor variable using the following levels:\n\nc(\"simple\", \"normal\", \"sophisticated\", \"luxury\")\n\nConvert the feature condition to an ordered factor variable using the following levels:\n\nc(\"need_of_renovation\", \"negotiable\",\"well_kept\",\"refurbished\",\n         \"first_time_use_after_refurbishment\",\n         \"modernized\", \"fully_renovated\", \"mint_condition\",\n         \"first_time_use\")\n\nConvert the feature geo_plz to an unordered factor.\nConvert every logical feature into a numerical feature such that TRUE corresponds to 1 and FALSE to 0.\nRemove any flat that costs more than 4000 EUR or is bigger than 200 \\(m^2\\) from the data set.\n\n\n\n\nExercise 3.6 Use the seed 24 and create a training and test split with \\(80\\%\\) training data based on the newly mutated data set. Then, with the same seed, create a \\(5\\)-fold cross-validation data set from the training data.\n\n\nExercise 3.7 Explain the following recipe by describing each function.\n\nrec_rent &lt;- recipe(\n    formula = baseRent ~., \n    data = data_train\n  ) %&gt;%\n  update_role(scoutId, new_role = \"ID\") %&gt;%\n  step_ordinalscore(interiorQual, condition)%&gt;%\n  step_dummy(geo_plz)%&gt;%\n  step_zv(all_predictors())\n\n\n\nExercise 3.8 In the previous exercises, we initially mutated the data set (cf. Exercise 3.5) before adding mutations using the recipe function. This procedure can add unnecessary complexity or confusion since the preprocessing steps are spread across the markdown document.\nTherefore, modify the code of Exercises Exercise 3.6 and Exercise 3.7 so that the preprocessing steps of Exercise 3.5 are included in the recipe.\nHint: You can use the step_select, step_mutate, step_naomit, step_novel, and step_dummy functions for incorporating the preprocessing steps.\n\n\nExercise 3.9 Create a lasso model with penalty set to tune. Then, use the following penalty values for tuning the lasso model. Finally, create a new workflow object to add the model specification and recipe.\n\npenalty_vals &lt;- tibble(penalty = seq(0, 20, length.out = 500))\n\n\n\n\nExercise 3.10 Train the lasso model using cross-validation across all penalty values. Then, evaluate the results by finding the optimal penalty value and the optimal penalty value according to the one standard error rule with respect to the metric MAE.\n\n\nExercise 3.11 Consider the following plot, which depicts the mean out-of-sample RMSE for different penalty values. The dashed lines represent the optimal penalty and the largest penalty value, such that the mean MSE is within one standard error of the optimum.\nDecide and present an argument for which line is the optimal penalty. Furthermore, explain why we would choose the non-optimal penalty in lasso regression.\n\n\n\n\n\n\n\n\n\n\n\nExercise 3.12 Given the optimal penalty, train the Train a lasso model with the optimal penalty value on the whole training data and find out which parameters are set to \\(0\\). Then, evaluate the model on the test data by calculating the out of sample MAE and \\(R^2\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "03_regularization.html#solutions",
    "href": "03_regularization.html#solutions",
    "title": "3  Regularization",
    "section": "3.3 Solutions",
    "text": "3.3 Solutions\n\nSolution 3.1 (Exercise 3.1). First, calculate the partial derivatives of\n\\[\\begin{equation}\\label{eq:Loss_sol}\nL(\\beta_0,\\beta_1) = \\frac{1}{N}\\sum_{n=1}^{N} (y_n - (\\beta_0 + \\beta_1 x_n))^2\n\\end{equation}\\]\nwith respect to \\(\\beta_1\\) and \\(\\beta_0\\).\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial \\beta_0} L(\\beta_0,\\beta_1) &= \\frac{1}{N}\\sum_{n=1}^{N} -2(y_n-(\\beta_0 + \\beta_1x_n))\\\\\n&= -2\\bar y + 2\\beta_0 +2\\beta_1 \\bar x \\\\\n&\\overset{!}{=}0 \\\\\n\\frac{\\partial}{\\partial \\beta_1} L(\\beta_0,\\beta_1) &= \\frac{1}{N}\\sum_{n=1}^{N} -x_n(2(y_n-(\\beta_0+\\beta_1x_n))) \\\\\n&= -2\\overline{xy} + 2\\frac{1}{N}\\sum_{n=1}^{N} x_n\\beta_0+\\beta_1x_n^2\\\\\n&= -2\\overline{xy} + 2\\beta_0 \\bar x + 2\\beta_1 \\overline{x^2}\\\\\n&\\overset{!}{=}0\n\\end{align*}\\]\nSolving the first term for \\(\\beta_0\\) yields\n\\[\\begin{equation*}\n\\beta_0 = \\bar y - \\beta_1\\bar x.\n\\end{equation*}\\]\nIn order to solve the second term for \\(\\beta_1\\) we can utilize this newly acquired representation of \\(\\beta_0\\):\n\\[\\begin{equation*}\n  -2\\overline{xy} + 2\\beta_0 \\bar x + 2\\beta_1 \\overline{x^2} = -2\\overline{xy} + 2(\\bar y - \\beta_1\\bar x) \\bar x + 2\\beta_1 \\overline{x^2} \\overset{!}{=}0.\n\\end{equation*}\\]\nThen,\n\\[\\begin{align*}\n-2\\overline{xy} + 2(\\bar y - \\beta_1\\bar x) \\bar x + 2\\beta_1 \\overline{x^2}\n&= -2\\overline{xy} + 2\\bar y \\bar x- 2\\beta_1\\bar x^2+ 2\\beta_1 \\overline{x^2}\\\\\n&\\overset{!}{=} 0.\n\\end{align*}\\]\nThis can easily be solved for \\(\\beta_1\\), which yields\n\\[\\begin{equation*}\n\\beta_1 = \\frac{\\sum_{n=1}^N (x_n - \\bar x)(y_n-\\bar y)}{\\sum_{n=1}^{N}(x_n-\\bar x)^2}.\n\\end{equation*}\\]\nNote, that\n\\[\\begin{align*}\n\\sum_{n=1}^N (x_n - \\bar x)(y_n-\\bar y) &= \\sum_{n=1}^N x_n y_n -\\bar xy_n-x_n\\bar y + \\bar x \\bar y\\\\\n&= N(\\overline{xy} - \\bar x\\bar y - \\bar x \\bar y + \\bar x \\bar y)\\\\\n&= N(\\overline{xy}-\\bar x\\bar y)\n\\end{align*}\\]\nand\n\\[\\begin{align*}\n\\sum_{n=1}^{N}(x_n-\\bar x)^2 &= \\sum_{n=1}^{N}x_n^2 - 2x_n\\bar x + \\bar x^2\\\\\n&= N(\\overline{x^2} - 2\\bar x ^2 + \\bar x^2)\\\\\n&= N(\\overline{x^2} - \\bar x ^2)\n\\end{align*}\\]\n\n\nSolution 3.2 (Exercise 3.2). The key point addresses the bias-variance trade-off.\nFrom the lecture, we know that\n\\[\\begin{equation*}\n\\hat \\beta_{\\mathrm{ridge}}(\\lambda) = \\frac{\\hat\\beta_{\\mathrm{OLS}}}{1+\\lambda}\n\\end{equation*}\\]\nif the features are standardized and orthogonal.\n\nBias:\nBy growing the parameter \\(\\lambda\\), the parameter \\(\\hat\\beta_{\\mathrm{OLS}}\\) shrinks. In other words, the regularization term encourages the model to have smaller coefficient values, which means it may not fit the training data as closely as an unregularized model. This means that systematic errors are introduced to the model’s predictions.\nVariance:\nBy growing the parameter \\(\\lambda\\), we reduce variance by shrinking the coefficients’ values, which discourages them from taking very high values. This effectively constrains the model’s complexity and makes it less prone to overfitting.\n\n\n\nSolution 3.3 (Exercise 3.3). \n\nLasso regression should be used in this scenario because it can perform feature selection by driving some coefficients to zero. This is especially helpful if there are many features as it helps in dealing with correlated predictors.\nRidge regression is more suitable because it provides a smoother and more continuous shrinkage of coefficients, which reduces the risk of overfitting.\nLasso regression might be a more suitable choice as it can perform feature selection and even drive the coefficient for some categorical values to 0.\n\n\n\nSolution 3.4 (Exercise 3.4). A healthcare dataset is given to predict a patient’s readmission probability with numerous correlated features. The aim is to build a model that predicts accurately, selects the most relevant features, and mitigates multicollinearity. Here, an elastic net is the preferred choice because it combines Ridge and Lasso regression, effectively handling multicollinearity while performing feature selection, making it ideal for this complex healthcare dataset.\n\n\nSolution 3.5 (Exercise 3.5). \n\ndata_muc_filtered &lt;- data_muc %&gt;%\n  select(!c(\"serviceCharge\",\"heatingType\",\"picturecount\",\"totalRent\",\n            \"firingTypes\",\"typeOfFlat\",\"noRoomsRange\", \"petsAllowed\",\n            \"livingSpaceRange\",\"regio3\",\"heatingCosts\", \"floor\",\n            \"date\", \"pricetrend\")) %&gt;%\n  na.omit %&gt;%\n  mutate(\n    interiorQual = factor(\n      interiorQual,\n      levels = c(\"simple\", \"normal\", \"sophisticated\", \"luxury\"),\n      ordered = TRUE\n    ),\n    condition = factor(\n      condition,\n      levels = c(\"need_of_renovation\", \"negotiable\",\"well_kept\",\"refurbished\",\n                 \"first_time_use_after_refurbishment\",\n                 \"modernized\", \"fully_renovated\", \"mint_condition\",\n                 \"first_time_use\"),\n      ordered = TRUE\n    ),\n    geo_plz = factor(geo_plz)\n  ) %&gt;%\n  filter(baseRent &lt;= 4000, livingSpace &lt;= 200)\n\n\n\nSolution 3.6 (Exercise 3.6). \n\nset.seed(24)\n\nsplit &lt;- initial_split(data_muc_filtered, 0.8)\ndata_train &lt;- training(split)\ndata_test &lt;- testing(split)\n\nfolds &lt;- vfold_cv(data_train, v = 5)\n\n\n\nSolution 3.7 (Exercise 3.7). \n\nWe first set up the recipe by specifying the formula and data used in each step. Note, that by using the expression baseRent ~. we indicate that we want to fit every variable in the data_train dataset on the dependent variable baseRent.\nThe update_role function assigns the feature scoutId to a new role called ID. By doing so, the feature scoutId is no longer used as a predictor and will no longer influence our model. We will still keep it in the loop, however in case we want to access a specific listing that is only accessible using the unique scoutId.\nWe then convert the factors interiorQual and condition to ordinal scores by using the step_ordinalscore function. The translation uses a linear scale, i.e. for the feature interiorQual the level simple corresponds to the value 0 and luxury corresponds to the value 4.\nAfterward, we create dummy variables for each zip code. Here, every zip code in the data_train is treated as a new variable. Thus, we are basically replacing the feature geo_plz with 82 new features, each representing one of the 82 zip codes available in the training data.\nThe step_zv (zero variance filter) function removes all variables that contain only a single value. If, for example, a zip code does not occur in any of the entries of data_train, the whole column will be set to zero and effectively not affect our model. Thus it is in our best interest to remove those columns.\n\n\n\nSolution 3.8 (Exercise 3.8). Creating a new data split based on the unmutated data.\n\nset.seed(24)\n\nsplit &lt;- initial_split(data_muc, 0.8)\ndata_train &lt;- training(split)\ndata_test &lt;- testing(split)\n\nfolds &lt;- vfold_cv(data_train, v = 5)\n\nInserting the previous preprocessing steps into the recipe framework. Note, that this only requires changing:\n\nselect to step_select,\nna.omit to step_naomit,\nmutate to step_mutate, and\nfilter to step_filter.\n\n\nrec_rent &lt;- recipe(\n    formula = baseRent ~., \n    data = data_train\n  ) %&gt;%\n  update_role(scoutId, new_role = \"ID\") %&gt;%\n  step_select(!c(\"serviceCharge\",\"heatingType\",\"picturecount\",\"totalRent\",\n            \"firingTypes\",\"typeOfFlat\",\"noRoomsRange\", \"petsAllowed\",\n            \"livingSpaceRange\",\"regio3\",\"heatingCosts\", \"floor\",\n            \"date\", \"pricetrend\")) %&gt;%\n  step_naomit(all_predictors()) %&gt;%\n  step_mutate(\n    interiorQual = factor(\n      interiorQual,\n      levels = c(\"simple\", \"normal\", \"sophisticated\", \"luxury\"),\n      ordered = TRUE\n    ),\n    condition = factor(\n      condition,\n      levels = c(\"need_of_renovation\", \"negotiable\",\"well_kept\",\"refurbished\",\n                 \"first_time_use_after_refurbishment\",\n                 \"modernized\", \"fully_renovated\", \"mint_condition\",\n                 \"first_time_use\"),\n      ordered = TRUE\n    ),\n    geo_plz = factor(geo_plz),\n    across(where(is.logical),as.numeric)\n  ) %&gt;%\n  step_filter(baseRent &lt;= 4000, livingSpace &lt;= 200) %&gt;%\n  step_ordinalscore(interiorQual, condition)%&gt;%\n  step_novel(geo_plz)%&gt;%\n  step_dummy(geo_plz)%&gt;%\n  step_zv(all_predictors())\n\n\n\nSolution 3.9 (Exercise 3.9). \n\nmodel_lasso &lt;- linear_reg(penalty = tune(), mixture = 1.0) %&gt;%\n  set_engine(engine = \"glmnet\", path_values = penalty_vals$penalty )\n\nwf_rent &lt;- workflow() %&gt;%\n  add_recipe(rec_rent) %&gt;%\n  add_model(model_lasso)\n\n\n\nSolution 3.10 (Exercise 3.10). \n\nlasso_tune_res &lt;- \n  wf_rent %&gt;% \n  tune_grid(\n    grid = penalty_vals,\n    metrics = multi_metric,\n    resamples = folds\n  )\n\ntib &lt;- lasso_tune_res %&gt;% collect_metrics %&gt;%\n  pivot_wider(\n    names_from = .metric,\n    values_from = c(mean, std_err)\n)\n\npenalty_one_std &lt;- select_by_one_std_err(\n  lasso_tune_res,\n  metric = \"mae\",\n  desc(penalty)\n  ) %&gt;%\n  pull(penalty)\n\npenalty_opt &lt;- select_best(lasso_tune_res, metric = \"mae\") %&gt;% pull(penalty)\n\nglue::glue(\"The optimal penalty is {round(penalty_opt,2)} and \\n the optimal penalty according to the one standard error rule is {round(penalty_one_std,2)}.\")\n\nThe optimal penalty is 0 and \nthe optimal penalty according to the one standard error rule is 20.\n\n\n\n\nSolution 3.11 (Exercise 3.11). The red line depicts the optimal penalty, since it intersects the minimum of the RMSE. Especially in Lasso regression an optimal penalty parameter is often smaller than we desire. The effect of a smaller penalty parameter is, that we do not eliminate as many features as we anticipated. By increasing the penalty we can effectively overcome this problem as more features are eliminated. A disadvantage however is, that we sacrefice out-of-sample performance, as the newly chosen penalty is not optimal anymore.\n\n\nSolution 3.12 (Exercise 3.12). \n\nglm_res_best&lt;- lasso_tune_res %&gt;%\n  select_by_one_std_err(metric = \"mae\", desc(penalty))\n\nbest_penalty &lt;- glm_res_best$penalty\n\n\nlast_glm_model &lt;- linear_reg(penalty = best_penalty, mixture = 1.0) %&gt;%\n  set_engine(\"glmnet\")\n\nwf_rent_final &lt;- wf_rent %&gt;% \n  update_model(last_glm_model)\n\nlast_glm_fit &lt;- \n  wf_rent_final %&gt;% \n  last_fit(split,\n           metrics = multi_metric)\n\nlast_glm_fit %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rsq     standard       0.824 pre0_mod0_post0\n2 mae     standard     254.    pre0_mod0_post0",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "03_regularization.html#footnotes",
    "href": "03_regularization.html#footnotes",
    "title": "3  Regularization",
    "section": "",
    "text": "The function names prep and bake are references to literal baking recipes: You start out by specifying the ingredients for something you want to bake. Then you prepare the ingredients before finally baking the prepped ingredients.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "04_regression_trees.html",
    "href": "04_regression_trees.html",
    "title": "4  Regression Trees",
    "section": "",
    "text": "4.1 Intermezzo: Imputation\nIn this fourth exercise session, we want to introduce regression trees. Regression trees, up to this day, are still one of the most important statistical models as they address important aspects like interpretability but also versatility in terms of the prediction approach since they are non-linear. Another important aspect is their integration with other, more powerful statistical models like random forests, which we will cover in a later session.\nBesides regression, tree-based models can also be used for classification tasks, however, in this exercise in particular, we will focus on the regression case and consider classification tasks later on.\nSome of the packages we will use throughout the session.\nWhen working with data sets, missing values can disrupt analysis and modeling processes. In previous exercises when working with the rental listing data set, we resorted to either remove missing values completely or (in case of numerical or ordinal features) assign the worst possible value. Both of these approaches have significant drawbacks:\nTo handle these drawbacks, several imputation techniques are commonly used:\nThe approaches above require the missing data to be numerical, else we can’t really calculate a mean or median.\nConsider the following two synthetic data set examples:\nset.seed(123)\n\nn&lt;- 100\n\ndata_synth_high_corr &lt;- tibble(\n  x = seq(0,10,length.out = n) + rnorm(n),\n  y = x + rnorm(n),\n  z = x + y,\n)\n\ndata_synth_low_corr &lt;- tibble(\n  x = seq(0,10,length.out = n),\n  y = exp(cos((x/10)*8*pi))+rnorm(n,0,0.5),\n  z = x+y\n)\nBoth data sets consist of three variables \\(x,y,z\\). In the first data set not only \\(x\\) and \\(y\\) are highly correlated (since \\(y=x+Y\\) with \\(Y\\sim N(0,1)\\)) but also \\(z\\) and \\(x\\) (since \\(z=x+y\\)) and \\(z\\) and \\(y\\) (since \\(z=x+y\\)). In the second data set \\(x\\) and \\(z\\) are still highly correlated, but \\(x\\) and \\(y\\) are not. Therefore, imputing \\(x\\) with \\(y\\) in a setting where correlation matters should influence the performance of the following applied imputaton techniques.\nConsider the case where \\(10\\%\\) of the \\(x\\)-values are removed. We can use \\(y\\) to impute the missing \\(x\\) values:\nIn the figure above we can observe that the highly correlated data is estimated better using the more sophisticated imputation models compared to the simple mean and median imputation.\nWhile sophisticated imputation techniques seem to outperform simple techniques on highly correlated data, either method fails to correctly assign missing values if the data is not correlated.\nHowever, as a rule of thumb, if computational resources are available, I recommend checking for correlation before applying any of the techniques.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression Trees</span>"
    ]
  },
  {
    "objectID": "04_regression_trees.html#intermezzo-imputation",
    "href": "04_regression_trees.html#intermezzo-imputation",
    "title": "4  Regression Trees",
    "section": "",
    "text": "By removing every NA value we potentially remove a large chunk of the underlying data which weakens the training capabilities.\nAssigning the worst possible value to each missing observation introduces a bias which can distort the true underlying distribution.\n\n\n\nMean imputation replaces missing values with the average of the observed data for a given feature, offering simplicity but also risking distortion if the data contains outliers.\nMedian imputation substitutes missing entries with the median value, making it more robust against extreme values.\nLinear imputation estimates the missing values using a linear model making it particularly robust if there are highly correlated features.\n\n\n\nk-nearest neighbors imputation identifies the k most similar records and fills missing values using their observed data. This approach can also handle ordinal or even nominal data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression Trees</span>"
    ]
  },
  {
    "objectID": "04_regression_trees.html#regression-trees-in-r",
    "href": "04_regression_trees.html#regression-trees-in-r",
    "title": "4  Regression Trees",
    "section": "4.2 Regression trees in R",
    "text": "4.2 Regression trees in R\nWe use the same white wine data set as in the previous sessions for the introduction.\n\n4.2.1 Data preprocessing, recipe and workflow creation\nFor detailed description of each parameter see Cortez et al.\nThe goal is to predict the alcohol contents of each wine given different attributes like density and residual sugar.\nThe following code cell contains the usual preprocessing steps required for fitting a model. For details, see tutorial on recipes and workflows.\n\nset.seed(123)\n\ndata_wine &lt;- read.csv(\"data/winequality-white.csv\")\n\nsplit_wine &lt;- initial_split(data_wine)\n\ndata_train_wine&lt;- training(split_wine)\ndata_test_wine&lt;- testing(split_wine)\n\nrec_wine &lt;- recipe(\n  alcohol ~.,\n  data = data_train_wine\n)\n\nwf_wine &lt;- workflow() %&gt;%\n  add_recipe(rec_wine)\n\n\n\n4.2.2 Fitting a basic tree\nFitting a basic tree model is as simple as fitting any other model using the tidymodels framework. The decision_tree function creates an {rpart} tree object. Since decision entail regression and classification trees, we have to specify the mode as \"regression\". Using the fit function, we can directly fit the model and evaluate it on the test set using the augment function and a preferred metric:\n\ntree_spec_basic&lt;- decision_tree(\n  mode = \"regression\"\n)\n\nwf_wine &lt;- wf_wine %&gt;% add_model(tree_spec_basic)\n\ntree_res_basic &lt;- wf_wine %&gt;%\n  fit(data_train_wine)\n\ntree_res_basic %&gt;%\n  augment(data_test_wine) %&gt;%\n  rmse(.pred,alcohol)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.632\n\n\nSince we have not passed any hyperparameters, the following rpart specific default parameters are used:\n\nmin_n = 20,\ntree_depth = round(minsplit/3), and\ncost_complexity = 0.01,\n\nTo potentially obtain better results, the parameters should be tuned. We, therefore, have to define a new model specification and set the tuneable parameters to tune().\n\ntree_spec_tune &lt;- decision_tree(\n  mode = \"regression\",\n  min_n = tune(),\n  tree_depth = tune(),\n  cost_complexity = tune()\n)\n\nIn Exercise 03, Section 3.1.2.3, we specified the candidate hyperparameters by defining a data frame with the respective hyperparameter names and candidate values, or by setting the grid attribute in the tune_grid function to a positive number indicating the number of automatically created candidate parameters.\nInstead of manually creating a data frame, we can use the extract_parameter_set_dials() function to extract all tuneable parameters and pass the output into the grid_regular() function. The grid_regular() function takes a positiv number levels \\(=n\\) as an input and returns a data frame where each column contains \\(n^{m}\\) candidate values, where \\(m\\) is the number of tunable hyperparameters.\n\ntree_grid &lt;- tree_spec_tune %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  grid_regular(levels = 5)\n\nSince there are \\(m = 3\\) hyperparameters and levels is set to \\(5\\), the data frame tree_grid contains \\(125\\) combinations of candidate hyperparameters.\nAfter specifying the candidate hyperparameters, we can tune the models following the standard procedure:\n\nset.seed(123)\n\nfolds_wine &lt;-vfold_cv(data_train_wine,5)\n\nmulti_metric &lt;- metric_set(rmse,rsq)\n\nwf_wine &lt;- wf_wine %&gt;%\n  update_model(tree_spec_tune)\n\ntree_res_tune &lt;- wf_wine %&gt;%\n  tune_grid(\n    grid = tree_grid,\n    resamples = folds_wine,\n    metrics = multi_metric\n  )\n\nGiven the tuning results, we can visualize our finding with the autoplot() function.\n\ntree_res_tune %&gt;% autoplot()+\n  theme_minimal(base_size = 12)+\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nUsing the select_best() function, we can select the best set of hyperparameters. After extracting these, passing them to the finalize_workflow() function together with the workflow object, updates the finalizes the workflow object by replacing the hyperparameters set to tune() with the selected hyperparameters. Passing the finalized workflow into the last_fit() function together with the whole data split fits the final model on the whole training data and evaluates it on the test data.\n\ntree_res_final &lt;- tree_res_tune %&gt;%\n  select_best(metric = \"rmse\") %&gt;%\n  finalize_workflow(wf_wine,.) %&gt;%\n  last_fit(split_wine) \n\ntree_res_final %&gt;%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard       0.496 pre0_mod0_post0\n2 rsq     standard       0.837 pre0_mod0_post0\n\n\nComparing the RMSE of the basic model (\\(0.632\\)) with the tuned model’s RMSE (\\(0.496\\)) shows that the tuning improved the performance of the tree!\n\n\n4.2.3 Visualizing results\nSince the basic tree only has a depth parameter with value \\(5\\) compared to the tuned tree with depth \\(15\\), we will consider the basic tree for a visualization. The {rpart.plot} library contains the eponymous function rpart.plot() that visualizes trees in a top-down fashion. Note: Before applying the rpart.plot() function, the fit_engine has to be extracted from the model. Strictly speaking, the object tree_res_basic contains a workflow object which in turns contains the actual model.\nEach node contains the percentage of samples at the specific level in the bottom entry and an estimate in the top entry. For example, after the first split, the left node contains \\(63\\%\\) of all samples and the estimate for the alcohol contents is \\(9.8\\) percent. The node on the right contains the remaining \\(37\\%\\) percent with an estimated alcohol content of \\(12\\) percent. The branches of the tree contain splitting conditions.\n\nlibrary(rpart.plot)\n\ntree_res_basic %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot(roundint=FALSE)\n\n\n\n\n\n\n\n\n\n\n4.2.4 cp-table\nThe complexity parameter (cp) table is a useful tool for asessing model performance as well. It contains the complecity parameter itself, and for each value the respective split, relative error, cross-validation error and cross-validation standard deviation.\n\ntree_res_basic %&gt;%\n  extract_fit_engine() %&gt;%\n  printcp()\n\n\nRegression tree:\nrpart::rpart(formula = ..y ~ ., data = data)\n\nVariables actually used in tree construction:\n[1] chlorides      density        residual.sugar\n\nRoot node error: 5587.9/3673 = 1.5213\n\nn= 3673 \n\n        CP nsplit rel error  xerror      xstd\n1 0.524872      0   1.00000 1.00029 0.0186933\n2 0.065803      1   0.47513 0.48862 0.0114048\n3 0.063815      2   0.40932 0.41249 0.0100885\n4 0.030752      3   0.34551 0.36373 0.0091955\n5 0.024820      4   0.31476 0.33361 0.0087150\n6 0.022225      5   0.28994 0.31958 0.0081634\n7 0.010587      6   0.26771 0.29882 0.0076719\n8 0.010347      7   0.25713 0.27522 0.0074280\n9 0.010000      8   0.24678 0.27426 0.0074155\n\n\nThe cptable provides, in addition to the error reduction from a split, further information which we will analyze in the following.\n\nThe column CP (Complexity Parameter) indicates an improvement value for each split.\nIn the first row, this means the improvement value from splitting the root node is calculated.\nThe subsequent improvement values also refer to the improvement relative to the error of the root node, which is why the following formula is used:\n\\[\\begin{align}\\label{eq:lorem}\n\\frac{|K_{\\text{parent}}|\\cdot\\text{MSE}_{K_{\\text{parent}}} -(|K_{\\text{child}_1}|\\cdot\\text{MSE}_{K_{\\text{child}_1}}+|K_{\\text{child}_2}|\\cdot\\text{MSE}_{K_{\\text{child}_2}})}{|K_{\\text{root}}|\\cdot\\text{MSE}_{K_{\\text{root}}}}\n\\end{align}\\]\nIn equation \\(\\eqref{eq:lorem}\\), the denominator is no longer \\(|K_{\\text{parent}}|\\cdot\\text{MSE}_{K_{\\text{parent}}}\\), but instead \\(|K_{\\text{root}}|\\cdot\\text{MSE}_{K_{\\text{root}}}\\).\nThe value CP thus describes the proportion of error improvement a split creates, relative to the total error of the model without any splits (i.e., only the root node).\nSo, for example, to calculate the CP value in the second row, we can use the tabular-hierarchical representation:\n\n\nn= 3673 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 3673 5587.8860 10.522440  \n   2) density&gt;=0.992695 2320 1499.3140  9.840027  \n     4) density&gt;=0.995555 1088  359.0238  9.416391  \n       8) density&gt;=0.99759 531  104.7181  9.180289 *\n       9) density&lt; 0.99759 557  196.4870  9.641472 *\n     5) density&lt; 0.995555 1232  772.5910 10.214150  \n      10) residual.sugar&lt; 8.15 1006  490.8750 10.055120  \n        20) chlorides&gt;=0.0475 440  177.2503  9.780076 *\n        21) chlorides&lt; 0.0475 566  254.4635 10.268930 *\n      11) residual.sugar&gt;=8.15 226  143.0244 10.922040 *\n   3) density&lt; 0.992695 1353 1155.6450 11.692570  \n     6) density&gt;=0.990315 985  665.7154 11.378780  \n      12) residual.sugar&lt; 4.05 730  384.3869 11.131920  \n        24) density&gt;=0.99147 374  141.8090 10.729500 *\n        25) density&lt; 0.99147 356  118.3850 11.554680 *\n      13) residual.sugar&gt;=4.05 255  109.4894 12.085480 *\n     7) density&lt; 0.990315 368  133.3411 12.532470 *\n\n\nThe CP value is then calculated using the deviance terms that result from splitting node 3) into nodes 6) and 7):\n\\[\\begin{equation*}\n\\frac{1155.6450-(665.7154+133.3411)}{5587.89} = 0.0638\n\\end{equation*}\\]\nThe last entry in the column CP (0.01) is the default parameter of the decision_tree function for the argument cp.\nThe column nsplit describes the number of splits. Since the last entry is 9, this means the feature space was split nine times.\nThe column rel error describes the relative error of the decision tree at the respective split in relation to the root node. This can be calculated using the formula:\n\\[\\begin{equation*}\n\\text{rel error}i = \\begin{cases}\n  1,\\qquad &\\text{if } i=1\\\\\n  \\text{rel error}{i-1}-\\text{CP}_{i-1},\\qquad &\\text{if } i&gt;1\n  \\end{cases}\n\\end{equation*}\\]\nSo, for example, to calculate the rel err for the last entry \\((i=9)\\), the formula with numbers inserted is:\n\\[\\begin{equation*}\n0.25713-0.0103 = 0.246783\n\\end{equation*}\\]\nThe columns xerror and xstd describe the rel err and its standard deviation, which result from a cross-validation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression Trees</span>"
    ]
  },
  {
    "objectID": "04_regression_trees.html#exercises",
    "href": "04_regression_trees.html#exercises",
    "title": "4  Regression Trees",
    "section": "4.3 Exercises",
    "text": "4.3 Exercises\n\n4.3.1 Theoretical exercises\nIn this first exercise, we want to gain intuition for the theoretical aspects of regression trees.\nBefore diving into the process of building and evaluating a tree rigorously, we first consider different representations of binary trees, check their validity, and decide for simple datasets, whether they are suitable for regression trees.\n\nExercise 4.1 Consider the following two splits of the feature space generated by two features \\(X_1\\) and \\(X_2\\). Argue, which one of the splits was generated by a binary splitting tree!\n\n\n\n\n\n\n\nExercise 4.2 Consider the following split generated by a binary tree. \\(t_1,…,t_4\\) denote the splitting conditions, \\(R_1,…,R_4\\) the final regions, and \\(X_1,X_2\\) the variables used for evaluating the splitting conditions.\nDraw a binary tree that corresponds to the split given below.\n\n\n\n\n\n\n\nExercise 4.3 For the following scatterplots, decide whether a simple linear model ( \\(y=\\hat \\beta_1x+\\hat \\beta_0\\) ) or a regression tree should be chosen for modeling the data.\n\n\n\n\n\n\n\n\n\n\n\nExercise 4.4 Now, that we have considered some visual examples of trees and gained an intuition of situations where trees might be a suitable model, we now want to focus on the process of building a tree.\nConsider the following dataset. Calculate the first optimal splitting point with respect to \\(x\\).\n\ndata_tmp &lt;- tibble(\n  x = c(1,0.5,2.0,5.5,4.5),\n  y = c(10,7,8,3,4)\n)\ndata_tmp\n\n# A tibble: 5 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1   1      10\n2   0.5     7\n3   2       8\n4   5.5     3\n5   4.5     4\n\n\nIn order to do so, you have to proceed as follows:\n\nDerive the order statistics of \\(\\{x_1,...,x_n\\}\\)\nDerive the set \\(S := \\left\\{\\frac{1}{2}(x_{(r)}+x_{(r+1)}):r=1,...,n-1\\right\\}\\) of all potential splitting points.\nFor each potential splitting point, derive the regions \\(R_1\\) and \\(R_2\\) and calculate the estimate \\(\\hat y_1\\) and \\(\\hat y_2\\) for the respective regions.\nCalculate the loss \\(\\mathcal{L}(y,\\hat y) := \\sum_{i:x_i\\in R_1} (y_i-\\hat y)^2 + \\sum_{i:x_i\\in R_2}(y_i-\\hat y)^2\\).\nDerive the optimal splitting point by settling for the splitting point leading to the smallest loss \\(\\mathcal{L}\\).\n\n\n\nExercise 4.5 Given the tibble data, create a simple scatter plot and add a dashed line indicating the initial splitting point. An example of what such a plot could look like can be found below.\n\n\n\n\n\n\n\n\n\n\n\nExercise 4.6 Calculate the improvement of the given split. Recall, that the improvement of a split is given by\n\\[\n\\frac{\\mathrm{MSE}_1 \\cdot n_1 - (\\mathrm{MSE}_2 \\cdot n_2 +  \\mathrm{MSE}_3 \\cdot n_3)}{\\mathrm{MSE_1}\\cdot n_1},\n\\]\nwhere \\(\\mathrm{MSE}_1\\) denotes the mean squared error of the region before the split and \\(\\mathrm{MSE_2}\\) and \\(\\mathrm{MSE_3}\\) are the mean square errors of the respective regions after the split. \\(n_i,\\, i=1,2,3\\) denotes the number of samples in the respective region.\n\n\n\n4.3.2 Programming Exercises\nIn this exercise, we want to apply our theoretical knowledge to training a tree-based model on the Apartment rental offers in Germany dataset. As in Session 03 we will be using the rental offers in Munich to build a predictive model for the base rent.\n\nExercise 4.7 Import the data set, create a training/testing split and a \\(5\\)-fold CV object on the training data using set.seed(24).\n\n\nExercise 4.8 Explain the syntax and semantics of the following code snippet.\n\ndata_muc %&gt;%\n  select_if(where(~sum(is.na(.))&gt;0)) %&gt;%\n  is.na() %&gt;%\n  colSums() %&gt;%\n  tibble( names = names(.),\n          n = .) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 12 × 2\n   names               n\n   &lt;chr&gt;           &lt;dbl&gt;\n 1 heatingCosts     3565\n 2 petsAllowed      2013\n 3 interiorQual     1584\n 4 firingTypes      1243\n 5 condition        1206\n 6 heatingType      1108\n 7 typeOfFlat        847\n 8 yearConstructed   751\n 9 floor             631\n10 totalRent         440\n11 serviceCharge     150\n12 pricetrend         22\n\n\n\n\nExercise 4.9 Create a recipe based on the following description:\n\nCreate a recipe for a regression model with baseRent as the target variable and all other columns as predictors.\nUpdate the role of the scoutId column to “ID”.\nRemove the following specified columns from the dataset:\n\nc(\"serviceCharge\",\"heatingType\",\"picturecount\",\"totalRent\",\n      \"firingTypes\",\"typeOfFlat\",\"noRoomsRange\", \"petsAllowed\",\n      \"livingSpaceRange\",\"regio3\",\"heatingCosts\", \"floor\",\n      \"date\", \"pricetrend\")\n\n [1] \"serviceCharge\"    \"heatingType\"      \"picturecount\"     \"totalRent\"       \n [5] \"firingTypes\"      \"typeOfFlat\"       \"noRoomsRange\"     \"petsAllowed\"     \n [9] \"livingSpaceRange\" \"regio3\"           \"heatingCosts\"     \"floor\"           \n[13] \"date\"             \"pricetrend\"      \n\n\nConvert interiorQual and condition into ordered factors with specified levels and geo_plz into an unordered factor.\nCreate a specification that assigns a previously unseen factor level to new using the step_novel() function.\nConvert geo_plz into dummy variables.\nCreate ordinal scores for every ordered predictor.\nImpute missing values for all ordered predictors using k-nearest neighbors.\nFilter rows in the dataset to retain only observations where baseRent is at most \\(4000\\) EUR and livingSpace is at most \\(200\\) sqm.\n\n\n\nExercise 4.10 Create an instance of the decision_tree class where the parameters min_n,tree_depth, and cost_complexity are set to tune.\n\n\nExercise 4.11 Create a workflow and add the previously specified model and recipe.\n\n\nExercise 4.12 Instead of specifying the grid manually, use the extract_parameter_set_dials function to create a regular grid with \\(4\\) levels.\n\n\n\nExercise 4.13 Tune the model on the cross-validation set created in Exercise 4.7. As for the Use the following metric set for evaluating the tuning results.\n\nmulti_metric &lt;- metric_set(rmse,rsq)\n\n\n\nExercise 4.14 Given the following plot. What can you say about the relationship between the tree parameters Tree Depth, Minimal Node Size, and Cost-Complexity Parameter with respect to the RMSE?\n\n\n\n\n\n\n\nExercise 4.15 Select the best model with respect to the metric MAE and fit a final model using these parameters. Then, fit the best model on the whole training data and evaluate it on the test data using the previously defined metrics set.\n\n\nExercise 4.16 It is usually easier to get a feeling for model performance by visualizing the results. One way to do that would be to plot the predicted values of our model against the true values. By adding a simple line through the origin with slope one, we can then evaluate the estimates as follows:\nPoints that are closely scattered around this line are well predicted, whereas points further away from this line indicate that the model performed badly.\nThe goal of this exercise is for you to rebuild the plot that is depicted below.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression Trees</span>"
    ]
  },
  {
    "objectID": "04_regression_trees.html#solutions",
    "href": "04_regression_trees.html#solutions",
    "title": "4  Regression Trees",
    "section": "4.4 Solutions",
    "text": "4.4 Solutions\n\nSolution 4.1 (Exercise 4.1). Split 1 can’t be produced by a binary tree, because the bottom-center rectangle is overlapping the left-most rectangle.\n\n\nSolution 4.2 (Exercise 4.2). \n\n\nSolution 4.3 (Exercise 4.3). \n\nFor the data in the first plot, we should use a simple linear model, as the data seems to follow a simple linear trend.\nA linear model is most likely not suitable for modelling this data, as the shape of the cloud of points looks more like a parabola instead of a line.\nAs the third plot consists of points that can be assigned to four (almost) distinct regions, a regression tree seems to be more suitable than a linear model.\nAt first, the data in the fourth plot seems to be too messy to make a decision. However, upon closer inspection, there are several indicators that a linear model might perform better:\n\nThe points in the center seem to follow a positive linear trend.\nThe deviations of points around this linear trend seem to be distributed in a way, that there are more points towards the line than further away. So the residuals could be assumed to be normally distributed.\n\n\n\n\nSolution 4.4 (Exercise 4.4). \n\nloss_x&lt;- function(data,r) {\n  xr = sort(data$x)[r]\n  y1 &lt;- mean(data$y[data$x&lt;=xr])\n  y2 &lt;- mean(data$y[data$x&gt;xr])\n\n  loss&lt;-sum((data$y[data$x&lt;=xr]-y1)^2) + sum((data$y[data$x&gt;xr]-y2)^2)\n  \n  return(loss)\n}\n\nSince we are interested in finding the optimal split with respect to \\(x_1=x\\), consider the sets of all possible splits \\[\\begin{equation*}\n  S := \\left\\{\\frac{1}{2}(x_{(r)}+x_{(r+1)}):r=1,...,n-1\\right\\} = \\{0.75,1.5,3.25,5\\}.\n\\end{equation*}\\]\nHere, \\(\\{x_{(r)},\\, r = 1,...,n\\} = \\{0.5,1,2,4.5,5.5\\}\\) denotes the order statistic of \\(x\\).\nFor \\(r=1\\) we have \\(s = 0.75\\) and \\[\\begin{align*}\nR_1(1,0.75) &= \\{x: x \\leq 0.75\\} = \\{0.5\\},\\\\\nR_2(1,0.75) &= \\{x: x &gt; 0.75\\} = \\{1.0,2.0,5.5,4.5\\}.\n\\end{align*}\\]\nThen,\n\\[\\begin{align*}\n\\hat y_1 &= \\frac{1}{|R_1|}\\sum_{i:x_{i}\\in R_1} y_i = \\frac{1}{1}\\cdot 7 = 7,\\\\\n\\hat y_2 &= \\frac{1}{|R_2|}\\sum_{i:x_{i}\\in R_2} y_i = \\frac{1}{4}(10+8+3+4) = 6.25.\n\\end{align*}\\]\nGiven the above, we can calculate the Loss with respect to \\(s = 0.75\\), which is given by\n\\[\\begin{align*}\n\\mathcal{L}(y,\\hat y) &= \\sum_{i:x_{i}\\in R_1} (y_i-\\hat y_{R_1})^2 + \\sum_{i:x_{i}\\in R_2} (y_i-\\hat y_{R_2})^2 \\\\\n&= (7 -7)^2 + (10-6.25)^2 + (8-6.25)^2 + (3-6.25)^2 + (4-6.25)^2\\\\\n&= 32.75\n\\end{align*}\\]\n\nloss_x(data_tmp,1)\n\n[1] 32.75\n\n\nFor \\(r=2\\) we have \\(s = 1.5\\) and \\[\\begin{align*}\nR_1(1,1.5) &= \\{x: x \\leq 1.5\\} = \\{0.5,1.0\\},\\\\\nR_2(1,1.5) &= \\{x: x &gt; 1.5\\} = \\{2.5,5.5,4.5\\}.\n\\end{align*}\\] Then, \\[\\begin{align*}\n\\hat y_1 &= \\frac{1}{|R_1|}\\sum_{i:x_{i}\\in R_1} y_i = \\frac{1}{2}\\cdot (7+10) = 8.5,\\\\\n\\hat y_2 &= \\frac{1}{|R_2|}\\sum_{i:x_{i}\\in R_2} y_i = \\frac{1}{3}(8+3+4) = 5.\n\\end{align*}\\] Given the above, we can calculate the Loss with respect to \\(s = 1.5\\), which is given by\n\\[\\begin{align*}\n\\mathcal{L}(y,\\hat y) &= \\sum_{i:x_{i}\\in R_1} (y_i-\\hat y_{R_1})^2 + \\sum_{i:x_{i}\\in R_2} (y_i-\\hat y_{R_2})^2 \\\\\n&= (7-8.5)^2 + (10-8.5)^2  + (8-5)^2 + (3-5)^2 + (4-5)^2\\\\\n&= 18.5\n\\end{align*}\\]\n\nloss_x(data_tmp,2)\n\n[1] 18.5\n\n\nFor \\(r=3\\) we have \\(s = 3.25\\) and \\[\\begin{align*}\nR_1(1,3.25) &= \\{x: x \\leq 3.25\\} = \\{0.5,1.0, 2.5 \\},\\\\\nR_2(1,3.25) &= \\{x: x &gt; 3.25\\} = \\{5.5,4.5\\}.\n\\end{align*}\\] Then, \\[\\begin{align*}\n\\hat y_1 &= \\frac{1}{|R_1|}\\sum_{i:x_{i}\\in R_1} y_i = \\frac{1}{3}\\cdot (7+10+8) = 8.333,\\\\\n\\hat y_2 &= \\frac{1}{|R_2|}\\sum_{i:x_{i}\\in R_2} y_i = \\frac{1}{2}(3+4) = 3.5.\n\\end{align*}\\] Given the above, we can calculate the Loss with respect to \\(s = 4\\), which is given by\n\\[\\begin{align*}\n\\mathcal{L}(y,\\hat y) &= \\sum_{i:x_{i}\\in R_1} (y_i-\\hat y_{R_1})^2 + \\sum_{i:x_{i}\\in R_2} (y_i-\\hat y_{R_2})^2 \\\\\n&= (7-8.333)^2 + (10-8.333)^2  + (8-8.333)^2 + (3-3.5)^2 + (4-3.5)^2\\\\\n&= 5.167\n\\end{align*}\\]\n\nloss_x(data_tmp,3)\n\n[1] 5.166667\n\n\nFor \\(r=4\\) we have \\(s = 5\\) and \\[\\begin{align*}\nR_1(1,5) &= \\{x: x \\leq 5\\} = \\{0.5,1.0, 2.5,4.5 \\},\\\\\nR_2(1,5) &= \\{x: x &gt; 5\\} = \\{5.5\\}.\n\\end{align*}\\] Then, \\[\\begin{align*}\n\\hat y_1 &= \\frac{1}{|R_1|}\\sum_{i:x_{i}\\in R_1} y_i = \\frac{1}{4}\\cdot (7+10+8+4) = 7.25,\\\\\n\\hat y_2 &= \\frac{1}{|R_2|}\\sum_{i:x_{i}\\in R_2} y_i = \\frac{1}{1}\\cdot 3 = 3.\n\\end{align*}\\] Given the above, we can calculate the Loss with respect to \\(s = 5\\), which is given by\n\\[\\begin{align*}\n\\mathcal{L}(y,\\hat y) &= \\sum_{i:x_{i}\\in R_1} (y_i-\\hat y_{R_1})^2 + \\sum_{i:x_{i}\\in R_2} (y_i-\\hat y_{R_2})^2 \\\\\n&= (7-7.25)^2 + (10-7.25)^2  + (8-7.25)^2 + (4-7.25)^2 + (3-3)^2\\\\\n&= 18.75\n\\end{align*}\\]\n\nloss_x(data_tmp,4)\n\n[1] 18.75\n\n\nSince \\(\\mathcal{L}(y,\\hat y)\\) is the lowest for \\(r=3\\), i.e., \\(\\mathcal{L}(y,\\hat y) = 5.167\\), \\(s = 3.25\\) is the optimal splitting point with respect to \\(x\\).\n\n\nSolution 4.5 (Exercise 4.5). \n\ntitle_text = \"Scatterplot showing the\n              &lt;span style='color:red'&gt;optimal threshold&lt;/span&gt; &lt;br/&gt;\n              for an initial split with respect to x\"\n\ndata_tmp %&gt;% ggplot(aes(x,y))+\n  geom_point(size = 3, alpha = 0.7) +\n  geom_vline(xintercept = 3.25, linetype = \"dashed\", color = \"red\") +\n  theme_minimal(base_size=11)+\n  theme(\n    plot.title = element_markdown()\n  )+\n  labs( x = \"x\",\n        title = title_text)\n\n\n\n\n\n\n\n\n\n\nSolution 4.6 (Exercise 4.6). The improvement is given by the following term.\n\\[\\begin{equation*}\n\\frac{\\mathrm{MSE}_1 \\cdot n_1 - (\\mathrm{MSE}_2 \\cdot n_2 +  \\mathrm{MSE}_3 \\cdot n_3)}{\\mathrm{MSE_1}\\cdot n_1}\n\\end{equation*}\\]\nCalculating \\(MSE_i\\) for \\(i=1,2,3\\) yields\n\\[\\begin{align*}\nn_1 \\cdot \\mathrm{MSE}_1 &= (10-6.4)^2 + (7-6.4)^2 + (8-6.4)^2 + (3-6.4)^2 + (4-6.4)^2 = 33.2, \\\\\nn_2 \\cdot \\mathrm{MSE}_2 &= (7-8.333)^2 + (10-8.333)^2  + (8-8.333)^2 = 4.667, \\\\\nn_3 \\cdot \\mathrm{MSE}_3 &= (3-3.5)^2 + (4-3.5)^2 = 0.5\\ .\n\\end{align*}\\]\nThe improvement for this split is therefore\n\\[\\begin{equation*}\n  \\frac{\\mathrm{MSE}_1 \\cdot n_1 - (\\mathrm{MSE}_2 \\cdot n_2 +  \\mathrm{MSE}_3 \\cdot n_3)}{\\mathrm{MSE_1}\\cdot n_1} = \\frac{33.2 - (4.667 + 0.5)}{33.2} = 0.8444\n\\end{equation*}\\]\n\n\nSolution 4.7 (Exercise 4.7). \n\ndata_muc &lt;- read.csv(\"data/rent_muc.csv\")\n\ndata_muc &lt;- data_muc %&gt;%\n  select(!c(\"serviceCharge\",\"heatingType\",\"picturecount\",\n             \"totalRent\",   \"firingTypes\",\"typeOfFlat\",\n             \"noRoomsRange\", \"petsAllowed\",\n             \"livingSpaceRange\",\"regio3\",\"heatingCosts\",\n             \"floor\",\"date\", \"pricetrend\")) %&gt;%\n  mutate(\n    interiorQual = factor(\n      interiorQual,\n      levels = c(\"simple\", \"normal\", \"sophisticated\", \"luxury\"),\n      ordered = TRUE\n      ),\n    condition = factor(condition,\n      levels = c(\"need_of_renovation\", \"negotiable\",\"well_kept\",\n                 \"refurbished\",\"first_time_use_after_refurbishment\",\n                 \"modernized\",\"fully_renovated\", \"mint_condition\",\n                 \"first_time_use\"),\n      ordered = TRUE),\n     geo_plz = factor(geo_plz),\n    across(where(is.logical),as.factor)\n  ) %&gt;%\n  filter(baseRent&lt;=3000, livingSpace&lt;=200)\n\n\nset.seed(24)\nsplit_rent &lt;- initial_split(data_muc)\ndata_train &lt;- training(split_rent)\ndata_test &lt;- testing(split_rent)\nfolds &lt;- vfold_cv(data_train, v = 3)\n\n\n\nSolution 4.8 (Exercise 4.8). \n\ndata_muc %&gt;%\n  select_if(where(~sum(is.na(.))&gt;0)) %&gt;%\n  is.na() %&gt;%\n  colSums() %&gt;%\n  tibble( names = names(.),\n          n = .) %&gt;%\n  arrange(desc(n))\n\n\nThe %&gt;% operator passes the data set data_muc to the next function.\nselect_if selects columns in a data frame based on the condition that where(~ sum(is.na(.)) &gt; 0) which checks if the sum of NA values in a column is greater than 0.\nThe is.na() function checks whether an entry in the data set is NA or not and returns True or FALSE respectively. Therefore, by applying the is.na() function, a data set containing only boolean values is returned.\nThe colSums function adds up all values in each column, where TRUE = 1 and FALSE = 0, returning a named vector containing the sum of all TRUE values and the respective variable names.\nThen, the data set is transformed using the tibble() function such that the returned tibble only consists of two columns containing the variable name and column sum.\nIn the last step, the data set is ordered with respect to the number of missing values in descending order using the arrange() and desc() function.\n\n\n\nSolution 4.9 (Exercise 5.7). \n\nrec_rent &lt;- recipe(\n    formula = baseRent ~., \n    data = data_train\n  ) %&gt;%\n  update_role(scoutId, new_role = \"ID\") %&gt;%\n  step_ordinalscore(all_ordered_predictors())%&gt;%\n  step_novel(all_factor_predictors())%&gt;%\n  step_unknown(all_factor_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors())%&gt;%\n  step_impute_mean(all_numeric_predictors()) \n\n\n\nSolution 4.10 (Exercise 4.10). \n\nmodel_rt_tune &lt;-  \n  decision_tree(\n    min_n = tune(),\n    tree_depth = tune(),\n    cost_complexity = tune()\n    ) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"rpart\")\n\n\n\nSolution 4.11 (Exercise 4.11). \n\nwf_rent &lt;- \n  workflow() %&gt;%\n  add_recipe(rec_rent) %&gt;%\n  add_model(model_rt_tune)\n\n\n\nSolution 4.12 (Exercise 4.12). \n\ntree_grid &lt;- wf_rent %&gt;% \n  extract_parameter_set_dials %&gt;% \n  grid_regular(levels = 4)\n\n\n\nSolution 4.13 (Exercise 4.13). \n\nmulti_metric &lt;- metric_set(rmse,rsq,mae)\n\n\nrt_res &lt;- \n  wf_rent %&gt;% \n  tune_grid(\n    grid = tree_grid,\n    metrics = multi_metric,\n    resamples = folds\n  )\n\nTo speed things up, the following tibble already contains the optimal parameters according to the tuning procedure:\n\nrt_res_tune_best &lt;- tibble(\n  min_n = 14,\n  tree_depth = 10,\n  cost_complexity = 0.0001\n)\n\n\n\nSolution 4.14 (Exercise 4.14). \n\nautoplot(rt_res) + theme_minimal()\n\n\n\n\n\n\n\n\n\nTree Depth:\n\nIncreasing tree depth (lines for depths 1, 5, 10, 15) generally reduces the RMSE across all plots, indicating better model performance as depth grows.\nHowever, for deeper trees, RMSE increases when the Cost-Complexity Parameter is high, due to over-regularization.\n\nMinimal Node Size:\n\nSmaller minimal node sizes (e.g., 2) allow the tree to split more finely, resulting in lower RMSE when the Cost-Complexity Parameter is low.\nLarger minimal node sizes (e.g., 40) limit splitting, which leads to higher RMSE across all complexity values.\n\nCost-Complexity Parameter:\n\nA small Cost-Complexity Parameter (e.g., (10^{-8})) corresponds to minimal pruning, leading to low RMSE (better performance).\nAs the Cost-Complexity Parameter increases, the tree gets pruned more aggressively, and RMSE rises, especially for shallow trees or larger node sizes.\n\n\n\n\nSolution 4.15 (Exercise 4.15). \n\nrt_res_best&lt;- rt_res %&gt;%\n  select_best(metric = \"mae\")\n\nlast_rt_fit &lt;- wf_rent %&gt;%\n  finalize_workflow(rt_res_best) %&gt;%\n  last_fit(split_rent)\n\nlast_rt_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard     341.    pre0_mod0_post0\n2 rsq     standard       0.660 pre0_mod0_post0\n\n\nUsing the parameters from the pre-specified tibble to avoid tuning time:\n\nlast_rt_fit &lt;- wf_rent %&gt;%\n  finalize_workflow(rt_res_tune_best) %&gt;%\n  last_fit(split_rent)\n\nlast_rt_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard     336.    pre0_mod0_post0\n2 rsq     standard       0.670 pre0_mod0_post0\n\n\n\n\nSolution 4.16 (Exercise 4.16). \n\ntitle_text &lt;- \"Predictions of the test set plotted against the actual values\"\nlast_rt_fit %&gt;%\n  collect_predictions() %&gt;%\n  ggplot(aes(baseRent, .pred)) +\n  geom_point(alpha = 0.4, color = \"cadetblue2\")+\n  geom_abline(slope = 1, lty = 2, color = \"indianred2\", alpha = 1) +\n  labs(\n    x = \"True base rent\",\n    y = \"Estimated base rent\",\n    title = title_text\n  )+\n  theme_minimal(base_size=14)+\n  coord_fixed()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regression Trees</span>"
    ]
  },
  {
    "objectID": "05_bagging_rf.html",
    "href": "05_bagging_rf.html",
    "title": "5  Random Forests",
    "section": "",
    "text": "5.1 Introduction\nIn this exercise session we will briefly talk about some theoretical considerations when applying bootstrap sampling, bagging and training a random forest before performing a more in-depth case study of binary classification. The case-study assumes that you are familiar with penalized logistic regressions (similar to lasso regression, see Session 03 ) and classification trees (similar to regression trees introduced in Session 4). We will however, revisit different evaluation metrics for a binary classifier before training any of the models which should help you to develop a feeling for model performance.\nNote, that some of the models we train might take a few (up to many) minutes depending on your hardware. One way to circumvent long training processes is to use a simple training/validation/test split instead of CV.\nIn this session, we primarily deal with binary classification. The target variable \\(Y\\) therefore takes on only two values. Usually, these values are encoded as \\(0\\) and \\(1\\), so that \\(Y\\) can be represented as a vector \\(Y\\in\\{0,1\\}^K\\). We are now interested in estimating the probability \\[\\begin{equation}\n  \\mathbb{P}(Y=1|X=x)\n\\end{equation}\\] for a data point \\(x\\in\\mathbb{R}^{J}\\). If the estimated probability \\(\\hat{p}\\) is greater than a certain, predefined threshold \\(q\\in(0,1)\\), then we assign the sample to class \\(1\\). Intuitively, the value \\(q=0.5\\) makes sense here, since with a probability \\(p&gt;0.5\\) we are more certain that the data point belongs to class \\(1\\) than to class \\(0\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Forests</span>"
    ]
  },
  {
    "objectID": "05_bagging_rf.html#evaluation-of-a-binary-classification-model",
    "href": "05_bagging_rf.html#evaluation-of-a-binary-classification-model",
    "title": "5  Random Forests",
    "section": "5.2 Evaluation of a Binary Classification Model",
    "text": "5.2 Evaluation of a Binary Classification Model\nSince an evaluation using metrics like \\(\\text{RMSE}\\) and \\(R^2\\) makes little sense in binary classification, we want to look at various alternative metrics in this section.\n\n5.2.1 Confusion Matrix\nThe so-called confusion matrix serves as the basis for the metrics we want to consider in the following. This cross-tabulation indicates how many data points were classified correctly or incorrectly. Here, a distinction is made in particular between the classes Negative and Positive which are encoded with the numeric values \\(0\\) and \\(1\\). Mostly, in the context of the confusion matrix, \\(1\\) stands for Positive and \\(0\\) for Negative.\n\n\n\nConfusion Matrix, Source: Wikipedia\n\n\nWe can best illustrate the fields of the confusion matrix using an example.\n\nExample 5.1 A research team at the university hospital has developed a test for detecting a rare heart disease. If doctors correctly detect this heart disease, it can be cured by a surgical intervention. The probability of a successful intervention is greater than the probability that a sick person survives the course of the disease. A positive test result is encoded as \\(1\\) or Predicted Positive and a negative result as \\(0\\) or Predicted Negative. Now there are the following possibilities:\n\nThe test is positive and the examined person has the disease. This is called a True Positive.\nThe test is positive and the examined person does not have the disease. This is called a False Positive.\nThe test is negative and the examined person does not have the disease. This is called a True Negative.\nThe test is negative and the examined person has the disease. This is called a False Negative.\n\n\nBesides correct classification (TP and TN), we should particularly deal with the two incorrect classification scenarios. In Example 5.1, a person falsely classified as negative (FN) is therefore a patient who has the heart disease but is not diagnosed. This type of error is particularly serious in the example, since the person has a higher survival probability through the surgical intervention. If a person is falsely classified as positive (FP), this means that the test turns out positive even though the person is not suffering from the heart disease. Under ordinary circumstances, such a misclassification would not be quite as serious, since the person is, after all, not suffering from the rare disease. However, if a surgical intervention is performed based on the test result anyway, then the patient is exposed to an unnecessary risk.\nHow well such a test actually works should therefore not only be determined by the absolute or relative number of correctly classified data points, but also by how high the corresponding error rates are.\nThis statement can be expressed more precisely with the following key figures:\n\nAccuracy: \\[\\begin{equation*}\n  \\text{Accuracy} = \\frac{TP+TN}{TP+TN+FP+FN}\n\\end{equation*}\\] Accuracy measures the relative proportion of correctly classified data points to the total number of data points. It describes the probability of a correct test result.\nSensitivity: \\[\\begin{equation*}\n  \\text{Sensitivity} = \\frac{TP}{TP+FN}\n\\end{equation*}\\] Sensitivity measures the relative proportion of data points correctly classified as positive to the total number of positive data points. It describes the conditional probability that a positive result is predicted, assuming that the result is actually positive.\nSpecificity: \\[\\begin{equation*}\n  \\text{Specificity} = \\frac{TN}{TN+FP}\n\\end{equation*}\\] Specificity is the counterpart to sensitivity. It measures the relative proportion of data points correctly classified as negative to the total number of negative data points. Specificity can also be interpreted as the conditional probability that a negative result is predicted, assuming that the result is actually negative.\nPrecision: \\[\\begin{equation*}\n  \\text{Precision} = \\frac{TP}{TP+FP}\n\\end{equation*}\\] Precision measures the relative proportion of data points correctly classified as positive to the total number of data points classified as positive. Particularly with unbalanced datasets1, precision is an important metric, as accuracy can lead to fallacies with unbalanced datasets.\n\n\n5.2.1.1 ROC Curve and Precision-Recall Curve\nWhen passing a sample into the classification model, the return-value is usually a probability \\(p\\in[0,1]\\) that denotes the probability of the sample belonging to the Positives (in this hypothetical setting we assume that there are two classes “Positives” and “Negatives”). Intuitively it makes sense to say, that a given sample \\(x\\) belongs to the Positives if \\(p\\geq q = 0.5\\). However, this threshold \\(q = 0.5\\) can be adjusted. Depending on this threshold \\(q\\) , the values in our confusion matrix change.\nExample:\nSet \\(q = 0\\), then \\(p\\) is always larger or equal to \\(q\\), which means that we assign every value to the positives. Then, our True Positive Rate (\\(\\mathrm{TPR} = \\frac{\\mathrm{TP}}{\\mathrm{P}}\\)) will be equal to \\(1\\) since all samples are assigned to the Positives. However, the True Negative Rate (\\(\\mathrm{TNR} = \\frac{\\mathrm{TN}}{\\mathrm{N}}\\)) will be equal to 0, since not one sample has been assigned to the Negatives, meaning that \\(\\mathrm{TN} = 0\\) .\nA way to visualize the change in our confusion matrix depending on the threshold \\(q\\) is given by the so-called ROC (Receiver-Operator Curve) curve and Precision-Recall Curve.\nROC Curve:\nThe ROC curve shows the \\(\\mathrm{TPR}\\) (also known as recall or sensitivity) plotted against the \\(\\mathrm{TNR}\\) (also known as 1-specificity). By plotting these two values against each other, we can identify a good model by checking whether the curve generated by all the thresholds is approaching the left top corner of a plot, indicating that both \\(\\mathrm{TPR}\\) and \\(\\mathrm{TNR}\\) are equal to 1, i.e. the model perfectly classifies all True Positives and all True Negatives. An exemplary plot can be found below.\n\n\n\nROC Curve, Source: developers.google.com\n\n\nPR Curve:\nThe Precision-Recall Curve on the other hand shows the Precision (\\(\\mathrm{Precision} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}\\) ) plotted against the Recall (\\(\\mathrm{Recall} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}\\)). If the line generated by a model is close to the top right corner, the model is relevant, has a high precision, and sensitivity.\nAn exemplary plot can be found below.\n\n\n\nPR Curve\n\n\nWhat does a high precision and recall actually mean (I find the definition above kind of hard to grasp)? Let us consider a simple example:\nExample:\nImagine you are developing a classification model for detecting a rare disease (Positves corresponds to detecting the decease). There are 10000 people in the observed data and only 10 of them are infected with said disease. One way to set up a model (regardless of the features) can be to simply label everyone as not having the disease which would effectively result in an accuracy of 99.9%, Hurray . However, this model is obviously not the best since we failed to identify any of the sick people correctly. So, after tweaking the model we now have a model that might be less accurate but identifies sick patients better. What does better in this context mean? By looking at the definition of Precision and Recall above, you may notice that they only differ by the second summand in the denominator of the fraction, namely False Positives and False Negatives. Before we continue, think for yourself, which of those are worse in the scenario of detecting a disease? The right answer would be False Negatives, since we fail to identify a sick patient! That is why a sensitive model is crucial here as well\nIn summary: especially for imbalanced data sets, we do not only want to achieve a high accuracy since that is fairly easy to achieve. We Also want a high precision or recall, focusing on one or the other depending on the model objective.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Forests</span>"
    ]
  },
  {
    "objectID": "05_bagging_rf.html#classification-with-trees",
    "href": "05_bagging_rf.html#classification-with-trees",
    "title": "5  Random Forests",
    "section": "5.3 Classification with Trees",
    "text": "5.3 Classification with Trees\nIn the previous section, we have already extensively discussed regression trees. The transition from regression to classification trees is ultimately not complex either! Instead of a real-valued target variable, we now consider estimated classes or class probabilities in the leaf nodes. The algorithm for estimating a classification tree remains almost unchanged compared to regression trees. Only the improvement value, which is calculated using the \\(\\text{MSE}\\) for regression trees, must be replaced by a suitable metric in the classification context. This metric is given by the Gini impurity. For a node \\(K_n\\), this is defined as \\[\\begin{equation*}\n  1-(p_1^2+p_0^2)\n\\end{equation*}\\]\nwhere \\(p_1\\) is the relative frequency of class \\(1\\) in the node and \\(p_0\\) is the relative frequency of class \\(0\\) in the node. It is then tested for which \\(p_i,\\: i=0,1\\) the removal leads to a larger impurity index. I.e., if \\(p_1\\geq p_0\\), the node will return class \\(1\\) as the classification value. The decision regarding the splitting variables works the same way: Test for which variable the greatest reduction in Gini impurity results, and use this for a further split.\nThe importance of the variables can then also be measured with respect to this impurity value, so that we can effectively use the same permutation feature importance method.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Forests</span>"
    ]
  },
  {
    "objectID": "05_bagging_rf.html#random-forests",
    "href": "05_bagging_rf.html#random-forests",
    "title": "5  Random Forests",
    "section": "5.4 Random forests",
    "text": "5.4 Random forests\nThroughout this exercise we will use the following libraries:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n#variable importance plots\n\nlibrary(\"vip\")\n\n#Stitching together plots and adding markdown text\n\nlibrary(\"patchwork\")\nlibrary(\"ggtext\")\n\nFor this sessions example, we will also use the white wine data set, but extend it by adding the red wine data set. The red wine data set can be downloaded directly from the UC Irvine Machine Learning Repository or by clicking the button below.\n\nDownload Red Wine Data\n\nA detailed description of each parameter see Cortez et al.\nNote, that importing the red wine data set with the read.csv function requires the argument sep = \";\" which indicates that\nthe columns in the csv are separated by a semicolon.\nAfter importing the data, we add a new column names wine_color that indicates the color of the wine. The wine color will be the target variable, meaning that we try to determine the color of a wine given all the other attributes.\nTo combine both data sets, we can use the rbind() function which binds together the rows of a data set. Before binding together the rows of the data set, we need to make sure that the names of the columns coincide. Otherwise, the columns can’t be matched.\n\ndata_wine_red&lt;-read.csv(\"data/winequality-red.csv\", sep = \";\")\ndata_wine_white&lt;- read.csv(\"data/winequality-white.csv\")\n\ndata_wine_red&lt;- data_wine_red %&gt;% mutate(wine_color = \"red\")\ndata_wine_white&lt;- data_wine_white %&gt;% mutate(wine_color = \"white\")\n\ndata_wine &lt;- rbind(data_wine_red,data_wine_white)\n\nThe newly created data set contains approximately \\(5000\\) wine samples with around \\(25\\%\\) being red wine and the remaining \\(75\\%\\) being white wine.\n\ndata_wine %&gt;% group_by(wine_color) %&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(ratio = n/sum(n))\n\n# A tibble: 2 × 3\n  wine_color     n ratio\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;\n1 red         1599 0.246\n2 white       4898 0.754\n\n\nSince our data set is imbalanced we should apply stratification in our data split. Stratification ensures that the same ration of red and white wine samples is in the training and testing data. After splitting our data, we can create a 5-fold CV object on the training data.\n\nset.seed(123)\nsplit_wine &lt;- initial_split(data_wine, strata = wine_color)\n\ndata_wine_train &lt;- training(split_wine)\ndata_wine_test &lt;- testing(split_wine)\n\nfolds_wine &lt;- vfold_cv(data_wine_train,5)\n\nThen, we can set up a receipe containing a simple formula and step to convert the target feature wine_color to type factor.\n\nrec_wine &lt;- recipe(\n  wine_color ~.,\n  data = data_wine_train\n  ) %&gt;%\n  step_string2factor(wine_color)\n\nA random forest model can be specified using the rand_forest() function. Additional arguments include, but are not limited to:\n\nmode: indicates whether a classifier or a regression model is specified. (required)\ntrees: indicates the number of trees fitted in the forest. (default = 500)\nmin_n: indicates the minimum number of data points in a node that is required for the node to be split further. (default = 20)\nmtry: indicates the number of variables to possibly split at in each node. (default = sqrt(ncol(data)-1)))\n\nNote, that the mtry parameter depends on the number of independent variables. If mtry = ncol(data)-1, meaning that we select every single independent variable for a potential split, we are creating a bag, rather than a random forest.\nBy setting every hyper parameter to tune(), we specify that the respective hyper parameters are to be tuned.\n\nrf_mod_tune_spec &lt;- rand_forest(\n  mode = \"classification\",\n  trees = tune(),\n  min_n = tune(),\n  mtry = tune()\n) %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\")\n\nAs with any other model, we can create a workflow, add the recipe and model specification, and create a metrics set. The metric set below contains the following metrics:\n\nroc_auc: measures the area under the receiver operator characteristic (values \\(\\in [0,1]\\), with \\(1\\) being the best possible value).\npr_auc: measures the area under the precision-recall curve (values \\(\\in [0,1]\\), with \\(1\\) being the best possible value).\nprecision: measures the positive predictive value (values \\(\\in [0,1]\\), with \\(1\\) being the best possible value).\nrecall: measures the true positive rate (values \\(\\in [0,1]\\), with \\(1\\) being the best possible value).\n\n\nwf_wine &lt;- workflow() %&gt;%\n  add_recipe(rec_wine) %&gt;%\n  add_model(rf_mod_tune_spec)\n\nmulti_metrics &lt;- metric_set(roc_auc,pr_auc,precision,recall)\n\nThe random forest model can be tuned on the 5-fold CV object in the same fashion as every other model. By specifying grid=10, we circumvent specifying the range for the mtry() parameter.\n\n\n\n\n\n\nWarning\n\n\n\nTuning a random forest can take a while. Instead of using 5-fold CV, a simple training/validation/test split can decrease training time.\n\n\n\nrf_tune_res &lt;- wf_wine %&gt;%\n  tune_grid(\n    resamples = folds_wine,\n    metrics = multi_metrics,\n    grid = 10\n  )\n\nAfter tuning the model, we can select the best set of hyper parameters with respect to different metrics. If we aim for a model that emphasizes correctly classifying the minority class, the metric pr_auc metric can be more useful (why?). We, therefore, select the best parameters according to the metric pr_auc and train a final model using these parameters.\n\nbest_parm_rf_wine &lt;- rf_tune_res %&gt;%\n  select_best(metric = \"pr_auc\")\n\nlast_rf_fit &lt;- wf_wine %&gt;%\n  finalize_workflow(best_parm_rf_wine) %&gt;%\n  last_fit(split_wine,\n           metrics= multi_metrics)\n\nTo evaluate our model, we can either collect the specified metrics using the collect_metrics() function, or generate PR- and ROC-curves.\nThe latter can be achieved with the following Code snippet. We first collect the predictions of the test data using the collect_predictions() function. Then, we generate a ROC- and PR-Curve using the functions roc_curve() and pr_curve(). The roc_curve() function returns a data frame containing three columns:\n\n.threshold: containing the threshold probability for which a sample is assigned to the positive class (in that case red).\nspecificity: containing the specificity of the model for the given thresholds.\nsensitivity: containing the sensitivity of the model for the given thresholds.\n\nThe pr_curve() function returns a similar data frame containing the recall and precision instead of specificity and sensitivity.\n\nrf_auc&lt;- last_rf_fit %&gt;%\n  collect_predictions() %&gt;%\n  roc_curve(wine_color,.pred_red) %&gt;% \n  mutate(model = \"Random Forest\")\n\nrf_pr&lt;- last_rf_fit %&gt;%\n  collect_predictions() %&gt;%\n  pr_curve(wine_color,.pred_red) %&gt;% \n  mutate(model = \"Random Forest\")\n\nWe can generate the curve plots using the autoplot() function or ggplot. An example for both can be found below.\n\nrf_auc %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\nrf_auc %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + \n  geom_path(lwd = 1.5) +\n  geom_abline(lty = 3) + \n  coord_equal() + \n  scale_color_manual(values = \"darkgreen\")+\n  labs(\n    title = \"ROC Curve for a random forest \\n that predicts the color of wine\"\n  )+\n  theme_minimal(base_size = 11)+\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nTo generate a feature importance plot with respect to the measure \"permutation\" feature importance, we first have to extract the fit engine from final model fit and then apply the vip() function of the vip library. The vip function creates a (ggplot) plot, showing the importance scores for the predictors in a model. The geom argument specifies what kind of plot is generated. Setting geom = \"col\" therefore creates a column plot. Other options include \"boxplot\", \"point\", and \"violin\".\n\nlibrary(vip)\n\nlast_rf_fit %&gt;%\n  extract_fit_engine()%&gt;%\n  vip(geom = \"col\",\n           aesthetics = list(fill = \"midnightblue\",\n                             alpha = 0.8)\n           )+\n  theme_minimal(base_size = 11)\n\n\n\n\n\n\n\n\nConsidering the figure above, the variable total.sulfur.dioxide has the highest score which indicates that this variable helps determining the color of the wines most.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Forests</span>"
    ]
  },
  {
    "objectID": "05_bagging_rf.html#exercises",
    "href": "05_bagging_rf.html#exercises",
    "title": "5  Random Forests",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises\n\n5.5.1 Theoretical Exercises\nIn this exercise we will discuss some aspects of bootstrap sampling, bagging and random forest.\n\nExercise 5.1 Assume, we have a data set with \\(n\\) sample and a bootstrap sample of size \\(n\\). Furthermore, assume that the probability of an observation not being in the bootstrap sample is \\(\\left(1-\\frac{1}{n}\\right)^n\\). Show that the probability for any sample \\(j\\) to be in the data set is approximately \\(0.6321206\\).\n\n\nExercise 5.2 In terms of bagging, explain the following sentence from the lecture:\n\nHaving similar trees leads to correlated estimates.\n\n\n\nExercise 5.3 Random forests can solve the problem mentioned in Exercise 5.2 of having trees that are too similar. Describe how this is achieved!\n\n\n\n5.5.2 Programming Exercises\nIn this exercise we want to utilize our newly gained knowledge about Bagging and compare a random forest model to a single classification tree and penalized logistic regression.\nThe dataset we will consider in this exercise will be the Credit Card Customers data set that can either be downloaded using the provided link or the button below.\n\nDownload BankChurners\n\nRecall that the data set consists of 10,127 entries that represent individual customers of a bank including but not limited to their age, salary, credit card limit, and credit card category.\nThe main idea for such classification tasks is the following:\n\nStart out by building a simple base model, which allows for an easy interpretation of parameters. A penalized logistic regression will be this base model in our case.\nMove to a slightly more complex model where the interpretation of model parameters is less straight forward, but the model performance increases. The model we will consider for this scenario is a decision tree.\nAs a last step, a highly complex model is trained where the focus is no longer on explainability rather than getting the best possible out of sample performance. An example of such a model is a random forest, which will also be our model of choice for this step.\n\nConsider the following glimpse into the dataset:\n\n\nRows: 10,127\nColumns: 21\n$ CLIENTNUM                &lt;int&gt; 768805383, 818770008, 713982108, 769911858, 7…\n$ Attrition_Flag           &lt;chr&gt; \"Existing Customer\", \"Existing Customer\", \"Ex…\n$ Customer_Age             &lt;int&gt; 45, 49, 51, 40, 40, 44, 51, 32, 37, 48, 42, 6…\n$ Gender                   &lt;chr&gt; \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\", \"M\", \"M\", …\n$ Dependent_count          &lt;int&gt; 3, 5, 3, 4, 3, 2, 4, 0, 3, 2, 5, 1, 1, 3, 2, …\n$ Education_Level          &lt;chr&gt; \"High School\", \"Graduate\", \"Graduate\", \"High …\n$ Marital_Status           &lt;chr&gt; \"Married\", \"Single\", \"Married\", \"Unknown\", \"M…\n$ Income_Category          &lt;chr&gt; \"$60K - $80K\", \"Less than $40K\", \"$80K - $120…\n$ Card_Category            &lt;chr&gt; \"Blue\", \"Blue\", \"Blue\", \"Blue\", \"Blue\", \"Blue…\n$ Months_on_book           &lt;int&gt; 39, 44, 36, 34, 21, 36, 46, 27, 36, 36, 31, 5…\n$ Total_Relationship_Count &lt;int&gt; 5, 6, 4, 3, 5, 3, 6, 2, 5, 6, 5, 6, 3, 5, 5, …\n$ Months_Inactive_12_mon   &lt;int&gt; 1, 1, 1, 4, 1, 1, 1, 2, 2, 3, 3, 2, 6, 1, 2, …\n$ Contacts_Count_12_mon    &lt;int&gt; 3, 2, 0, 1, 0, 2, 3, 2, 0, 3, 2, 3, 0, 3, 2, …\n$ Credit_Limit             &lt;dbl&gt; 12691.0, 8256.0, 3418.0, 3313.0, 4716.0, 4010…\n$ Total_Revolving_Bal      &lt;int&gt; 777, 864, 0, 2517, 0, 1247, 2264, 1396, 2517,…\n$ Avg_Open_To_Buy          &lt;dbl&gt; 11914.0, 7392.0, 3418.0, 796.0, 4716.0, 2763.…\n$ Total_Amt_Chng_Q4_Q1     &lt;dbl&gt; 1.335, 1.541, 2.594, 1.405, 2.175, 1.376, 1.9…\n$ Total_Trans_Amt          &lt;int&gt; 1144, 1291, 1887, 1171, 816, 1088, 1330, 1538…\n$ Total_Trans_Ct           &lt;int&gt; 42, 33, 20, 20, 28, 24, 31, 36, 24, 32, 42, 2…\n$ Total_Ct_Chng_Q4_Q1      &lt;dbl&gt; 1.625, 3.714, 2.333, 2.333, 2.500, 0.846, 0.7…\n$ Avg_Utilization_Ratio    &lt;dbl&gt; 0.061, 0.105, 0.000, 0.760, 0.000, 0.311, 0.0…\n\n\nSince some of the features are kind of ambiguous, let us briefly talk about what they mean.\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nCLIENTNUM\nClient number. Unique identifier for the customer holding the account\n\n\nAttrition_Flag\nInternal event (customer activity) variable - if the account is closed then 1 else 0\n\n\nMonths_on_book\nPeriod of relationship with bank\n\n\nMonths_Inactive_12_mon\nNo. of months inactive in the last 12 months\n\n\nCredit_Limit\nCredit Limit on the Credit Card\n\n\nTotal_Revolving_Bal\nPortion of credit card spending that goes unpaid at the end of a billing cycle\n\n\nAvg_Open_To_Buy\nOpen to Buy Credit Line (Average of last 12 months)\n\n\nTotal_Amt_Chng_Q4_Q1\nChange in Transaction Amount (Q4 over Q1)\n\n\nTotal_Trans_Amt\nTotal Transaction Amount (Last 12 months)\n\n\nTotal_Trans_Ct\nTotal Transaction Count (Last 12 months)\n\n\nTotal_Ct_Chng_Q4_Q1\nChange in Transaction Count (Q4 over Q1)\n\n\nAvg_Utilization_Ratio\nAverage Card Utilization Ratio (Divide the total balance by the total credit limit)\n\n\n\n\nExercise 5.4 In the first exercise session, we already performed some exploratory data analysis, focusing on the demographics of the customers. Since we are mainly interested in predicting the attrition flag, find out the no-information rate (NIR) defined by\n\\[\\begin{equation*}\n  \\max\\left(\\frac{\\mathrm{P}}{\\mathrm{N+P}},\\frac{\\mathrm{N}}{\\mathrm{N+P}}\\right)\n\\end{equation*}\\]\n\n\nExercise 5.5 Before splitting the data, some preprocessing steps should be applied to the whole dataset:\n\nUse the following code snippet to convert all the \"Unknown\" and \"unknown\" values into NA values:\n\nacross(\n  where(~is.character(.)|is.factor(.)),\n  ~if_else(.%in% c(\"Unknown\",\"unknown\"),NA,.)\n)\n\nConvert the features Income_Category and Education_Level to ordered factors by using the levels provided below:\n\nlevels_income &lt;- c(\"Less than $40K\",\"$40K - $60K\",\n             \"$60K - $80K\",\"$80K - $120K\",\"$120K +\")\n\nlevels_education &lt;- c(\"Uneducated\", \"High School\",\"College\",\n                      \"Graduate\",  \"Post-Graduate\", \"Doctorate\")\n\nThe target variable Attrition_Flag currently has the values \"Existing Customer\" and \"Attrited Customer\". Change the variable into an unordered factor variable which has the value 0, if a customer is of the class \"Existing Customer\" and 1 if the customer is of the class \"Attrited Customer\".\n\n\n\nExercise 5.6 Create a training and test split based on the previously modified dataset using set.seed(121) and a 5-fold CV object based on the training data. Use stratification for the target variable Attrition_Flag to ensure that the ratio of positive and negative sample remains the same in the training and testing data.\n\n\nExercise 5.7 Create a recipe by following the steps described below.\n\nAs a formula, fit the variable Attrition_Flag on every other feature and set the data parameter to data_train.\nUpdate the role of the variable CLIENTNUM by setting it to \"ID\".\nCreate ordnial scores for all ordered predictors.\nUse the step_unknown()-function on all factor predictors to handle unknown values.\nCreate dummy variable for the features Maritial_Status, Gender and Card_Category.\nUse mean imputation for all the numeric predictors.\nCreate a zero-variance and correlation filter for the data.\n\n\n\nExercise 5.8 Create a workflow object and add the newly created recipe rec_ci. Afterwards, create a metric_set that contains the metrics roc_auc,pr_auc,accuracy,precision, and recall.\n\n\nExercise 5.9 (Tuning a lasso model)  \n\nUtilize the logistic_reg function to create a lasso model.\nCreate a regular grid for the logistic model penalty with \\(30\\) levels.\nTune the linear model using the 5-fold CV object created in Exercise 5.6, the grid specified in 2., and the metric set specified in Exercise 5.8.\n\n\n\nExercise 5.10  \n\nGiven the results of the previous exercise, select the best model according to the “one-standard” rule based on the \"pr_auc\" metric.\nTrain a final model on the whole training data.\nCreate two data frames containing the points of the models’ PR- and ROC-curve and visualize them.\n\n\n\nExercise 5.11 (Bonus Exercise) The following exercise is not mandatory but still helps for gaining a deeper understanding of the penalization behavior. Since we have used a lasso logistic regression, some of the parameters might have been driven to 0. Find if there were any!\n\n\nExercise 5.12 Repeat Exercise 5.9 - Exercise 5.10 by tuning a classification tree. Tune the parameters min_n, tree_depth, and cost_complexity using 5-fold CV and a regular grid with four levels. Instead of using the one standard error rule, use the select_best function instead.\n\n\nExercise 5.13 Use the vip::vip function to find the most important features of the final classification tree.\n\n\nExercise 5.14 Repeat Exercise 5.12 and Exercise 5.13 for a random forest model with \\(1000\\) trees. Tune the parameters mtry and min_n with a grid of size \\(10\\) using 5-fold CV.\n\n\nExercise 5.15 Given the following plots, answer the following questions:\n\nWhat can be said about the discriminatory power of the classes?\nWhich curve should be considered for assessing the accurracy of the models?\nWhich model performs the best?\n\n\n\n\n\n\n\n\nExercise 5.16 (Bonus exercise) Use the previously saved data frames containing the ROC- and PR-curve data for each model to recreate the plot in exercise Exercise 5.15",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Forests</span>"
    ]
  },
  {
    "objectID": "05_bagging_rf.html#solutions",
    "href": "05_bagging_rf.html#solutions",
    "title": "5  Random Forests",
    "section": "5.6 Solutions",
    "text": "5.6 Solutions\n\nSolution 5.1 (Exercise 5.1). For \\(n\\) sufficiently big, the estimate \\(\\left(1-\\frac{1}{n}\\right)^n \\approx \\exp(-1)\\) holds. Since the probability of any sample not being in the data set is therefore approximately \\(\\exp(-1)\\), we can simply calculate the complementary probability. The complementary probability is given by \\(1-\\exp(-1)\\approx 0.63212\\).\n\n\nSolution 5.2 (Exercise 5.2). Bagging trees leads to fitting many trees with similar structure as the same features tend to be selected in the same step in different trees. Given that the estimates are depending on the splits of a tree, the estimates can be highly correlated if the tree structures are similar.\n\n\nSolution 5.3 (Exercise 5.3). When bagging trees, the number of features for building a tree stays the same. A random forest on the other hand only selects a subset of all the features. This ensures that there is enough variability in the different trees and thus directly tackles the problem of the trees being too similar.\n\n\nSolution 5.4 (Exercise 5.4). \n\nNIR&lt;- credit_info %&gt;%\n  group_by(Attrition_Flag)%&gt;%\n  summarise(n=n()) %&gt;%\n  mutate(NIR = n/sum(n)) %&gt;%\n  pluck(3) %&gt;%\n  max()\n\nglue::glue(\n  \"The NIR of the underlying dataset is {round(NIR,3)},\n   meaning that a classification model should have\n   an accuracy of at least {round(NIR,3)}.\"\n  )\n\nThe NIR of the underlying dataset is 0.839,\nmeaning that a classification model should have\nan accuracy of at least 0.839.\n\n\n\n\nSolution 5.5 (Exercise 5.5). \n\ncredit_info &lt;- credit_info %&gt;%\n  mutate(\n    across(\n      where(~is.character(.)|is.factor(.)),\n      ~if_else(.%in% c(\"Unknown\",\"unknown\"),NA,.)\n    ),\n    Income_Category = factor(\n        Income_Category,\n        levels = levels_income,\n        ordered = TRUE\n        ),\n    Education_Level = factor(\n        Education_Level,\n        levels = levels_education,\n        ordered = TRUE\n    ),\n    Attrition_Flag = factor(\n      if_else(\n        Attrition_Flag==\"Existing Customer\",\n        0, 1)\n    ),\n    Attrition_Flag = fct_rev(Attrition_Flag)\n  )\n\n\n\nSolution 5.6 (Exercise 5.6). Create a training and test split using set.seed(121) and a 5-fold CV object based on the training data.\n\nset.seed(121)\nsplit &lt;- initial_split(credit_info, strata = Attrition_Flag)\ndata_train_ci &lt;- training(split)\ndata_test_ci &lt;- testing(split)\nfolds_ci &lt;- vfold_cv(data_train_ci, v = 5)\n\n\n\nSolution 5.7 (Exercise 5.7). \n\nrec_ci &lt;- recipe(\n  formula = Attrition_Flag ~.,\n  data = data_train_ci\n) %&gt;%\n  update_role(CLIENTNUM, new_role = \"Unique Identifier\")%&gt;%\n  step_ordinalscore(all_ordered_predictors()) %&gt;%\n  step_unknown(all_factor_predictors()) %&gt;%\n  step_dummy(Marital_Status,Gender,Card_Category)%&gt;%\n  step_impute_mean(all_numeric_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_corr(all_predictors())\n\n\n\nSolution 5.8 (Exercise 5.8). \n\nci_wf &lt;- workflow() %&gt;%\n  add_recipe(rec_ci) \n\nmulti_metrics &lt;- metric_set(roc_auc,pr_auc,accuracy,recall)\n\n\n\nSolution 5.9 (Exercise 5.9). \n\nlog_mod_tune_spec &lt;-logistic_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\nci_wf &lt;- ci_wf %&gt;% add_model(log_mod_tune_spec)\n\nlr_grid &lt;- ci_wf %&gt;%\n  extract_parameter_set_dials %&gt;%\n  grid_regular(levels = 30)\n\nlr_tune_res &lt;- ci_wf %&gt;%\n  tune_grid(\n    grid = lr_grid,\n    metrics = multi_metrics,\n    resamples = folds_ci\n  )\n\n\n\nSolution 5.10 (Exercise 5.10). \n\nlr_res_best &lt;- lr_tune_res %&gt;%\n  select_by_one_std_err(metric = \"pr_auc\", desc(penalty))\n\nlast_lr_fit &lt;- ci_wf %&gt;%\n  finalize_workflow(lr_res_best) %&gt;%\n  last_fit(split,\n           metrics = multi_metrics)\n\nlr_auc&lt;- last_lr_fit %&gt;%\n  collect_predictions() %&gt;%\n  roc_curve(Attrition_Flag,.pred_1) %&gt;% \n  mutate(model = \"Logistic Regression\")\n\nlr_pr&lt;- last_lr_fit %&gt;%\n  collect_predictions() %&gt;%\n  pr_curve(Attrition_Flag,.pred_1) %&gt;%\n  mutate(model = \"Logistic Regression\")\n\n\np1 &lt;- lr_auc %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + \n  geom_path(lwd = 1.5, alpha = 0.8) +\n  geom_abline(lty = 3) + \n  coord_equal() + \n  scale_color_viridis_d(option = \"plasma\", end = .6)+\n  ylim(c(0,1))+\n  theme_minimal(base_size = 11)+\n  theme(legend.position = \"none\")\n\np2 &lt;- lr_pr %&gt;%\n  ggplot(aes(x = recall, y = precision, col = model)) + \n  geom_path(lwd = 1.5, alpha = 0.8) +\n  coord_equal() + \n  scale_color_viridis_d(option = \"plasma\", end = .6)+\n  ylim(c(0,1))+\n  theme_minimal(base_size = 11)+\n  theme(legend.position = \"none\")\n\np&lt;-p1 | p2\n\np + plot_annotation(\n  title = \"ROC curve and Precision-Recall curve for a penalized logistic regression\"\n)\n\n\n\n\n\n\n\n\n\n\nExercise 5.17 (Exercise 5.17)  \n\nlast_lr_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  filter(estimate == 0) %&gt;%\n  select(term)\n\n# A tibble: 12 × 1\n   term                  \n   &lt;chr&gt;                 \n 1 Customer_Age          \n 2 Education_Level       \n 3 Income_Category       \n 4 Months_on_book        \n 5 Credit_Limit          \n 6 Total_Amt_Chng_Q4_Q1  \n 7 Avg_Utilization_Ratio \n 8 Marital_Status_Single \n 9 Marital_Status_unknown\n10 Card_Category_Gold    \n11 Card_Category_Platinum\n12 Card_Category_Silver  \n\nlast_lr_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  filter(estimate &gt; 0) %&gt;%\n  arrange(desc(estimate)) %&gt;%\n  select(term)\n\n# A tibble: 6 × 1\n  term                    \n  &lt;chr&gt;                   \n1 Total_Ct_Chng_Q4_Q1     \n2 Total_Relationship_Count\n3 Gender_M                \n4 Marital_Status_Married  \n5 Total_Trans_Ct          \n6 Total_Revolving_Bal     \n\n\n\n\nSolution 5.11 (Exercise 5.12). \n\nct_model_spec &lt;- decision_tree(\n  min_n = tune(),\n  tree_depth = tune(),\n  cost_complexity = tune()\n) %&gt;%\n  set_mode(\"classification\")\n\nci_wf &lt;- ci_wf %&gt;% update_model(ct_model_spec)\nct_grid &lt;- ci_wf %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  grid_regular(levels = 4)\n\nct_tune_res &lt;- ci_wf %&gt;%\n tune_grid(\n   grid = ct_grid,\n   metrics = multi_metrics,\n   resamples = folds_ci\n)\n\nct_res_best &lt;- ct_tune_res %&gt;% \n  select_best(metric = \"pr_auc\")\n\nlast_ct_fit &lt;- ci_wf %&gt;%\n  finalize_workflow(ct_res_best) %&gt;%\n  last_fit(split,\n           metrics = multi_metrics)\n\n\nct_auc&lt;- last_ct_fit %&gt;%\n  collect_predictions() %&gt;%\n  roc_curve(Attrition_Flag,.pred_1) %&gt;%\n  mutate(model = \"Classification Tree\")\n\nct_pr&lt;- last_ct_fit %&gt;%\n  collect_predictions() %&gt;%\n  pr_curve(Attrition_Flag,.pred_1) %&gt;%\n  mutate(model = \"Classification Tree\")\n\np1 &lt;- ct_auc %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + \n  geom_path(lwd = 1.5, alpha = 0.8) +\n  geom_abline(lty = 3) + \n  coord_equal() + \n  scale_color_viridis_d(option = \"plasma\", end = .6)+\n  ylim(c(0,1))+\n  theme_minimal(base_size = 11)+\n  theme(legend.position = \"none\")\n\np2 &lt;- ct_pr %&gt;%\n  ggplot(aes(x = recall, y = precision, col = model)) + \n  geom_path(lwd = 1.5, alpha = 0.8) +\n  coord_equal() + \n  scale_color_viridis_d(option = \"plasma\", end = .6)+\n  ylim(c(0,1))+\n  theme_minimal(base_size = 11)+\n  theme(legend.position = \"none\")\n\np&lt;-p1 | p2\n\np + plot_annotation(\n  title = \"ROC curve and PR curve for a classification tree\"\n)\n\n\n\n\n\n\n\n\n\n\nSolution 5.12 (Exercise 5.13). \n\nlast_ct_fit %&gt;%\n  extract_fit_engine()%&gt;%\n  vip(geom = \"col\", aesthetics = list(fill = \"midnightblue\", alpha = 0.8)) +\n  scale_y_continuous(expand = c(0, 0))+\n  theme_minimal(base_size = 11)\n\n\n\n\n\n\n\n\n\n\nSolution 5.13 (Exercise 5.14). \n\ncores &lt;- parallel::detectCores()\n\nrf_model_spec &lt;- rand_forest(\n  mode = \"classification\",\n  mtry = tune(),\n  min_n = tune(),\n  trees = 1000\n) %&gt;%\n  set_engine(\"ranger\",\n             num.threads = cores,\n             importance = \"permutation\")\n\nci_wf &lt;- ci_wf %&gt;% update_model(rf_model_spec)\n\nrf_res &lt;- ci_wf %&gt;% \n  tune_grid(grid = 10,\n            metrics = multi_metrics,\n            resamples = folds_ci,\n            control = control_grid(save_pred = TRUE)\n  )\n\nrf_res_best &lt;- rf_res %&gt;% select_best(metric = \"roc_auc\")\n\nrf_auc &lt;- \n  rf_res %&gt;% \n  collect_predictions(parameters = rf_res_best) %&gt;% \n  roc_curve(Attrition_Flag, .pred_1) %&gt;% \n  mutate(model = \"Random Forest\")\n\nrf_pr &lt;- \n  rf_res %&gt;% \n  collect_predictions(parameters = rf_res_best) %&gt;% \n  pr_curve(Attrition_Flag, .pred_1) %&gt;% \n  mutate(model = \"Random Forest\")\n\np1 &lt;- rf_auc %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + \n  geom_path(lwd = 1.5, alpha = 0.8) +\n  geom_abline(lty = 3) + \n  coord_equal() + \n  scale_color_viridis_d(option = \"plasma\", end = .6)+\n  ylim(c(0,1))+\n  theme_minimal(base_size = 11)+\n  theme(legend.position = \"none\")\n\np2 &lt;- rf_pr %&gt;%\n  ggplot(aes(x = recall, y = precision, col = model)) + \n  geom_path(lwd = 1.5, alpha = 0.8) +\n  coord_equal() + \n  scale_color_viridis_d(option = \"plasma\", end = .6)+\n  ylim(c(0,1))+\n  theme_minimal(base_size = 11)+\n  theme(legend.position = \"none\")\n\np&lt;-p1 | p2\n\np + plot_annotation(\n  title = \"ROC Curve and PR curve for a random forest\"\n)\n\n\n\n\n\n\n\n\nFeature importance plot:\n\nlast_rf_fit &lt;- ci_wf %&gt;%\n  finalize_workflow(rf_res_best) %&gt;%\n  last_fit(split)\n\nlast_rf_fit %&gt;%\n  extract_fit_parsnip()%&gt;%\n  vip(geom = \"col\", aesthetics = list(fill = \"midnightblue\", alpha = 0.8)) +\n  scale_y_continuous(expand = c(0, 0))+\n  theme_minimal(base_size = 11)\n\n\n\n\n\n\n\n\n\n\nSolution 5.14 (Exercise 5.16). \n\ncols &lt;- c(\"#80003A\",\"#506432\",\"#FFC500\")\nnames(cols) &lt;- c(\"cl\", \"lr\", \"rf\")\nplot_title &lt;- glue::glue(\n    \"ROC- and PR-curve for &lt;span style='color:{cols['lr']};'&gt;\n    penalized logistic regression&lt;/span&gt;,&lt;br&gt;\n    &lt;span style='color:{cols['cl']};'&gt;classification tree&lt;/span&gt;,\n    and &lt;span style='color:{cols['rf']};'&gt;random forest&lt;/span&gt;\"\n    )\np1 &lt;- bind_rows(ct_auc, lr_auc, rf_auc) %&gt;% \n  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + \n  geom_path(lwd = 1.5) +\n  geom_abline(lty = 3) + \n  coord_equal() + \n  scale_color_manual(values = unname(cols))+\n  theme_minimal(base_size = 11)+\n  theme(legend.position = \"none\")\n  \n\np2 &lt;- bind_rows(ct_pr, lr_pr, rf_pr) %&gt;% \n  ggplot(aes(x = recall, y = precision, col = model)) + \n  geom_path(lwd = 1.5, alpha = 0.8) +\n  coord_equal() + \n  scale_color_manual(values = unname(cols))+\n  theme_minimal(base_size = 11)+\n  theme(legend.position = \"none\")\n  \n\n(p1|p2) +\n  plot_annotation(\n  title = plot_title,\n  theme = theme(plot.title = element_markdown()))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Forests</span>"
    ]
  },
  {
    "objectID": "05_bagging_rf.html#footnotes",
    "href": "05_bagging_rf.html#footnotes",
    "title": "5  Random Forests",
    "section": "",
    "text": "Datasets in which negatives are far more represented than positives↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Random Forests</span>"
    ]
  },
  {
    "objectID": "06_boosting.html",
    "href": "06_boosting.html",
    "title": "6  Boosting",
    "section": "",
    "text": "6.1 Introduction\nIn this exercise session, we will consider multiple advanced machine learning models. Our base model will not be a penalized logistic regression as in Session 05 rather than a random forest. The models we are considering subsequently are also widely used in application as their performance on classification tasks is superb! However, similar to random forests, their explainability is still subpar compared to a simple logistic regression or classification tree. Before we learn how to train and finetune these models, we will discuss some theoretical aspects.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Boosting</span>"
    ]
  },
  {
    "objectID": "06_boosting.html#introduction",
    "href": "06_boosting.html#introduction",
    "title": "6  Boosting",
    "section": "",
    "text": "6.1.1 Confusion matrices in R\nSince we will be working with on a classification task in the exercises, being able to construct a confusion matrix is crucial.\nConsider the following example data set which is part of the {yardstick} library:\n\nlibrary(tidyverse)\nlibrary(yardstick)\n\ntwo_class_example %&gt;% glimpse()\n\nRows: 500\nColumns: 4\n$ truth     &lt;fct&gt; Class2, Class1, Class2, Class1, Class2, Class1, Class1, Clas…\n$ Class1    &lt;dbl&gt; 0.0035892426, 0.6786210540, 0.1108935221, 0.7351617031, 0.01…\n$ Class2    &lt;dbl&gt; 9.964108e-01, 3.213789e-01, 8.891065e-01, 2.648383e-01, 9.83…\n$ predicted &lt;fct&gt; Class2, Class1, Class2, Class1, Class2, Class1, Class1, Clas…\n\n\nSay, we want to label change the label Class1 to Positive and Class2 to Negative. Then, we can simply apply the mutate() function:\n\ntwo_class_example &lt;- two_class_example %&gt;%\n  mutate(\n    truth = fct_relabel(truth,\n                        ~if_else(.==\"Class1\",\"Positive\",\"Negative\")\n                        ),\n    predicted = fct_relabel(predicted,\n                          ~if_else(.==\"Class1\",\"Positive\",\"Negative\")\n                          )\n  )\n\nTo create a simple confusion matrix, we can use the conf_mat function that is also part of the {yardstick} library:\n\n(cm_example &lt;- two_class_example %&gt;%\n   conf_mat(truth =truth,estimate = predicted))\n\n          Truth\nPrediction Positive Negative\n  Positive      227       50\n  Negative       31      192\n\n\nWe can also use the ggplot function to create a visually more appealing version of this matrix. To do so, we first have to convert the confusion matrix into a proper data frame and set the levels of the Predictions and Truth.\n\ncm_tib&lt;- as_tibble(cm_example$table)%&gt;% \n  mutate(\n    Prediction= fct_rev(Prediction),\n    Truth = factor(Truth)\n)\n\nIn the snippet above we had to reverse the levels of the variable Prediction so that we can place the TP values in the top left, and TN values in the bottom right of the confusion matrix.\nOnce the confusion matrix has been converted to a data frame, we can pass it into the ggplot function with the argument fill set to n. The geom_tile() function places a tile at each coordinate provided by the data frame (Note: Coordinates are discrete and given by Negative and Positive). The argument colour = \"gray50\" adds a gray border to each tile. By adding the geom_text() function where the aesthetics are provided by the label argument, we can add the number of samples falling into each class (TP, TN, FP, and FN) to each tile. The scale_fill_gradient() function, allows to change the colors of the tiles with respect to the value of n. Here, a low value of n indicates the the tile will be \"white\" and a high value of n indicates that the color of the tile will be light green (with HEX Code \"#9AEBA3\"). Setting the theme to minimal, and removing the legend yields a cleaner representation of the confusion matrix.\n\ncm_tib %&gt;% ggplot(aes(x = Prediction, y = Truth,fill = n)) +\n    geom_tile( colour = \"gray50\")+\n    geom_text(aes(label = n))+\n    scale_fill_gradient(low = \"white\", high = \"cadetblue2\")+\n    theme_minimal()+\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n6.1.2 Tuning an XGBoost model\nSince we have intensively covered random forests in previous exercises, we only consider an XGBoost model in this introduction. AdaBoost is rarely used in practice anymore, which is why we will directly move towards training an XGBoost model. The approach is similar to training and tuning every other model but compared to previous exercises we will not perform cross validation, rather than a simple training/validation/test split to save some time.\nWe can create an XGBoost model by using the boost_tree function. Looking at the documentation, you will notice that there are quite a few parameters for us to consider:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\ntrees\nNumber of trees contained in the ensamble\n\n\ntree_depth\nInteger for the maximum depth of the trees\n\n\nmin_n\nMinimum number of data points in a node required for a split\n\n\nmtry\nNumber of randomly selected at each split\n\n\n\nThe parameters above are not new to us. In fact, they are the exact same parameters we use for training a random forest model. That is why we will not go into detail with respect to the ones above.\nThere are, however, a few new parameters that are worth an explanation:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nloss_reduction\nNumber for the reduction in loss that is required to split further \\(\\in [0,\\infty]\\)\n\n\nsample_size\nSubsample ratio of the training instances \\(\\in (0,1].\\)\n\n\nlearn_rate\nRate at which the algorithm adapts from iteration to iteration \\(\\in [0,1]\\).\n\n\n\nThe three parameters above have only been referenced in the lectures so far, so let’s quickly describe them in a bit more detail.\n\n6.1.2.0.1 loss_reduction :\nIn Exercise 6.2, we will derive the optimal expansion coefficient \\(\\alpha\\) (similar to the coefficients in linear regression) which solves the minimization problem\n\\[\n(\\alpha_b,h_b) = \\arg \\min_{\\alpha &gt;0, h\\in\\mathcal{H}}\\sum_{n=1}^N L(y_n,\\hat{f}^{(b-1)}(x_n)+\\alpha h(x_n))\n\\]\nHere, \\(L\\) denotes a loss function that we aim to minimize with respect to \\(\\alpha\\) and an additional (potentially weak) learner \\(h\\) that we add to the previous estimator.\nIf the term\n\\[\n\\left| \\sum_{n=1}^N L(y_n,\\hat{f}^{(b-1)}(x_n)+\\alpha h(x_n)) - \\sum_{n=1}^N L(y_n,\\hat{f}^{(b)}(x_n)+\\alpha h(x_n)) \\right|,\n\\]\ni.e., the loss reduction between step \\(b\\) and \\(b+1\\) is smaller than the parameter loss_reduction, the algorithm stops.\n\n\n6.1.2.0.2 sample_size :\nLet \\(q\\in(0,1]\\) denote the sample_size parameter and \\(N\\) the number of samples in our training data. Then, XGBoost selects \\(q\\cdot N\\) samples prior to growing trees. This subsampling occurs once in every boosting iteration.\n\n\n6.1.2.0.3 learn_rate :\nIn simple terms, the learning rate specifies how quickly the model adapts to the training data. An analogy can be drawn to gradient based models that use gradient descent on a loss function. Here, the goal is to minimize the loss function by stepping towards its minimum. To illustrate the learning rate in a gradient descent context, consider the following examples where we can imagine the polynomial of degree four to be a loss function that we try to minimize.\n\nChoosing a learning rate that is too high, might result in missing an optimal model because it is being stepped over, while a learning rate chosen too small might result in the objective never being reached at all.\nSimilar to choosing a learning rate that is too high, we could also choose a learning rate that is too low, resulting in the global minimum never being reached at all.\n\nThe learning rate in the XGBoost algorithm describes a factor \\(\\gamma\\) that scales the output of the most recently fit tree that is added to the model. In simple terms, the learning rate in the XGBoost algorithm describes a shrinkage parameter.\nIn the following example, we will try to predict the base rent prices in Munich using an XGBoost model. The data Apartment rental offers in Germany is the same as in Exercise 04.\n\nlibrary(tidymodels)\n\ndata_muc &lt;- read.csv(\"data/rent_muc.csv\")\n\ndata_muc &lt;- data_muc %&gt;%\n  select(!c(\"serviceCharge\",\"heatingType\",\"picturecount\",\n             \"totalRent\",   \"firingTypes\",\"typeOfFlat\",\n             \"noRoomsRange\", \"petsAllowed\",\n             \"livingSpaceRange\",\"regio3\",\"heatingCosts\",\n             \"floor\",\"date\", \"pricetrend\")) %&gt;%\n  mutate(\n    interiorQual = factor(\n      interiorQual,\n      levels = c(\"simple\", \"normal\", \"sophisticated\", \"luxury\"),\n      ordered = TRUE\n      ),\n    condition = factor(condition,\n      levels = c(\"need_of_renovation\", \"negotiable\",\"well_kept\",\n                 \"refurbished\",\"first_time_use_after_refurbishment\",\n                 \"modernized\",\"fully_renovated\", \"mint_condition\",\n                 \"first_time_use\"),\n      ordered = TRUE),\n     geo_plz = factor(geo_plz),\n    across(where(is.logical),as.factor)\n  ) %&gt;%\n  filter(baseRent&lt;=3000, livingSpace&lt;=200)\n\nInstead of using a cross validation approach, we will use a simple training/validation/test split to reduce computing time.\nBy using the validation split function on the training data, we split the training data into a training and validation subset. The data_val object can then be passed into the tune_grid function in the same fashion as we did with a cross validation object.\n\nset.seed(24)\nsplit_rent &lt;- initial_split(data_muc)\ndata_train &lt;- training(split_rent)\ndata_val &lt;- validation_split(data_train)\ndata_test &lt;- testing(split_rent)\n\nPreprocessing of the data is handled by the following recipe.\n\nrec_rent &lt;- recipe(\n    formula = baseRent ~., \n    data = data_train\n  ) %&gt;%\n  update_role(scoutId, new_role = \"ID\") %&gt;%\n  step_ordinalscore(all_ordered_predictors())%&gt;%\n  step_novel(all_factor_predictors())%&gt;%\n  step_unknown(all_factor_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors())%&gt;%\n  step_impute_mean(all_numeric_predictors()) \n\nAfter preprocessing the data, we can create a workflow object and specify our XGBoost model.\n\nwf_rent &lt;- workflow() %&gt;%\n  add_recipe(rec_rent)\n\nWe want to tune every parameter except for trees. Since we are using a regression model, we need to use the mode \"regression\".\n\nset.seed(121)\n\nxgb_model &lt;- boost_tree(\n  trees = 1000,\n  tree_depth = tune(),\n  min_n = tune(),\n  mtry = tune(),         \n  loss_reduction = tune(),                     \n  learn_rate = tune()                          \n) %&gt;%\n  set_mode(\"regression\")\n\nwf_rent &lt;- wf_rent %&gt;% add_model(xgb_model)\n\nTo tune the model and select the best model based on the performance on the validation data, we use the tune_grid function.\n\nmulti_metrics &lt;- metric_set(rmse,rsq,mae)\n\nxgb_tune_res &lt;- wf_rent %&gt;%\n  tune_grid(\n    resamples = data_val,\n    metrics = multi_metrics,\n    grid = 20,\n  )\n\nAfter tuning the model parameters, we use the optimal candidate hyperparameters to train a final model on all the training data and evaluate it on the test data.\n\nxgb_best_parm &lt;- xgb_tune_res %&gt;% select_best(metric = \"rmse\")\n\nlast_xgb_fit &lt;- wf_rent %&gt;%\n  finalize_workflow(xgb_best_parm) %&gt;%\n  last_fit(split_rent)\n\nlast_xgb_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard     255.    pre0_mod0_post0\n2 rsq     standard       0.808 pre0_mod0_post0\n\n\nIn Exercise 04, where we performed the same regression task with a decision tree, the OOS performance was substantially worse:\n\n\n\nMetric\nEstimate\n\n\n\n\nRMSE\n622\n\n\n\\(R^2\\)\n0.616\n\n\n\nWe can, therefore, conclude that the XGBoost model is more suitable for estimating the base rent for rental apartments in Munich.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Boosting</span>"
    ]
  },
  {
    "objectID": "06_boosting.html#exercises",
    "href": "06_boosting.html#exercises",
    "title": "6  Boosting",
    "section": "6.2 Exercises",
    "text": "6.2 Exercises\n\n6.2.1 Theoretical exercises\n\nExercise 6.1 Explain in your own words, the difference between Boosting (Trees), Bagging (Trees), and Random Forests.\n\n\nExercise 6.2 On slide 89 in the lecture notes, the AdaBoost algorithm stated as follows.\n\n\n\n\n\n\nFigure 6.1: AdaBoost Algorithm\n\n\n\nIn the third line of Figure 6.1 , the scaling coefficients \\(\\alpha_b\\) at step \\(b\\in\\{1,..,B\\}\\) are set to\n\\[\\begin{equation}\n  \\alpha_b = \\frac{1}{2}\\log\\left(\\frac{1-\\mathrm{err}_b}{\\mathrm{err}_b}\\right).\n\\end{equation}\\]\nThe goal of this exercise is to figure out, why the scaling coefficients are defined that way. The essence of this derivation lies in the more general idea of boosting, where the minimization problem at step \\(b\\in\\{1,...,B\\}\\) is given by (cf. Slide 91)\n\\[\\begin{equation}\\label{eq:general_loss}\n  (\\alpha_b,h_b) = \\underset{\\alpha&gt;0,h\\in\\mathcal{H}}{\\mathrm{argmin}} \\sum_{i=1}^{n}L(y_i,\\hat{f}^{(b-1)}(x_i)+\\alpha h(x_i))\n\\end{equation}\\]\nFor AdaBoost, the loss function \\(L\\) is defined by\n\\[\\begin{equation}\\label{eq:exponential_loss}\n  L(y,\\hat{f}(x)) = \\exp(-y\\hat{f}(x))\n\\end{equation}\\]\nBy minimizing \\(\\eqref{eq:general_loss_trans}\\) with respect to \\(\\alpha\\), we obtain the desired coefficient.\n\nShow that \\[\\begin{equation}\\label{eq:general_loss_trans}\n  \\underset{\\alpha&gt;0,h\\in\\mathcal{H}}{\\mathrm{argmin}} \\sum_{i=1}^{n}L(y_i,\\hat{f}^{(b-1)}(x_i)+\\alpha h(x_i)) = \\underset{\\alpha&gt;0,h\\in\\mathcal{H}}{\\mathrm{argmin}} \\sum_{i=1}^{n} w_b(i) \\exp(-\\alpha y_i h(x_i))\n\\end{equation}\\]\nShow that the objective function of the right hand side of \\(\\eqref{eq:general_loss_trans}\\) can be expressed as\n\\[\\begin{equation} \\label{eq:exp_minus}\n  e^{-\\alpha}\\sum_{y_i = h(x_i)} w_b(i) + e^{\\alpha}\\sum_{y_i \\neq h(x_i)} w_b(i)\n\\end{equation}\\]\nShow that \\(\\eqref{eq:exp_minus}\\) is equal to \\[\\begin{equation}\\label{eq:sum_shit2}\n(e^{\\alpha}-e^{-\\alpha})\\sum_{i = 1}^n w_b(i)I(y_i\\neq h(x_i)) + e^{-\\alpha} \\sum_{i=1}^n w_b(i)\n\\end{equation}\\]\nArgue by using \\(\\eqref{eq:sum_shit2}\\) that for any \\(\\alpha &gt;0\\) the solution to \\(\\eqref{eq:general_loss}\\) for \\(h\\) is given by\n\\[\\begin{equation}\\label{eq:min_h}\n  h_b = \\underset{h}{\\mathrm{argmin}}\\sum_{i=1}^{n} w_b(i) I(y_i\\neq h(x_i)).\n\\end{equation}\\]\nFinally, plug the objective function \\(\\eqref{eq:sum_shit2}\\) into \\(\\eqref{eq:general_loss_trans}\\) and show that minimizing the loss function for \\(\\alpha\\) yields\n\\[\\begin{equation}\n  \\alpha_b =      \\frac{1}{2}\\log\\left(\\frac{1-\\mathrm{err}_b}{\\mathrm{err}_b}\\right),\n\\end{equation}\\]\nwhere \\[\\begin{equation}\n  \\mathrm{err}_b =  \\frac{\\sum_{i=1}^nw_b(i)I(y_i\\neq h_b(x_i))}{\\sum_{i=1}^nw_b(i)}.\n\\end{equation}\\]\nHint: You can assume that the candidate for \\(\\alpha\\) is indeed a minimizer.\n\n\n\n\n6.2.2 Programming Exercises\nThe following exercise is similar to Exercise 5.3.2. However, instead of fitting penalized logistic regression and classification tree, we fit a XGBoost and LightGBM model on the credit card data.\n\nlibrary(\"finetune\")\nlibrary(\"bonsai\")\nlibrary(\"patchwork\")\nlibrary(\"ggtext\")\n\nThe dataset we will consider in this exercise will be the Credit Card Customers data set that we already used in previous exercises. You can either download it again using the provided link or the button below.\n\nDownload BankChurners\n\nRecall that the data set consists of 10,127 entries that represent individual customers of a bank including but not limited to their age, salary, credit card limit, and credit card category.\nThe goal is to find out whether a customer will stay or leave the bank given the above features.\nThe following training, validation and test split should be used for training the models of the subsequent exercises.\n\ncredit_info &lt;- read.csv(\"data/BankChurners.csv\")\n\nlevels_income &lt;- c(\"Less than $40K\",\"$40K - $60K\",\n             \"$60K - $80K\",\"$80K - $120K\",\"$120K +\")\n\nlevels_education &lt;- c(\"Uneducated\", \"High School\",\"College\",\n                      \"Graduate\",  \"Post-Graduate\", \"Doctorate\")\ncredit_info &lt;- credit_info %&gt;%\n  mutate(\n    across(\n      where(~is.character(.)|is.factor(.)),\n      ~if_else(.%in% c(\"Unknown\",\"unknown\"),NA,.)\n    ),\n    Income_Category = factor(\n        Income_Category,\n        levels = levels_income,\n        ordered = TRUE\n        ),\n    Education_Level = factor(\n        Education_Level,\n        levels = levels_education,\n        ordered = TRUE\n    ),\n    Attrition_Flag = factor(\n      if_else(\n        Attrition_Flag==\"Existing Customer\",\n        0, 1)\n    ),\n    Attrition_Flag = fct_rev(Attrition_Flag)\n  )\n\n\nset.seed(121)\nsplit &lt;- initial_split(credit_info, strata = Attrition_Flag)\ndata_train_ci &lt;- training(split)\ndata_val_ci &lt;- validation_split(data_train_ci)\ndata_test_ci &lt;- testing(split)\n\nPreprocessing of the data is handled by the following recipe.\n\nrec_ci &lt;- recipe(\n  formula = Attrition_Flag ~.,\n  data = data_train_ci\n) %&gt;%\n  update_role(CLIENTNUM, new_role = \"Unique Identifier\")%&gt;%\n  step_ordinalscore(all_ordered_predictors()) %&gt;%\n  step_unknown(all_factor_predictors()) %&gt;%\n  step_dummy(Marital_Status,Gender,Card_Category)%&gt;%\n  step_impute_mean(all_numeric_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_corr(all_predictors())\n\nci_wf &lt;- workflow() %&gt;%\n  add_recipe(rec_ci) \n\nmulti_metrics &lt;- metric_set(roc_auc,pr_auc,accuracy,recall)\n\nNote, that we encoded the target variable Attrition_Flag with new labels, namely Positive and Negative. Positive corresponds to a customer leaving the bank, while Negative corresponds to a customer staying with the bank.\n\nExercise 6.3 Create and train a random forest model using the provided recipe with \\(1000\\) trees and tune the parameters mtry and min_n.\nTune the model on a grid of size 20 using the tune_grid function on the validation split generated with the training data.\nFind the best model by evaluating the tuning results with respect to the models’ accuracy.\nBased on these parameters train a model on the whole training data.\n\n\nExercise 6.4 Create two tibbles containing the data necessary to plot a ROC- and PR curve. When creating the tibbles, add a column containing the model name \"Random forest\", so that we can correctly identify the models later during model evaluation.\n\n\nExercise 6.5 Tune a XGBoost model in the same fashion as the random forest. Set the number of trees to 1000, and every other parameter, except for sample_size, to tune().\nAfter tuning and refitting the best model on the whole training data, repeat Exercise 6.4 for this XGBoost model on the test data.\n\n\n\n\n\n\n\nNote\n\n\n\nThe following model is not relevant for the exam. However, it is extremely relevant in today’s ML landscape, so I encourage you to solve the following exercises as well.\n\n\n\nExercise 6.6 (Bonus Exercise) The last model we want to train is called LightGBM. It was developed by Microsoft and is, as well as XGBoost, a gradient-based ensemble learner. An advantage compared to XGBoost is the focus on performance and scalability, meaning that it is designed to work well on CPUs while trying to at least match the performance of XGBoost.\nThe steps for training a LightGBM model are exactly the same as for training an XGBoost model, except for the model specification. Here we set the engine to \"lightgbm\" instead of \"xgboost\". Every other parameter stays the same, thanks to the {tidymodels} framework.\nRepeat Exercise 6.5 for a LightGBM model.\n\n\n\n\n\n\n\nTip\n\n\n\nIf you get stuck recreating the following plots, revisit the solutions to Exercise Sheet 05, where we created the same plot for a penalized logistic regression, a classification tree, and a random forest.\n\n\n\nExercise 6.7 Create a plot showing the ROC- and PR-curve for each of the models we trained in the previous exercises (Random Forest, XGBoost, LightGBM). Compare the performances visually and decide which model performed the best. For reference, you can find what such a plot could look like below.\n\n\nExercise 6.8 For each of the previously trained models (Random Forest, XGBoost, LightGBM), create a confusion matrix based on the test sets to evaluate which model performed best on unseen data.\n\n\nExercise 6.9 For the confusion matrices above, find out which model has the overall best out-of-sample performance. For this best model, calculate the following metrics:\n\nSensitivity\nPrecision\nAccuracy",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Boosting</span>"
    ]
  },
  {
    "objectID": "06_boosting.html#solutions",
    "href": "06_boosting.html#solutions",
    "title": "6  Boosting",
    "section": "6.3 Solutions",
    "text": "6.3 Solutions\n\nSolution 6.1 (Exercise 6.1). \n\nBagging (bootstrap aggregation) is a special case of random forests. Here, we also create a predetermined number of trees. However, the main difference is that in Bagging the full set of features is considered when creating a split for a node. In a random forest, only a subset of all features is randomly considered when creating a split for a new node.\nBoosting (Trees) combines many weak learners, e.g., tree stumps, to make a prediction. Compared to Bagging and Random forests, those weak learners are weighted, e.g., one tree stump has more say than another when making a final decision. Furthermore, weak learners are not created independently because each weak learner is built by considering the previous learners’ mistakes.\n\n\n\nSolution 6.2 (Exercise 6.2). \n\nPlugging \\(\\eqref{eq:exponential_loss}\\) into \\(\\eqref{eq:general_loss}\\) yields\n\\[\\begin{align*}\n  \\underset{\\alpha&gt;0,h\\in\\mathcal{H}}{\\mathrm{argmin}}\\sum_{i=1}^{n}L(y_i,\\hat{f}^{(b-1)}(x_i)+\\alpha h(x_i)) &=\\underset{\\alpha&gt;0,h\\in\\mathcal{H}}{\\mathrm{argmin}}\\sum_{i=1}^{n}\\exp(-y_i(\\hat{f}^{(b-1)}(x_i)+\\alpha h(x_i)))\\\\\n  &= \\underset{\\alpha&gt;0,h\\in\\mathcal{H}}{\\mathrm{argmin}}\\sum_{i=1}^{n} \\underbrace{\\exp(-y_i\\hat{f}^{(b-1)}(x_i))}_{\\coloneqq w_b(i)}\\exp(-\\alpha y_i h(x_i)) \\\\\n  &= \\underset{\\alpha&gt;0,h\\in\\mathcal{H}}{\\mathrm{argmin}}\\sum_{i=1}^{n} w_b(i)\\exp(-\\alpha y_i h(x_i)).\n\\end{align*}\\]\nSince \\(y_i\\in \\{1,-1\\}\\) and \\(h(x_i)\\in \\{-1,1\\}\\) as well, either \\(y_i\\cdot h(x_i) = 1\\) if \\(y_i = h(x_i)\\), or \\(y_i\\cdot h(x_i) = -1\\) if \\(y_i \\neq h(x_i)\\) (since one of the two terms is equal to \\(-1\\) and the other equal to \\(1\\)). The condition above can be formalized as\n\\[\\begin{equation}\n    y_i\\cdot h(x_i) = \\begin{cases}1 &\\text{ if } y_i=h(x_i)\\\\\n                                   -1 &\\text{ if } y_i\\neq h(x_i)\n                      \\end{cases},\n\\end{equation}\\] and rewriting the right hand side of \\(\\eqref{eq:general_loss_trans}\\) using these conditions yields \\[\\begin{align*}\n  \\sum_{i=1}^n w_b(i)\\exp(-\\alpha y_i h(x_i)) &= \\sum_{i=1}^n (I(y_i = h(x_i))+I(y_i \\neq h(x_i)) w_b(i)\\exp(-\\alpha y_i h(x_i))\\\\\n  &=\\sum_{i=1}^n I(y_i = h(x_i)) w_b(i)\\exp(-\\alpha y_i h(x_i)) \\\\\n  &\\qquad +\\sum_{i=1}^n I(y_i \\neq h(x_i))w_b(i)\\exp(-\\alpha y_i h(x_i))\\\\\n  &= \\sum_{y_i = h(x_i)} w_b(i)\\exp(-\\alpha\\cdot 1) + \\sum_{y_i \\neq h(x_i)} w_b(i)\\exp(-\\alpha\\cdot -1)\\\\\n  &= e^{-\\alpha} \\sum_{y_i = h(x_i)} w_b(i)+ e^{\\alpha}\\sum_{y_i \\neq h(x_i)} w_b(i)\\\\\n\\end{align*}\\]\nBy expanding and rearranging \\(\\eqref{eq:sum_shit2}\\), we obtain \\[\\begin{align}\n    &e^\\alpha \\sum_{i=1}^n w_b(i)I(y_i\\neq h(x_i)) +\n      e^{-\\alpha}\\sum_{i = 1}^n w_b(x_i)-w_b(i)I(y_i\\neq h(x_i))\\\\\n    &\\quad =e^\\alpha \\sum_{y_i\\neq h(x_i)}^n w_b(i) +\n      e^{-\\alpha}\\sum_{I(y_i= h(x_i))}^n w_b(x_i).\n\\end{align}\\]\nUsing the results of sub tasks 1)-3), we can rewrite the minimization problem of \\(\\eqref{eq:general_loss}\\) as follows:\n\\[\\begin{equation}\\label{eq:exr4eq}\n\\underset{\\alpha&gt;0,h\\in\\mathcal{H}}{\\mathrm{argmin}}\\left\\{ (e^{\\alpha}-e^{-\\alpha})\\sum_{i = 1}^n w_b(i)I(y_i\\neq h(x_i)) + e^{-\\alpha} \\sum_{i=1}^n w_b(i)\\right\\}\n\\end{equation}\\]\n\\(\\eqref{eq:exr4eq}\\) only contains one term that depends \\(h\\), i.e.\n\\[\\begin{equation}\n    (e^{\\alpha}-e^{-\\alpha})\\sum_{i = 1}^n w_b(i)I(y_i\\neq h(x_i)).\n\\end{equation}\\]\nTherefore, any function \\(h\\) that minimizes \\(\\eqref{eq:min_h}\\) also minimizes \\(\\eqref{eq:exr4eq}\\).\nTo minimize \\(\\eqref{eq:sum_shit2}\\) with respect to \\(\\alpha\\), we have to set the derivative of \\(\\eqref{eq:sum_shit2}\\) with respect to \\(\\alpha\\) to 0. Define \\(t\\coloneqq \\sum_{i = 1}^n w_b(i)I(y_i\\neq h(x_i))\\) and \\(s\\coloneqq \\sum_{i = 1}^n w_b(i)\\). Then, \\[\\begin{equation*}\n  \\frac{\\partial}{\\partial \\alpha} (e^\\alpha - e^{-\\alpha})t+e^{-\\alpha}s) = te^\\alpha-(s-t)e^{-\\alpha}.\n\\end{equation*}\\] Now, \\[\\begin{align*}\n  \\frac{\\partial}{\\partial \\alpha} ((e^\\alpha - e^{-\\alpha})t+e^{-\\alpha}s) = te^\\alpha-(s-t)e^{-\\alpha} &\\overset{!}{=} 0 \\\\\n  &\\iff te^\\alpha = (s-t)e^{-\\alpha} \\\\\n  &\\iff e^{2\\alpha} = \\frac{(s-t)}{t} \\\\\n  &\\iff 2\\alpha = \\log\\left(\\frac{(s-t)}{t}\\right)\\\\\n  &\\iff \\alpha = \\frac{1}{2}\\log\\left(\\frac{(s-t)}{t}\\right).\n\\end{align*}\\] Defining \\[\\begin{equation}\n\\mathrm{err}_b \\coloneqq \\frac{\\sum_{i = 1}^n w_b(i)I(y_i\\neq h(x_i))}{\\sum_{i = 1}^n w_b(i)}\n\\end{equation}\\] yields \\[\\begin{equation}\n\\frac{1-\\mathrm{err}_b}{\\mathrm{err}_b} = \\frac{1}{\\mathrm{err}_b}-1 = \\frac{\\sum_{i = 1}^n w_b(i)}{\\sum_{i = 1}^n w_b(i)I(y_i\\neq h(x_i))}-1 = \\frac{s}{t}-1 = \\frac{s-t}{t}.\n\\end{equation}\\] Finally, re-substituting \\(s\\) and \\(t\\) in \\(\\frac{1}{2}\\log\\left(\\frac{(s-t)}{t}\\right)\\) yields \\[\\begin{equation}\n  \\alpha = \\frac{1}{2}\\log\\left(\\frac{1-\\mathrm{err}_b}{\\mathrm{err}_b}\\right).\n\\end{equation}\\]\n\n\n\nSolution 6.3 (Exercise 6.3). \n\nset.seed(121)\n\ncores &lt;- parallel::detectCores()\n\nrf_model &lt;- rand_forest(\n  mode = \"classification\",\n  mtry = tune(),\n  min_n = tune(),\n  trees = 1000\n) %&gt;%\n  set_engine(\"ranger\",\n             num.threads = cores\n  )\n\nci_wf &lt;- ci_wf %&gt;% add_model(rf_model)\n\nrf_tune_res &lt;- ci_wf %&gt;% \n    tune_grid(grid = 20,\n              resamples = data_val_ci,\n              metrics = multi_metrics\n    )\n\nrf_best_parm &lt;- rf_tune_res %&gt;%\n  select_best(metric = \"accuracy\")\n\nlast_rf_fit &lt;- ci_wf %&gt;%\n  finalize_workflow(rf_best_parm) %&gt;%\n  last_fit(split)\n\n\n\nSolution 6.4 (Exercise 6.4). \n\nrf_roc &lt;- last_rf_fit %&gt;% \n  collect_predictions() %&gt;% \n  roc_curve(Attrition_Flag, .pred_1) %&gt;% \n  mutate(model = \"Random Forest\")\n\nrf_pr &lt;- last_rf_fit %&gt;% \n  collect_predictions() %&gt;% \n  pr_curve(Attrition_Flag, .pred_1) %&gt;% \n  mutate(model = \"Random Forest\")\n\n\n\nSolution 6.5 (Exercise 6.5). \n\nset.seed(121)\n\nxgb_model &lt;- boost_tree(\n  trees = 1000,\n  tree_depth = tune(),\n  min_n = tune(),\n  mtry = tune(),         \n  loss_reduction = tune(),                     \n  learn_rate = tune()                          \n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\nci_wf &lt;- ci_wf %&gt;% update_model(xgb_model)\n\ndoParallel::registerDoParallel()\n\nxgb_tune_res &lt;- tune_grid(\n  ci_wf,\n  resamples = data_val_ci,\n  grid = 20,\n  metrics = multi_metrics\n)\n\n\nxgb_best_parm &lt;- xgb_tune_res %&gt;% select_best(metric = \"accuracy\")\n\nlast_xgb_fit &lt;- ci_wf %&gt;%\n  finalize_workflow(xgb_best_parm) %&gt;%\n  last_fit(split)\n\nxgb_roc &lt;- \n  last_xgb_fit %&gt;% \n  collect_predictions() %&gt;% \n  roc_curve(Attrition_Flag, .pred_1) %&gt;% \n  mutate(model = \"XGBoost\")\n\nxgb_pr &lt;- \n  last_xgb_fit %&gt;% \n  collect_predictions() %&gt;% \n  pr_curve(Attrition_Flag, .pred_1) %&gt;% \n  mutate(model = \"XGBoost\")\n\n\n\nSolution 6.6 (Exercise 6.6). \n\nset.seed(121)\n\nlightgbm_model &lt;- boost_tree(\n  trees = 1000,\n  tree_depth = tune(),\n  min_n = tune(),\n  loss_reduction = tune(),                     \n  mtry = tune(),         \n  learn_rate = tune()                          \n) %&gt;%\n  set_engine(\"lightgbm\") %&gt;%\n  set_mode(\"classification\")\n\nci_wf &lt;- ci_wf %&gt;% update_model(lightgbm_model)\n\nlightgbm_res &lt;- tune_grid(\n  ci_wf,\n  resamples = data_val_ci,\n  grid = 20,\n  metrics = multi_metrics\n)\n\nlightgbm_res_best &lt;- lightgbm_res %&gt;% select_best(metric = \"accuracy\")\n\nlast_lightgbm_fit &lt;- ci_wf %&gt;%\n  finalize_workflow(lightgbm_res_best) %&gt;%\n  last_fit(split)\n\nlightgbm_roc &lt;- last_lightgbm_fit %&gt;% \n  collect_predictions() %&gt;% \n  roc_curve(Attrition_Flag, .pred_1) %&gt;% \n  mutate(model = \"lightGBM\")\n\nlightgbm_pr &lt;- last_lightgbm_fit %&gt;% \n  collect_predictions() %&gt;% \n  pr_curve(Attrition_Flag, .pred_1) %&gt;% \n  mutate(model = \"lightGBM\")\n\n\n\nSolution 6.7 (Exercise 6.7). \n\ncols &lt;- c(\"#80003A\",\"#506432\",\"#FFC500\")\nnames(cols) &lt;- c(\"lgbm\", \"rf\", \"xgb\")\nplot_title &lt;- glue::glue(\n  \"ROC- and PR-Curve for a &lt;span style='color:{cols['rf']};'&gt;Random Forest&lt;/span&gt;,&lt;br&gt;\n   &lt;span style='color:{cols['xgb']};'&gt;XGBoost model&lt;/span&gt;,\n   and &lt;span style='color:{cols['lgbm']};'&gt;LightGBM model&lt;/span&gt;\"\n  )\np1 &lt;- bind_rows(rf_roc, xgb_roc, lightgbm_roc) %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + \n  geom_path(lwd = 1.5, alpha = 0.8) +\n  geom_abline(lty = 3) + \n  coord_equal() + \n  scale_color_manual(values = unname(cols))+\n  theme_minimal(base_size = 14)+\n  theme(legend.position = \"none\")\n\np2 &lt;- bind_rows(rf_pr, xgb_pr, lightgbm_pr) %&gt;% \n  ggplot(aes(x = recall, y = precision, col = model)) + \n  geom_path(lwd = 1.5, alpha = 0.8) +\n  coord_equal() + \n  scale_color_manual(values = unname(cols))+\n  theme_minimal(base_size = 14)+\n  theme(legend.position = \"none\")\n\n(p1|p2) +\n  plot_annotation(\n  title = plot_title,\n  theme = theme(plot.title = element_markdown()))\n\n\n\n\n\n\n\n\nSolution 6.8 (Exercise 6.8). \n\ncols &lt;- c(\"#80003A\",\"#506432\",\"#FFC500\")\nnames(cols) &lt;- c(\"lgbm\", \"rf\", \"xgb\")\n\ntitle_tib &lt;- tibble(\n  x=0,\n  y=1,\n  label = glue::glue(\n  \"&lt;p&gt;&lt;b&gt;Confusion matrices for a\n  &lt;span style='color:{cols['rf']};'&gt;random forest&lt;/span&gt;, &lt;br/&gt; \n  &lt;span style='color:{cols['xgb']};'&gt;XGBoost model&lt;/span&gt;,\n  and &lt;span style='color:{cols['lgbm']};'&gt;LightGBM model&lt;/span&gt;.&lt;/b&gt;&lt;/p&gt;\n  &lt;p&gt; Looking at the number of &lt;b&gt;True Positives &lt;/b&gt;(top left panel) &lt;br/&gt;\n  and &lt;b&gt;True Negatives&lt;/b&gt; (bottom right panel), it becomes &lt;br /&gt;\n  clear that the\n  &lt;span style='color:{cols['lgbm']};'&gt;LightGBM model&lt;/span&gt; performs best.&lt;br /&gt;\n  Additionally, the &lt;b&gt;True Positive rate&lt;/b&gt; (ratio of customers &lt;br /&gt;\n  that have been correctly identified to truely leave the bank)&lt;br /&gt;\n  is the highest, and the number of &lt;b&gt;False Positives&lt;/b&gt; &lt;br /&gt;\n  (top right panel) is the lowest for the \n  &lt;span style='color:{cols['lgbm']};'&gt;LightGBM model&lt;/span&gt;.&lt;/p&gt;\"\n    )\n)\n\ncm_plot &lt;- function(last_fit_model,high){ \n  cm &lt;- last_fit_model %&gt;%\n    collect_predictions() %&gt;%\n    conf_mat(Attrition_Flag, .pred_class)\n  \n  cm_tib &lt;- as_tibble(cm$table)%&gt;% mutate(\n    Prediction = factor(Prediction),\n    Truth = factor(Truth),\n    Prediction = factor(Prediction, \n                        levels = rev(levels(Prediction)))\n  )\n  \n  cm_tib %&gt;% ggplot(aes(x = Prediction, y = Truth,fill = n)) +\n    geom_tile( colour = \"gray50\")+\n    geom_text(aes(label = n))+\n    scale_fill_gradient(low = \"white\", high = high)+\n    theme_minimal()+\n    theme(legend.position = \"none\")\n}\n\n# Random Forest\ncm1&lt;- cm_plot(last_rf_fit,\"#506432\")\n\n# XGBoost\ncm2&lt;- cm_plot(last_xgb_fit,\"#FFC500\")\n\n# LightGBM\ncm3 &lt;- cm_plot(last_lightgbm_fit,\"#80003A\")\n\ntitle_pane &lt;- ggplot()+\n  geom_richtext(\n    data = title_tib,\n    aes(x, y, label = label),\n    hjust = 0, vjust = 1, \n    label.color = NA\n  ) +\n  xlim(0, 1) + ylim(0, 1)+\n  theme_void()\n\ncm1+cm2+cm3+title_pane+\nplot_layout(ncol =2, widths = c(1,1.04))\n\n\n\n\n\n\n\n\nSolution 6.9 (Exercise 6.9). According to the confusion matrices, ROC-, and PR-Curve the LightGBM model performs best.\n\nSensitivity: \\[\n  \\frac{\\mathrm{TP}}{\\mathrm{P}} = \\frac{374}{374+33} = 0.9189189\n\\]\nPrecision: \\[\n  \\frac{\\mathrm{TP}}{\\mathrm{PP}} = \\frac{374}{374+20} = 0.9492386\n\\]\nAccuracy: \\[\n\\frac{\\mathrm{TP}+\\mathrm{TN}}{\\mathrm{P}+\\mathrm{N}} = \\frac{374+2105}{374+2105+20+33} = 0.9790679\n      \\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Boosting</span>"
    ]
  },
  {
    "objectID": "07_MMc.html",
    "href": "07_MMc.html",
    "title": "7  Maximum Margin Classifier",
    "section": "",
    "text": "7.1 Introduction\nLinear maximum margin classifiers which are also known as linear support vector machines allow us to classify binary data using a geometric approach. In two dimensions, we can use a simple line that separates the classes (under the assumption that they are indeed separable) and additionally maximizes the margins between those classes. In higher dimensions, the line is replaced by a hyperplane.\nThe goal of this chapter is to gain an intuition about how such a line can be derived visually and analytically.\nThroughout the exercises, the following libraries are used for creating figures:\nlibrary(\"tidyverse\")\nlibrary(\"ggtext\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Maximum Margin Classifier</span>"
    ]
  },
  {
    "objectID": "07_MMc.html#exercises",
    "href": "07_MMc.html#exercises",
    "title": "7  Maximum Margin Classifier",
    "section": "7.2 Exercises",
    "text": "7.2 Exercises\n\nExercise 7.1 (Visual derivation) To solve the first exercise, you can either draw everything in a hand sketched figure, or create your own figures with the {ggplot} library. Consider the following data set comprised of ten data points and two classes with labels -1 and 1.\n\n\ndata &lt;- tibble(x = c(-1,-1,0,0,1,2,2,2.5,3,4),\n            y = c(-1,1,1,0,0,2.5,2,3,2.5,4),\n            label = factor(c(rep(1,5),rep(-1,5)))\n            )\n\n\nGenerate a scatter plot visualizing the data points and their respective classes. Hint: You can visualize the classes using colors or shapes.\nTo find the optimal separation line geometrically, it is often useful to consider the convex hull of the dataset. Start out by drawing the convex hull in the figure generated in Exercise 1.\n\n\n\n\n\n\nNoteA note on convex hulls\n\n\n\n\n\nRecall, that the convex hull of a set of points \\(X\\) is defined as the minimal convex set containing \\(X\\). To create a convex hull with ggplot, consider the following example:\nExample: Let \\(X = \\{(0,0)^\\top,(0.25,0.75)^\\top,(0.5,0.5)^\\top,(1,1)^\\top,(1,0)^\\top\\}\\).\n\ndata_example &lt;- tibble(x1 = c(0,0.25,0.5,1,1),\n                       x2 = c(0,0.75,0.5,0,1),\n                       label = factor(rep(1,5))\n                       )\np_example &lt;- data_example %&gt;% ggplot(aes(x=x1,y=x2)) +\n  geom_point(size = 2)+\n  theme_minimal()\np_example\n\n\n\n\n\n\n\n\nThen, the convex hull can be generated as follows:\n\nhull_example &lt;- data_example %&gt;%\n  group_by(label) %&gt;%\n  slice(chull(x1,x2))\n\np_example +\n  geom_polygon(data = hull_example,\n               aes(x=x1, y=x2,color = label, fill = label),\n               alpha = 0.3)\n\n\n\n\n\n\n\n\n\n\n\nThe line with the maximal margin is defined by the line with minimal distance between the points of the different classes. Find these two points on the convex hulls you have just drawn/plotted and label them with \\(c_1\\) and \\(c_{-1}\\) (for the classes with label \\(1\\) and \\(-1\\), respectively). Note that every point on a convex hull is a possible candidate, and they do not necessarily need to correspond with the data points.\nConnect the points \\(c_1\\) and \\(c_{-1}\\) with a line perpendicular to the points.\nThe separation line passes through the center of the line you have just drawn and is orthogonal to it, i.e., the two lines enclose a 90° angle. Draw/plot the separation line \\(s\\), the line \\(l_1\\) that passes through the support vectors of the class with label \\(1\\), and the line \\(l_{-1}\\) that passes through the support vector of the class with label \\(-1\\).\nAdd two arbitrary points from each class to the feature space, so that the separation line \\(s\\) found in the previous exercise does not change.\nStart fresh with the same data and add a new data point \\(x_5\\) that belongs to a class of your choice so that the new margin between the two classes is equal to \\(1\\). As before, sketch/plot the convex hull, the two points points \\(\\tilde c_1\\) and \\(\\tilde c_{-1}\\) on the convex hull, and the three lines \\(\\tilde l_1,\\tilde l_{-1}\\), and \\(\\tilde s\\) as in Exercise 1.-4.\n\n\n\nExercise 7.2 (Analytical derivation of a maximum margin classifier) From the previous exercise, we know what kind of separation line we can expect when using support vector machines and how to find it graphically. However, determining the separation line analytically is difficult, even for the simple problem of Exercise 7.1. The number of variables and conditions make this task impractical for a “Pen and Paper” exercise. However, to still get a basic idea of the linear SVM algorithm, we consider an even simpler problem with only two points \\(x_1,x_2\\in\\mathbb{R}^2\\) that each belongs to their own class. The two points are given by\n\\[\\begin{align*}\n  x_1 &= \\begin{pmatrix} 1\\\\1 \\end{pmatrix} \\in \\omega_1 = \\left\\{x_i:T_i = 1\\right\\},\\\\\n  x_2 &= \\begin{pmatrix} 2\\\\3 \\end{pmatrix} \\in \\omega_{-1} = \\left\\{x_i:T_i = -1\\right\\}.\n\\end{align*}\\]\nSince both points are the single representative of their respective classes, they are also the support vectors. Furthermore, they are located on the margin. The goal of this exercise is to find the parameters of a separation line with maximal margin.\nRecall from the lecture, that the dual problem is given by\n\\[\\begin{equation*}\n  L_D = \\sum_{i=1}^{2} \\alpha_i - \\frac{1}{2}\\sum_{i=1}^{2}\\sum_{j=1}^{2}\\alpha_i\\alpha_jT_iT_jx_i^\\top x_j\n\\end{equation*}\\]\nsubject to the constraint\n\\[\n\\sum_{i=1}^{2} \\alpha_iT_i = 0.\n\\]\nTechnically, we also need the constraint \\(\\alpha_i \\geq 0,\\, \\forall i\\). However, to keep things simple, we assume that this is satisfied here.\n\nSet up the Lagrange function by plugging all the values into \\(L_D\\) and the constraint above. Subsequently, simplify the terms.\nTo maximize the term \\(L_D\\) under the constraint \\[ \\sum_{i=1}^{2} \\alpha_iT_i = 0, \\] we need an additional Lagrange function with Lagrange multiplier \\(\\lambda\\)\n\\[\\begin{equation*}\n  \\Lambda (\\alpha_1,\\alpha_2,\\lambda) = L_D + \\lambda \\left(\\sum_{i=1}^2 \\alpha_i T_i\\right).\n\\end{equation*}\\]\nMaximize this function and show that the optimal values are given by\n\\[\\begin{equation*}\n  \\alpha_1^*  = \\frac{2}{5} \\quad \\mathrm{and}\\quad \\alpha_2^* = \\frac{2}{5}.\n\\end{equation*}\\]\nIt is sufficient to only calculate the potential extrema since we will also assume for them to be actual extrema.\nBased on the previous results, calculate the line parameters \\(w_1, w_2\\), and $ b$.\nAdd the line to the figure below.\n\ndata_ex02 &lt;- tibble(x1= c(1,2), x2 = c(1,3), \"T\" = factor(c(1,-1)))\n\ndata_ex02 %&gt;% ggplot(aes(x=x1,y=x2, color = T)) +\n  geom_point(size = 3) +\n  xlim(1,2) +\n  ylim(1,3) +\n  labs(\n  color = \"Labels\"\n  )+\n  theme_minimal()\n\n\n\n\n\n\n\n\nSuppose we want to classify a new point \\(x_3 = (2,2)^\\top\\). Assign this point to the correct class, both visually and using the decision function \\[\n  f(\\tilde x) = \\mathrm{sign}\\left(\\sum_{i=1}^{2} \\alpha_i^*T_i(x_i^\\top \\tilde x+b)\\right).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Maximum Margin Classifier</span>"
    ]
  },
  {
    "objectID": "07_MMc.html#solutions",
    "href": "07_MMc.html#solutions",
    "title": "7  Maximum Margin Classifier",
    "section": "7.3 Solutions",
    "text": "7.3 Solutions\n\nSolution 7.1 (Exercise 7.1). \n\n\n\ncols &lt;- c(\"1\" = \"darkblue\", \"-1\" = \"darkorange\")\n\ntitle_text &lt;- glue::glue(\n\"Classes \n &lt;span style='color:{cols['1']};'&gt;\n  &omega;&lt;sub&gt;1&lt;/sub&gt; (\\u25CF)\n &lt;/span&gt;\n and \n &lt;span style='color:{cols['-1']};'&gt;\n   &omega;&lt;sub&gt;-1&lt;/sub&gt;(\\u25A0)\n &lt;/span&gt; \n\")\n\np &lt;- ggplot() +\n  geom_point(data = data,\n             aes(x=x,y=y,color = label, shape = label),\n             size = 2)+\n  scale_color_manual(values = cols) +\n  scale_shape_manual(values = c(15, 16)) +\n  labs(\n    title = title_text\n  )+\n  theme_minimal()+\n  theme(\n    plot.title = element_markdown(),\n    legend.position = \"None\"\n  )+\n  coord_fixed()\np\n\n\n\n\n\n\n\n\nhull &lt;- data %&gt;% group_by(label) %&gt;% slice(chull(x,y))\np1 &lt;- p +\n    geom_polygon(data = hull,\n                 aes(x=x, y=y,color = label, fill = label),\n                 alpha = 0.3)+\n    scale_fill_manual(values = cols)\np1\n\n\n\n\n\n\n\n\ndf_annotate &lt;- tibble(\n  label = c(\n    \"&lt;span style='color:red;'&gt; c&lt;sub&gt;1&lt;/sub&gt;&lt;/span&gt;\",\n    \"&lt;span style='color:red;'&gt; c&lt;sub&gt;-1&lt;/sub&gt;&lt;/span&gt;\"\n  ),\n  x = c(0.5,2),\n  y = c(0.5,2),\n  hjust = c(-0.3, 1.2)\n)\ndf_c &lt;- tibble(x = c(0.5,2),\n             y = c(0.5,2))\n\np2 &lt;- p1 + geom_point(data = df_c,\n                aes(x=x,y=y),\n                shape = c(8,8),\n                size = 3,\n                color = c(\"red\",\"red\"))+\n  geom_richtext(data = df_annotate,\n                aes(x=x, y=y, label=label, hjust = hjust),\n                fill = NA,\n                label.color = NA)\np2\n\n\n\n\n\n\n\n\ndf_line &lt;- tibble(x1 = 0.5, x2 = 2, y1 = 0.5, y2 = 2)\np3 &lt;- p2 + geom_segment(data = df_line,\n                   aes(x = x1, y = y1, xend = x2, yend = y2),\n                   color = \"red\")\np3\n\n\n\n\n\n\n\n\ndf_annotate_l &lt;- tibble(\n  label = c(\n    \"&lt;span style='color:grey50;'&gt; l&lt;sub&gt;1&lt;/sub&gt;&lt;/span&gt;\",\n    \"&lt;span style='color:grey50;'&gt; l&lt;sub&gt;-1&lt;/sub&gt;&lt;/span&gt;\",\n    \"&lt;span style='color:grey50;'&gt; s&lt;/span&gt;\"\n  ),\n  x = c(-0.75,3.5,0.25),\n  y = c(1.5,1,2.5)\n)\n\np4 &lt;- p3 + geom_abline(slope = -1,\n                       intercept = 2.5,\n                       color = \"grey50\")+\n           geom_abline(slope = -1,\n                       intercept = 1,\n                       color = \"grey50\",\n                       linetype = 2)+\n           geom_abline(slope = -1,\n                       intercept = 4,\n                       color = \"grey50\",\n                       linetype = 2)+\n           geom_richtext(data = df_annotate_l,\n                aes(x=x, y=y, label=label),\n                fill = NA,\n                label.color = NA)\np4\n\n\n\n\n\n\n\n\ntitle_text &lt;- glue::glue(\n\"Classes\n  &lt;span style='color:{cols['1']};'&gt;\n    &omega;&lt;sub&gt;1&lt;/sub&gt; (\\u25CF)\n  &lt;/span&gt;\nand \n  &lt;span style='color:{cols['-1']};'&gt;\n    &omega;&lt;sub&gt;-1&lt;/sub&gt;(\\u25A0)\n  &lt;/span&gt;,\n&lt;/br&gt; and additional points \n  &lt;span style='color:{cols['1']};'&gt; \n    x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;(\\u25B2)\n  &lt;/span&gt;\nand\n  &lt;span style='color:{cols['-1']};'&gt;\n    x&lt;sub&gt;3&lt;/sub&gt;,x&lt;sub&gt;4&lt;/sub&gt;(\\u25C6)\n  &lt;/span&gt;\"\n)\ndf_x &lt;- tibble(x = c(-0.5,0,2.5,3),\n               y = c(-0.5,0.5,2.5,3),\n               label =factor(c(1,1,-1,-1)))\np5 &lt;- p4 + geom_point(data = df_x,\n                      aes(x=x,y=y, color = label),\n                      shape = c(17,17,18,18),\n                      size = 3) +\n  labs(\n    title = title_text\n  )+\n  theme_minimal(base_size = 11)+\n  theme(\n    plot.title = element_markdown(),\n    legend.position = \"None\"\n  )\np5\n\n\n\n\n\n\n\n\ntitle_text &lt;- glue::glue(\n\"Classes\n&lt;span style='color:{cols['1']};'&gt;\n  &omega;&lt;sub&gt;1&lt;/sub&gt; (\\u25CF)\n&lt;/span&gt;\nand\n&lt;span style='color:{cols['-1']};'&gt;\n  &omega;&lt;sub&gt;-1&lt;/sub&gt;(\\u25A0)\n&lt;/span&gt;.\"\n)\n\n#Define new Data and Hull\ndata_new &lt;- rbind(data,c(1,2,1))\nhull_new &lt;- data_new %&gt;% group_by(label) %&gt;% slice(chull(x,y))\n\n#New Annotations\n## Annotate c\ndf_annotate_new &lt;- tibble(\n  label = c(\n    \"&lt;span style='color:red;'&gt; c&lt;sup&gt;~&lt;/sup&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;/span&gt;\",\n    \"&lt;span style='color:red;'&gt; c&lt;sup&gt;~&lt;/sup&gt;&lt;sub&gt;-1&lt;/sub&gt;&lt;/span&gt;\"\n  ),\n  x = c(0.5,2),\n  y = c(2,2),\n  hjust = c(-0.3, 1.2)\n)\ndf_c_new &lt;- tibble(x = c(1,2),\n             y = c(2,2))\n## Annotate l and s\ndf_annotate_l_new &lt;- tibble(\n  label = c(\n    \"&lt;span style='color:grey50;'&gt; l&lt;sup&gt;~&lt;/sup&gt;&lt;sub&gt;-1&lt;/sub&gt;&lt;/span&gt;\",\n    \"&lt;span style='color:grey50;'&gt; l&lt;sup&gt;~&lt;/sup&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;/span&gt;\",\n    \"&lt;span style='color:grey50;'&gt; s&lt;sup&gt;~&lt;/sup&gt;&lt;/span&gt;\"\n  ),\n  x = c(2.25,0.75,1.4),\n  y = c(3,3,3)\n)\n\n# Generate new plot\np_new &lt;- ggplot() +\n  geom_point(data = data_new,\n             aes(x=x,y=y,color = label, shape = label),\n             size = 2)+\n  geom_polygon(data = hull_new,\n               aes(x=x, y=y,color = label, fill = label),\n               alpha = 0.3)+\n  geom_point(data = df_c_new,\n                aes(x=x,y=y),\n                shape = c(8,8),\n                size = 3,\n                color = c(\"red\",\"red\"))+\n  geom_richtext(data = df_annotate_new,\n                aes(x=x, y=y, label=label, hjust = hjust),\n                fill = NA,\n                label.color = NA)+\n  geom_vline(xintercept = 1.5, color = \"grey50\")+\n  geom_vline(xintercept = 1, color = \"grey50\", linetype = 2)+\n  geom_vline(xintercept = 2, color = \"grey50\", linetype = 2)+\n  geom_richtext(data = df_annotate_l_new,\n       aes(x=x, y=y, label=label),\n       fill = NA,\n       label.color = NA)+\n  scale_fill_manual(values = cols)+\n  scale_color_manual(values = cols) +\n  scale_shape_manual(values = c(15, 16)) +\n  labs(\n    title = title_text\n  )+\n  theme_minimal()+\n  theme(\n    plot.title = element_markdown(),\n    legend.position = \"None\"\n  )+\n  coord_fixed()\n\np_new\n\n\n\n\n\n\n\n\n\nSolution 7.2 (Exercise 7.2). \n\nSetting up the Lagrange function:\n\\[\\begin{align*}\n  L_D &= (\\alpha_1+\\alpha_2) - \\frac{1}{2}(\n      \\alpha_1^2\\cdot 1^2\\cdot 2 +\n      \\alpha_1\\cdot\\alpha_2\\cdot1\\cdot-1\\cdot 5 +\n      \\alpha_2\\cdot\\alpha_1\\cdot-1\\cdot1\\cdot5 +\n      \\alpha_2^2\\cdot(-1)^2\\cdot 13)\\\\\n      &= (\\alpha_1+\\alpha_2) - \\frac{1}{2}(2\\cdot\\alpha_1^2-2\\cdot 5\\cdot \\alpha_1\\cdot \\alpha_2+13\\cdot\\alpha_2^2)\\\\\n      &= \\alpha_1+\\alpha_2 - \\alpha_1^2+ 5\\cdot \\alpha_1 \\cdot\\alpha_2 - \\frac{13}{2}\\alpha_2^2\n\\end{align*}\\]\nSetting up the constraint\n\\[\n\\alpha_1-\\alpha_2 = 0\n\\]\nIn order to optimize \\[\n\\Lambda(\\alpha_1,\\alpha_2,\\lambda) = \\alpha_1+\\alpha_2 - \\alpha_1^2+ 5\\cdot \\alpha_1 \\cdot\\alpha_2 - \\frac{13}{2}\\cdot\\alpha_2^2 +\\lambda\\cdot(\\alpha_1-\\alpha_2),\n\\] first calculate the partial derivatives with respect to \\(\\alpha_1,\\alpha_2,\\lambda\\):\n\\[\\begin{align}\n\\frac{\\partial \\Lambda}{\\partial\\alpha_1} &= 1-2\\cdot\\alpha_1+5\\cdot\\alpha_2 +\\lambda \\tag{1}\\\\\n\\frac{\\partial \\Lambda}{\\partial\\alpha_2} &= 1+5\\cdot\\alpha_1- 13\\cdot\\alpha_2 -\\lambda \\tag{2}\\\\\n\\frac{\\partial \\Lambda}{\\partial\\lambda} &= \\alpha_1-\\alpha_2 \\tag{3}\n\\end{align}\\]\nIn order to obtain an extrema, we have to set each of the equations above to \\(0\\) and solve them for \\(\\alpha_1\\) and \\(\\alpha_2\\).\nAdding the terms \\((1)\\) and \\((2)\\), we obtain\n\\[\\begin{align*}\n&1-2\\cdot\\alpha_1+5\\cdot\\alpha_2 +\\lambda +1+5\\cdot\\alpha_1- 13\\cdot\\alpha_2 -\\lambda &&= 0\\\\\n\\iff &2+3\\cdot\\alpha_1-8\\alpha_2 &&=0 \\\\\n\\iff &\\alpha_1 &&=\\frac{8\\cdot\\alpha_2-2}{3}\n\\end{align*}\\]\nPlugging \\(\\alpha_1\\) into \\((3)\\) then yields\n\\[\\begin{align*}\n  &\\frac{8\\cdot\\alpha_2-2}{3} - \\alpha_2 &&= 0\\\\\n  \\iff &\\frac{5\\cdot\\alpha_2-2}{3}&&= 0\\\\\n  \\iff &\\alpha_2 &&= \\frac{2}{5}\n\\end{align*}\\]\nSince we also need to satisfy \\(\\alpha_1 - \\alpha_2 = 0\\) and thus \\(\\alpha_1 = \\alpha_2\\), we can deduce \\(\\alpha_1 = \\frac{2}{5}\\).\nTherefore \\(\\alpha_1^* = \\alpha_2^* = \\frac{2}{5}\\).\nThe first order conditions require\n\\[w = \\sum_{i=1}^{2}\\alpha_i\\cdot T_i x_i, \\]\ni.e., \\[\n\\begin{pmatrix} w_1\\\\w_2 \\end{pmatrix} =\n\\alpha_1^*\\cdot T_1 \\cdot x_1 + \\alpha_2^*\\cdot T_2 \\cdot x_2 =\n\\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\end{pmatrix} - \\begin{pmatrix} \\frac{4}{5} \\\\ \\frac{6}{5} \\end{pmatrix} =\n-\\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{4}{5} \\end{pmatrix}\n\\] Calculating \\(b\\) using the Karush–Kuhn–Tucker conditions yields\n\\[\n\\alpha_i\\left(T_i\\left(x_i^\\top w + b\\right) -1 \\right) = 0 \\iff b = \\frac{1}{T_i} - x_i^\\top w\n\\] Plugging in the values we obtained for \\(w\\):\n\\[\nb =\n\\frac{1}{1}-\\begin{pmatrix} 1\\\\ 1\\end{pmatrix}^\\top \\begin{pmatrix} -\\frac{2}{5} \\\\ -\\frac{4}{5} \\end{pmatrix} =\n\\frac{1}{-1}-\\begin{pmatrix} 2\\\\ 3\\end{pmatrix}^\\top \\begin{pmatrix} -\\frac{2}{5} \\\\ -\\frac{4}{5} \\end{pmatrix} =\n\\frac{11}{5}\n\\] The separation line is therefore give by\n\\[\nw_1 \\cdot x_1 + w_2 \\cdot x_2 + b = 0 \\iff x_2 = \\frac{-b-w_1\\cdot x_1}{w_2} = \\frac{-\\frac{11}{5}+\\frac{2}{5}\\cdot x_1}{-\\frac{4}{5}} = \\frac{11}{4}-\\frac{1}{2}x_1\n\\]\n\n\ndata_ex02 %&gt;% ggplot(aes(x=x1,y=x2, color = T)) +\n  geom_point(size = 3) +\n  geom_abline( slope = -0.5, intercept = 11/4) +\n  xlim(1,2) +\n  ylim(1,3) +\n  labs(color = \"Labels\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\ndata_ex02 %&gt;% ggplot() +\n  geom_point(aes(x=x1,y=x2, color = T), size = 3) +\n  geom_abline( slope = -0.5, intercept = 11/4) +\n  geom_point(data = tibble(x=2,y=2), aes(x=x,y=y), size = 3, shape = 17)+ \n  xlim(0,4) +\n  ylim(0,4) +\n  labs(color = \"Labels\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nSince the point \\((2,2)\\) is above the decision line, it belongs to class \\(\\omega_{-1}\\).\nPlugging all the values into the decision function yields:\n\\[\\begin{align*}\nf\\begin{pmatrix}2\\\\2\\end{pmatrix} &= \\mathrm{sign}\\left(\\frac{2}{5} \\cdot 1 \\cdot \\begin{pmatrix}1\\\\1\\end{pmatrix}^\\top \\cdot  \\begin{pmatrix}2\\\\2\\end{pmatrix} +\\frac{11}{5} + \\frac{2}{5}\\cdot -1 \\left(\\begin{pmatrix}2\\\\3\\end{pmatrix}^\\top \\cdot  \\begin{pmatrix}2\\\\2\\end{pmatrix} + \\frac{11}{5}\\right)\\right)\\\\\n&= \\mathrm{sign}\\left(-\\frac{12}{5}\\right)\\\\\n&= -1.\n\\end{align*}\\] Therefore, the new point belongs to class \\(\\omega_{-1}\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Maximum Margin Classifier</span>"
    ]
  },
  {
    "objectID": "08_SVM.html",
    "href": "08_SVM.html",
    "title": "8  Support Vector Machines and Stacking",
    "section": "",
    "text": "8.1 Introduction\nIn this exercise session, we discuss how to train different support vector machines (SVMs) and create a stack in R.\nSVMs and stacks are available in the {tidymodels} framework which speeds setting up a training and testing routine significantly.\nWe will shortly discuss how to train and tune SVMs on a toy data set in R, before training a stack on the same data.\nLibraries used throughout the session:\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(finetune)\nlibrary(patchwork)\nlibrary(ggtext)\nlibrary(stacks)\nlibrary(kernlab)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Support Vector Machines and Stacking</span>"
    ]
  },
  {
    "objectID": "08_SVM.html#introduction",
    "href": "08_SVM.html#introduction",
    "title": "8  Support Vector Machines and Stacking",
    "section": "",
    "text": "8.1.1 Support Vector Machines\nRecall, solving the the dual problem\n\\[\\begin{equation}\nL_D = \\sum_{i=1}^{n} \\alpha_i-\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^\\top x_j, \\quad \\text{s.t.} \\quad \\alpha_i \\geq 0\\text{ for all } i\n\\end{equation}\\]\nyields a linear SVM.\nInstead of using a linear SVM enrich our feature space by replacing the term \\(x_i^\\top x_j\\) with a function \\(K:\\mathbb{R}^k\\times\\mathbb{R}^k\\to \\mathbb{R}\\). For polynomials of degree \\(d\\geq 2\\), we apply the kernel function \\(K(x_i,x_j)= (1+x_i^\\top x_j)^d\\). Another popular choice we will consider is the so-called radial basis kernel (RBF) given by\n\\[\\begin{equation}\n  K(x_i,x_j)= \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{c}\\right)^d, \\quad c\\geq 0.\n\\end{equation}\\]\nThe {tidymodels} frameworks contains three different SVM model specifications, namely\n\nsvm_linear, which specifies a simple linear SVM,\nsvm_poly, which specifies a polynomial SVM (including polynomials of degree 1), and\nsvm_rbf, which specifies an SVM with RBF Kernel.\n\nConsider the following toy data set:\n\nset.seed(121)\ndata_toy &lt;- tibble(\n  x1 = c(\n    rnorm(25,0,0.5),\n    rnorm(25,1,0.5)\n  ),\n  x2 = c(\n    rnorm(25,0,0.5),\n    rnorm(25,1,0.5)\n  ),\n  y = factor(rep(c(1,-1),each = 25))\n)\n\nThe two class data contains a two dimensional dataset where both axes are generated using a normal distribution. The samples with label 1 are drawn from a \\(\\mathcal{N}((0,0)^\\top, 0.5 \\cdot I D_2)\\) distribution, while the samples with label -1 are drawn from a \\(\\mathcal{N}((1,1)^\\top, 0.5 \\cdot I D_2)\\) distribution.\n\ndata_toy %&gt;% ggplot(aes(x=x1,y=x2, color = y)) +\n  geom_point(size = 2)+\n  labs(color = \"Label\")+\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nWithout creating a workflow or recipe, we now want to train three different SVMs.\nUsing the svm_poly function, we can specify a linear SVM for the toy data by setting the degree to 1 and mode to \"classification\". Setting the engine to \"kernlab\" and applying the fit function trains the linear SVM.\n\nsvm_lin_res &lt;- svm_poly(\n  mode = \"classification\",\n  degree = 1\n) %&gt;%\n  set_engine(\"kernlab\") %&gt;%\n  fit(y ~ ., data_toy)\n\nTo plot the decision surfaces, we need to extract the fit engine and pass it to the kernlabs::plot function:\n\nsvm_lin_res %&gt;%\n  extract_fit_engine() %&gt;%\n  plot(data = data_toy) \n\n\n\n\n\n\n\n\nThe plot depicts the contour plot of the decision surface of the linear SVM. The boundary between the two classes (where the decision function equals zero) is represented by the transition area in the middle of the plot, where the colors shift from blue to red. The greyed-out area indicates a lower confidence for the decision, meaning that the algorithm is not sure whether the values belong to class \\(-1\\) or \\(1\\). Any point on the blue shape is classified as a member of the class with label 1, whereas any point located on the red shade is classified as a member of the class with label -1. The filled shapes denote support vectors, i.e., vectors that influenced the decision surfaces, while shapes that are only outlined do not represent support vectors. △ indicate samples with class label -1 and ∘ indicate samples with class label 1.\nInstead of using the kernlab::plot() function, you can also visualize the decision surface using ggplot.\nHowever, as it turns out, this is relatively complicated. In case you are still interested, the following snippet generates the decision surface using ggplot.\n\n\nCode\nlibrary(ggnewscale)\n\nsupp_vec &lt;- svm_poly(\n  mode = \"classification\",\n  degree = 1\n) %&gt;%\n  set_engine(\"kernlab\", scaled = c(F,F,T,T)) %&gt;%\n  fit(y ~ ., data_toy) %&gt;%\n  extract_fit_engine() %&gt;%\n  xmatrix() %&gt;%\n  pluck(1) %&gt;%\n  as_tibble()\n\n\ndata_toy &lt;- data_toy %&gt;%\n  mutate(supp_vec = data_toy$x1 %in% supp_vec$x1)\n\nx_grid &lt;- expand_grid(x1 = seq(-2,2,length.out = 100),\n                      x2 = seq(-2,2,length.out = 100)) %&gt;%\n  bind_cols(predict(svm_lin_res,.,type=\"prob\")) %&gt;%\n  mutate(val = (`.pred_-1`*2)-1) \n\n#transform probabilities to take values between -1,1\n\n\nx_grid %&gt;% ggplot(aes(x=x1,y=x2))+\n  geom_raster(aes(fill= val), interpolate = TRUE)+\n  geom_point(data = data_toy %&gt;% filter(supp_vec==F),\n             aes(shape = y),\n             cex = 3,\n             show.legend=FALSE) +\n  scale_shape_manual(values = c(\"-1\" = 1,\"1\" = 2))+\n  new_scale(\"shape\")+\n  geom_point(data = data_toy %&gt;% filter(supp_vec==T),\n             aes(shape = y),\n             cex = 3) +\n  labs(\n    shape = \"Support Vectors\"\n  )+\n  coord_fixed(expand = FALSE)+\n  scale_fill_gradient2(high = \"dodgerblue4\",\n                       midpoint = 0,\n                       low = \"firebrick2\")+\n  labs(fill = \"Decision Value\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n8.1.2 Stacking\nThe idea behind linear stacking is relatively simple: Assume there are \\(M\\) different models \\(\\hat{f}_1,...,\\hat{f}_M\\), all fitted on the same training data. Then, a target can be predicted by taking a weighted average of all \\(M\\) models, i.e., \\[\\begin{equation}\n  \\hat{y} = \\sum_{m=1}^{M}\\omega_m\\hat{f}_m(x),\n\\end{equation}\\]\nwhere \\(\\omega_1,...,\\omega_M\\in [0,1]\\) such that \\(\\sum_{m=1}^{M}\\omega_m = 1\\).\nTo find out which \\(\\{\\omega_m\\}_{m=1}^{M}\\) yield the best results, we can solve the optimization problem minimizing the loss function\n\\[\\begin{equation}\n\\underset{\\omega_1,...,\\omega_M}{\\min} L(y,\\hat{y}) = \\underset{\\omega_1,...,\\omega_M}{\\min} \\left\\{\\frac{1}{n}\\sum_{i=1}^n \\left(y_i-\\sum_{m=1}^M \\omega_m \\hat{f}_m(x_i)\\right)^2\\right\\}.\n\\end{equation}\\]\nThe modeling process for a stack with {tidymodels} can be described as follows:\n\nDefine candidates for the stack by fitting (tuned) models to the training data by using the control_stack_grid function in the tune or model specification.\nInitialize a stack object using the stacks function.\nAdd candidate models to the stack using the add_candidates function.\nPass the stack object to the blend_predictions function, which specifies how the predictions of each candidate are evaluated.\nFit the candidate ensemble with non-zero stacking coefficients using the fit_members function.\nPredict on test data using the predict function to evaluate out-of-sample performance.\n\nConsider the following toy example data which is created in a similiar fashion as the data set from the previous section.\n\nset.seed(121)\n\ndata_toy_big &lt;- tibble(\n  x1 = c(\n    rnorm(500,0,0.5),\n    rnorm(500,1,0.5)\n  ),\n  x2 = c(\n    rnorm(500,0,0.5),\n    rnorm(500,1,0.5)\n  ),\n  y = factor(rep(c(1,-1),each = 500))\n)\n\nConsider the following figure, displaying the extended data set:\n\ndata_toy_big %&gt;% ggplot(aes(x=x1,y=x2, color = y)) +\n  geom_point(size = 2)+\n  labs(color = \"Label\")+\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nConsidering the figure above, it is quickly becomes evident that the classes are not linearly seperable.\nWe, therefore, now want to tune and fit different classification models and combine them to a stack predict the class labels as good as possible.\n\nFirst, set up the control grid:\n\nctrl_grid &lt;- control_stack_grid()\n\nThen, create a data split and tune the candidate models. For our simple example, we will tune a random forest and XGBoost classifier using a simple training, validation and test split. The candidate models for the stack are then given by the models fitted with different hyperparameters. First, we create the data split:\n\nset.seed(121)\nsplit_toy &lt;- initial_split(data_toy_big,prop = 4/5)\ndata_train &lt;- training(split_toy)\ndata_val &lt;- validation_split(data_train, prop = 4/5)\ndata_test &lt;- testing(split_toy)\n\nThen, we train several random forests by tuning the hyper parameters min_n and trees :\n\nset.seed(121)\nrf_model_spec &lt;- rand_forest(\n  mode = \"classification\",\n  mtry = 1,\n  min_n = tune(),\n  trees = tune()\n) %&gt;%\n  set_engine(\"ranger\")\n\nrec_toy&lt;- recipe(\n  y~., data = data_train\n)\n\nwf_toy &lt;- workflow() %&gt;%\n  add_model(rf_model_spec) %&gt;%\n  add_recipe(rec_toy)\n\nrf_tune_res &lt;- wf_toy %&gt;% \n  tune_grid(grid = 10,\n            resamples = data_val,\n            control = ctrl_grid\n  )\n\nThen, we train a XGBoost model in the same fashion:\n\nset.seed(121)\nxgb_model_spec &lt;- boost_tree(\n  trees = 1000,\n  tree_depth = tune(),\n  min_n = tune(),\n  mtry = 1,         \n  loss_reduction = tune(),                     \n  learn_rate = tune()                          \n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\nwf_toy &lt;- wf_toy %&gt;% update_model(xgb_model_spec)\n\nxgb_tune_res &lt;- wf_toy %&gt;% tune_grid(\n  resamples = data_val,\n  grid = 20,\n  control = ctrl_grid\n)\n\nThe stack candidates can be added with the add_candidates() function:\n\nstack_toy &lt;- stacks() %&gt;%\n  # add candidate members\n  add_candidates(rf_tune_res) %&gt;%\n  add_candidates(xgb_tune_res)\n\nBy using the blend_predictions function, we can specify, how the predictions should be blended together. The options we are interested in are the following:\n\nmixture: A number between zero and one (inclusive) specifying the proportion of L1 regularization (i.e. lasso) in the model. mixture = 1 indicates a pure lasso model, mixture = 0 indicates ridge regression, and values in the open interval (0,1) specify an elastic net.\npenalty: A vector containing penalty values for the amount of regularization used in member weighting. If more than one value is contained in the vector, the library will tune over the candidte values to select the best penalty.\nmetric: The metric(s) to use in tuning the regularization penalty on the stacking coefficients.\ntimes: Number of bootstrap samples tuned over by the model that determines stacking coefficients.\n\n\nset.seed(121)\nstack_toy &lt;- stack_toy %&gt;% blend_predictions(\n   mixture = 0.9,\n   penalty = 10^(-6:-1),\n   metric = metric_set(pr_auc),\n   times = 20\n )\n\nFinally, we can fit the stack by passing the previously specified stack object that contains the candidates and blend specifications to the fit_members() function.\n\nset.seed(121)\nstack_toy &lt;- stack_toy %&gt;%\n fit_members()\n\nWe want to evaluate our stack and in order to do so, we first need to create predictions from our test dataset. The following code snippet creates predictions based on the test set and binds them to the test data columns.\n\nstack_pred &lt;-\n  data_test %&gt;%\n  bind_cols(predict(stack_toy, .))\n\n\nTo evaluate the stack results, we can use the typical metrics provided in the {yardstick} library. For example, the accuracy of our stacked model is around \\(92\\%\\).\n\nstack_pred %&gt;% accuracy(truth = y, .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary          0.92\n\n\nNote, to obtain probabilities for the predictions the argument type = \"prob\" has to be passed in the predict function.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Support Vector Machines and Stacking</span>"
    ]
  },
  {
    "objectID": "08_SVM.html#exercises",
    "href": "08_SVM.html#exercises",
    "title": "8  Support Vector Machines and Stacking",
    "section": "8.2 Exercises",
    "text": "8.2 Exercises\nThe goal of this exercise is to create an SVM model with an RBF kernel that serves as a classifier for an underlying dataset.\nThe dataset we will consider in this exercise will be the Credit Card Customers data set that we already used in previous exercises. You can either download it again using the provided link or the button below.\n\nDownload BankChurners\n\nRecall that the data set consists of 10,127 entries that represent individual customers of a bank including but not limited to their age, salary, credit card limit, and credit card category.\nThe goal is to find out whether a customer will stay or leave the bank given the above features.\nThe following training, validation and test split should be used for training the models of the subsequent exercises.\n\ncredit_info &lt;- read.csv(\"data/BankChurners.csv\")\n\nset.seed(121)\nsplit &lt;- initial_split(credit_info, strata = Attrition_Flag)\ndata_train_ci &lt;- training(split)\ndata_val_ci &lt;- validation_split(data_train_ci)\ndata_test_ci &lt;- testing(split)\n\nPreprocessing of the data is handled by the following recipe.\n\nlevels_income &lt;- c(\"Less than $40K\",\"$40K - $60K\",\n                   \"$60K - $80K\",\"$80K - $120K\",\"$120K +\")\n\nlevels_education &lt;- c(\"Uneducated\", \"High School\",\"College\",\n                      \"Graduate\",  \"Post-Graduate\", \"Doctorate\")\n\nrec_ci &lt;- recipe(Attrition_Flag ~., data = data_train_ci) %&gt;%\n  update_role(CLIENTNUM, new_role = \"ID\") %&gt;%\n  step_mutate_at(all_nominal_predictors(),\n               fn = ~if_else(.%in% c(\"Unknown\",\"unknown\"),NA,.)\n  )%&gt;%\n  step_string2factor(Income_Category,\n                     levels = levels_income,\n                     ordered = TRUE) %&gt;%\n  step_string2factor(Education_Level,\n                     levels = levels_education,\n                     ordered = TRUE) %&gt;%\n  step_ordinalscore(all_ordered_predictors()) %&gt;%\n  step_unknown(all_factor_predictors()) %&gt;%\n  step_impute_knn(all_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_corr(all_predictors())\n\nci_wf &lt;- workflow() %&gt;%\n  add_recipe(rec_ci) \n\nNote, that compared to the previous session, we did not change the values of the target variable Attrition_Flag. By changing the targets name in the recipe, we potentially run into problems when fitting the stack. Therefore, we change the labels once all the predictions have been generated to obtain an easier to understand confusion matrix.\n\nExercise 8.1 In this exercise, we want to tune and evaluate an SVM. The hyperparameter cost, i.e. the parameter \\(D\\) in the maximum margin formulation\n\\[\\begin{align*}\n  &\\max_{\\omega,b,\\|\\omega\\|\\leq 1} D\\quad \\text{ s.t. }\\quad y_i(x_i^\\top\\omega + b)\\geq D,\\quad \\forall i=1,...,n.\n\\end{align*}\\]\nis the only parameter we wish to tune.\nRecall, that the cost parameter penalizes samples that are predicted to be in the wrong class. A larger cost will thus lead to a more flexible model with fewer misclassifications (Bias-Variance tradeoff).\n\nCreate and tune a SVM model with RBF kernel, where the cost parameter is set to tune. Candidates for the optimal cost parameter can be created by passing grid = 20 in the tune_grid() function.\nEvaluate the performance on the test set by plotting a ROC and PR Curve.\nCreate a confusion matrix for the test data and calculate the accuracy, precision and recall.\n\n\n\nExercise 8.2 In this last exercise, we want to create a stack model to predict the target variable Attrition_Flag.\nThe potential stack candidates are the XGBoost and random forest tuning results of Exercise 06.\n\nCreate a control stack grid that can be added to the tuning specifications.\nRetune the XGBoost and random forest model and add the candidates to a stack. As grid size for the hyper parameters, choose 30.\nAdd the candidate models to a stack and blend the predictions. To blend the predictions, use a pure lasso approach and tune the penalty with a grid given by the vector 10^(-6:-2). Use the metric accuracy to select the best model coefficients.\nFinally, fit the stack on the whole training data and evaluate it on the test data by generating a confusion matrix and calculating the sensitivity, precision, and accuracy. Before generating the confusion matrix, change the labels of the target feature and prediction to \"Negative\" if the customer is (predicted to be) an existing customer and to \"Positive\" if the customer is (predicted to be) an attrited customer. Is the stacked model better than the LightGBM model that was presented in Session 06?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Support Vector Machines and Stacking</span>"
    ]
  },
  {
    "objectID": "08_SVM.html#solutions",
    "href": "08_SVM.html#solutions",
    "title": "8  Support Vector Machines and Stacking",
    "section": "8.3 Solutions",
    "text": "8.3 Solutions\n\nSolution 8.1 (Exercise 8.1). \n\nsvm_model_rbf_tune &lt;- svm_rbf(\n  mode = \"classification\",\n  cost = tune()\n  ) %&gt;%\n  set_engine(\"kernlab\")\n\nci_wf &lt;- ci_wf %&gt;% add_model(svm_model_rbf_tune)\n\n\nsvm_rbf_tune_res &lt;- ci_wf %&gt;% \n  tune_grid(\n    resamples = data_val_ci,\n    grid = 20\n  )\n\n\nsvm_tune_best &lt;- svm_rbf_tune_res %&gt;% select_best(metric = \"roc_auc\")\n\nlast_rbf_fit &lt;- ci_wf %&gt;%\n  finalize_workflow(svm_tune_best) %&gt;%\n  last_fit(split)\n      \n      \nsvm_rbf_roc_tuned &lt;- last_rbf_fit %&gt;% \n  collect_predictions() %&gt;% \n  roc_curve(Attrition_Flag, `.pred_Attrited Customer`) %&gt;% \n  mutate(model = 'SVM')\n\nsvm_rbf_pr_tuned &lt;- last_rbf_fit %&gt;% \n  collect_predictions() %&gt;% \n  pr_curve(Attrition_Flag,  `.pred_Attrited Customer`) %&gt;% \n  mutate(model = 'SVM')\n\n\ncols &lt;- c(\"darkgreen\")\nnames(cols) &lt;- c(\"svm\")\nplot_title &lt;- glue::glue(\n  \"ROC- and PR-Curve for a\n  &lt;span style='color:{cols['svm']};'&gt;SVM with RBF kernel&lt;/span&gt;\"\n  )\np1 &lt;- svm_rbf_roc_tuned %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + \n  geom_path(lwd = 1.5, alpha = 0.8) +\n  geom_abline(lty = 3) + \n  coord_equal() + \n  scale_color_manual(values = unname(cols))+\n  theme_minimal(base_size = 14)+\n  theme(legend.position = \"none\")\n\np2 &lt;- svm_rbf_pr_tuned %&gt;% \n  ggplot(aes(x = recall, y = precision, col = model)) + \n  geom_path(lwd = 1.5, alpha = 0.8) +\n  coord_equal() + \n  scale_color_manual(values = unname(cols))+\n  theme_minimal(base_size = 14)+\n  theme(legend.position = \"none\")\n\n\n(p1|p2) +\n  plot_annotation(\n  title = plot_title,\n  theme = theme(plot.title = element_markdown()))\n\n\n\n\n\n\n\n\n\n\nSolution 8.2 (Exercise 8.2). \n\n\n\nctrl_grid_ci &lt;- control_stack_grid()\n\nFirst, we specify the random forest model and add it to the recipe.\n\nrf_model &lt;- rand_forest(\n  mode = \"classification\",\n  mtry = tune(),\n  min_n = tune(),\n  trees = 1000\n) %&gt;%\n  set_engine(\"ranger\")\n\nci_wf &lt;- ci_wf %&gt;% update_model(rf_model)\n\nThen, we train it on the training data and validate the candidate hyper parameters on the validation data. By setting the argument control to crtl_grid_ci, we specify that the candidate models can be added to the stack.\n\nset.seed(121)\nrf_tune_res &lt;- ci_wf %&gt;% \n    tune_grid(grid = 30,\n              resamples = data_val_ci,\n              control = ctrl_grid_ci\n    )\n\nAfter tuning the random forest, we specify and tune a XGBoost model.\n\nset.seed(121)\nxgb_model &lt;- boost_tree(\n  trees = 1000,\n  tree_depth = tune(),\n  min_n = tune(),\n  mtry = tune(),         \n  loss_reduction = tune(),                     \n  learn_rate = tune()                          \n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\nci_wf &lt;- ci_wf %&gt;% update_model(xgb_model)\n\ndoParallel::registerDoParallel()\n\nxgb_tune_res &lt;- tune_grid(\n  ci_wf,\n  resamples = data_val_ci,\n  grid = 30,\n  control = ctrl_grid_ci\n)\n\n\n\nstack_ci &lt;- stacks() %&gt;%\n  add_candidates(rf_tune_res) %&gt;%\n  add_candidates(xgb_tune_res) %&gt;%\n  blend_predictions(\n   mixture = 1.0,\n   penalty = 10^(-6:-2),\n   metric = metric_set(accuracy),\n   times = 20\n ) \n\n\n\nset.seed(121)\nstack_ci &lt;- stack_ci %&gt;%\n fit_members()\n\nstack_pred &lt;- data_test_ci %&gt;%\n  bind_cols(predict(stack_ci, .)) %&gt;%\n  mutate(\n    .pred_class = factor(\n      if_else(.pred_class == 'Existing Customer',\n              \"Negative\",\n              \"Positive\" )\n      ),\n    Attrition_Flag = factor(\n      if_else(\n        Attrition_Flag == 'Existing Customer',\n        \"Negative\",\n        \"Positive\")\n      )\n  )\n\n\nstack_pred %&gt;%\n  conf_mat(Attrition_Flag,.pred_class) %&gt;%\n  pluck(1) %&gt;%\n  as_tibble() %&gt;%\n  mutate(\n      Prediction = factor(Prediction,\n                          levels = rev(levels(factor(Prediction)))),\n      Truth = factor(Truth)\n  ) %&gt;%\n  ggplot(aes(x = Prediction, y = Truth,fill = n)) +\n    geom_tile( colour = \"gray50\")+\n    geom_text(aes(label = n))+\n    scale_fill_gradient(low = \"white\", high = \"lightgreen\")+\n    theme_minimal()+\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\\[\n  \\mathrm{Sensitivity} = \\frac{370}{370+37} = 0.9090909\n\\] \\[\n  \\mathrm{Prescision} = \\frac{370}{370+18} = 0.9536082\n\\]\n\\[\n  \\mathrm{Accuracy} = \\frac{370+2107}{2532} = 0.978278\n\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Support Vector Machines and Stacking</span>"
    ]
  },
  {
    "objectID": "09_NN.html",
    "href": "09_NN.html",
    "title": "9  Neural Networks",
    "section": "",
    "text": "9.1 Introduction\nIn this exercise session, we will consider some theoretical and practical aspects of neural networks. Neural networks are by now, probably the most commonly used models in Machine Learning Research and application, as they offer a highly versatile and well-performing approach for both, classification and regression tasks. However, as we have seen when considering models like XGBoost with great performance comes great computational cost. Training neural networks not only requires a lot of data to yield satisfactory results but can also take a long time to train, depending on the number of features and samples.\nTraining neural networks using the {tidymodels} approach is certainly possible with the {brulee} library, but not ideal. Most machine learning libraries and neural network architectures are far more accessible in scripting languages like Python, which encompass a much wider variety of development frameworks.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "09_NN.html#introduction",
    "href": "09_NN.html#introduction",
    "title": "9  Neural Networks",
    "section": "",
    "text": "9.1.1 Installing TensorFlow for R\nThe {tensorflow} library provides a high-level API like Keras for model development, that is easy to use and highly productive.\nTo to get everything working, you will need to follow the steps outlined below.\n\nInstall the {tensorflow} package using the {remotes} package:\n\n# install.packages(\"remotes\")\nremotes::install_github(\"rstudio/tensorflow\")   \n\nOnce the {tensorflow} package is installed and updated, we need to make sure that Python is installed on our machine. The reason is, that TensoFflow is written in C++ and CUDA with Python being an interface to those languages. Since this interface has not yet been translated directly to R the most efficient approach was to create another interface that allows R code to be translated into Python without having to write a single line of Python code. To check if Python is already installed, you can execute the following snippet. If the snipped returns an empty character then Python is likely not yet installed.\n\nSys.which(\"python\")\n\nPython can then be installed by utilizing the {reticulate} library:\n\n# install.packages(\"reticulate\")\nreticulate::install_python()\n\nAfter installing Python and the {tensorflow} R library, we need to create a Python virtual environment and install the TensorFlow package in this virtual environment. This can be done with the following snippet:\n\nlibrary(tensorflow)\ninstall_tensorflow(envname = \"r-tensorflow\") \n\nAn additional layer on top of TensorFlow is provided by Keras. Keras is a Python interface that communicates with TensorFlow with the benefit of being simple, flexible, and powerful.\n\ninstall.packages(\"keras\")\nlibrary(keras)\ninstall_keras()\n\n\nIn summary, the process can be illustrated by the following diagram:\n\n\n\n\n\nIf everything was installed successfully, the following snippet should run without any issues:\n\n\n\n\n\n\nWarningTroubleshooting\n\n\n\n\n\nThere are several common problems you can run into while trying to install Python and TensorFlow which will be listed below with a potential solution.\n\nInstallation of {tensorflow}:\n\nremotes::install_github(\"rstudio/tensorflow\")\n\nIf the return value is\n\nSkipping install of ‘tensorflow’ from a github remote, the SHA1 (eeb1e66b) has not changed since last install. Use force = TRUE to force installation\n\nthen {tensorflow} has already been installed.\nInstallation of the Python package tensorflow:\n\nlibrary(tensorflow)\ninstall_tensorflow(envname = \"r-tensorflow\") \n\nIf the return value is\n\nError in install_tensorflow(envname = “r-tensorflow”) : You should call install_tensorflow()/install_keras() only in a fresh R session that has not yet initialized Keras and TensorFlow (this is to avoid DLL in use errors during installation)\n\nthen restart the R under Session -&gt; Restart R. It is likely that the Python package has already been installed, so instead of calling install_tensorflow(envname = \"r-tensorflow\") again, try the following snippet instead:\n\nlibrary(tensorflow)\ntf$constant(\"Hello TensorFlow!\")\n\ntf.Tensor(b'Hello TensorFlow!', shape=(), dtype=string)\n\n\nInstallation of the Python package keras:\n\nlibrary(keras3)\ninstall_keras()\n\nIf the return value is\n\nError in tensorflow::install_tensorflow(method = method, conda = conda, : You should call install_tensorflow()/install_keras() only in a fresh R session that has not yet initialized Keras and TensorFlow (this is to avoid DLL in use errors during installation)\n\nthen restart the R under Session -&gt; Restart R. It is likely that the Python package has already been installed, so instead of calling install_keras() again, try the following snippet instead:\n\nlibrary(tensorflow)\nreticulate::use_condaenv(\"r-tensorflow\", required = TRUE)\nlibrary(keras3)\ntf$constant(\"Hello TensorFlow!\")\n\ntf.Tensor(b'Hello TensorFlow!', shape=(), dtype=string)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "09_NN.html#introduction-1",
    "href": "09_NN.html#introduction-1",
    "title": "9  Neural Networks",
    "section": "9.2 Introduction",
    "text": "9.2 Introduction\nBesides the keras and tensorflow library we will need the following as well:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n9.2.1 Estimating rent prices with feed forward neural networks\nIn this example, we once again, try to predict the base rent prices in Munich using the data Apartment rental offers in Germany, which is the same as in Exercise 04.\nRecall that it contains roughly 850 rental listings for flats in Augsburg sourced from the online platform ImmoScout24 on three different dates in 2018 and 2019.\n\ndata_muc &lt;- read.csv(\"data/rent_muc.csv\")\n\nPreprocessing the dataset:\n\ndata_muc &lt;- data_muc %&gt;%\n  select(!c(\"serviceCharge\",\"heatingType\",\"picturecount\",\"totalRent\",\n            \"firingTypes\",\"typeOfFlat\",\"noRoomsRange\", \"petsAllowed\",\n            \"livingSpaceRange\",\"regio3\", \"floor\", \"date\")) %&gt;%\n  mutate(\n    interiorQual = factor(\n      interiorQual,\n      levels = c(\"simple\", \"normal\", \"sophisticated\", \"luxury\"),\n      ordered = TRUE\n    ),\n    condition = factor(\n      condition,\n      levels = c(\"need_of_renovation\", \"negotiable\",\"well_kept\",\n                 \"refurbished\",\"first_time_use_after_refurbishment\",\n                 \"modernized\",\"fully_renovated\", \"mint_condition\",\n                 \"first_time_use\"),\n      ordered = TRUE\n    ),\n    geo_plz = factor(geo_plz),\n    across(where(is.logical),~if_else(.==TRUE,1,0))) %&gt;%\n  filter(\n    baseRent &lt;= 4000,\n    livingSpace &lt;= 200  \n  )\n\n\nset.seed(24)\nsplit_rent &lt;- initial_split(data_muc)\ndata_train &lt;- training(split_rent)\ndata_test &lt;- testing(split_rent)\n\nPreprocessing of the data is handled by the following recipe:\n\nrec_rent &lt;- recipe(\n  formula = baseRent ~., \n  data = data_train\n) %&gt;%\n  update_role(scoutId, new_role = \"ID\") %&gt;%\n  step_novel(all_nominal_predictors())%&gt;%\n  step_unknown(all_nominal_predictors(), all_ordered_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors())%&gt;%\n  step_ordinalscore(all_ordered_predictors())%&gt;%\n  step_impute_mean(all_numeric_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nCompared to the previously used recipes, we used an additional step called step_normalize(), that normalizes the specified variables. In other words, by applying the step_normalize() function, we transform the specified data such that each passed feature has a mean of zero and standard deviation of one.\nAfter setting up the recipe, the approach of building a neural network model starts to differ from the usual {tidymodels} procedure. Since Keras and TensorFlow models are not part of the {tidymodels} framework, we have to apply the recipe to the training and test data manually and add it to the training procedure. This can be done by using the previously introduced prep and bake functions. The preprocessed datasets then need to be split into training features and training labels. When creating the training features, we need to make sure to remove the feature scoutId as it should not be used as a predictor.\n\ndata_train_nn &lt;- rec_rent %&gt;%\n  prep() %&gt;%\n  bake(data_train)\n\ntrain_features &lt;- data_train_nn %&gt;%\n  select(-c(\"scoutId\", \"baseRent\"))\n\ntrain_labels &lt;- data_train_nn %&gt;%\n  select(\"baseRent\")\n\ndata_test_nn &lt;- rec_rent %&gt;%\n  prep(training = data_train) %&gt;%\n  bake(data_test) \n\ntest_features &lt;- data_test_nn %&gt;%\n  select(-c(\"scoutId\", \"baseRent\"))\n\ntest_labels &lt;- data_test_nn %&gt;%\n  select(c(\"baseRent\"))\n\nOnce we have successfully set up our data, we can move on to specifying our neural network architecture.\nTo specify a neural network architecture with two hidden layers and two dropout layers, we write a function build_model that takes the following inputs:\n\nn_input: input dimension (number of training labels).\nh1: dimension of the first hidden layer.\nh2: dimension of the second hidden layer.\nh3: dimension of the third hidden layer.\nn_output: output dimension.\nd1: dropout rate of the first hidden layer.\nd2: dropout rate of the second hidden layer.\nd3: dropout rate of the third hidden layer.\n\nGiven those inputs, the function should define a sequential model with 3 dense layers (where the first layer is equipped with the tanh activation function, and the second and third layer with the \"relu\" activation function) and 2 dropout layers in between.\n\nbuild_model &lt;- function(n_input, h1, h2, h3, n_output, d1, d2, d3) {\n  \n  model &lt;- keras_model_sequential(input_shape = c(n_input)) %&gt;%\n    \n    layer_dense(units = h1, activation = 'relu') %&gt;%\n    layer_batch_normalization() %&gt;%\n    layer_dropout(rate = d1) %&gt;% \n    \n    layer_dense(units = h2, activation = 'relu') %&gt;%\n    layer_batch_normalization() %&gt;%\n    layer_dropout(rate = d2) %&gt;%\n    \n    layer_dense(units = h3, activation = 'relu') %&gt;%\n    layer_batch_normalization() %&gt;%\n    layer_dropout(rate = d3) %&gt;%\n    \n    layer_dense(units = n_output, activation = 'relu') \n  \n  return(model)\n}\n\nThe idea behind writing such a function is to create a simple wrapper function that allows to specify the network parameters.\nWe can now define a model dnn_model with the newly written function from the previous code cell by passing the following inputs:\n\nn_input= ncol(training_features) (note, that the variable name might differ in your implementation)\nh1=256\nh2=128\nh3=64\nn_output = 1\nd1 = 0.2\nd2 = 0.1\nd3 = 0.1\n\nOnce the model is specified, compile the model using the compile function with loss specified as \"mae\" and optimizer \"optimizer_adam\", where the learning rate is equal to 0.001 and the weight decay equal to 0. By setting the additional argument metrics = 'RootMeanSquaredError', we can directly evaluate the model using the same metric as for the models we used in previous exercises.\nAfter compiling the model, we need to train it by using the fit function. As input parameters for the fit function, use the previously defined training features and training labels. Furthermore, set the validation split to 0.175 and the number of epochs to 400. By setting the argument verbose=1, we can track the training process in the Viewer and Console pane.\n\nset.seed(24)\ntensorflow::set_random_seed(20)\n\ndnn_model &lt;- build_model(ncol(train_features),\n                         256,\n                         128,\n                         64,\n                         1,\n                         0.2,\n                         0.2,\n                         0.2\n            )\n\ndnn_model &lt;- dnn_model %&gt;%\n  compile(\n    loss = 'mse',\n    optimizer = optimizer_adam(0.00075, weight_decay = 0.001),\n    metrics = 'RootMeanSquaredError'\n  )\n\nhistory &lt;- dnn_model %&gt;% fit(\n  as.matrix(train_features),\n  as.matrix(train_labels),\n  validation_split = 0.1,\n  verbose = 1,\n  epochs = 400\n)\n\nWe can also visualize the training procedure using the {ggplot2} library:\n\nhistory %&gt;%\n  pluck(2) %&gt;%\n  as_tibble() %&gt;%\n  select(-c(\"loss\",\"val_loss\")) %&gt;%\n  pivot_longer(cols=c(\"RootMeanSquaredError\",\n                      \"val_RootMeanSquaredError\"),\n               names_to = \"set_name\") %&gt;%\n  ggplot(aes(x=seq(1,nrow(.)),y=value, color = set_name))+\n    geom_point(alpha=0.4)+\n    geom_line(alpha=0.4)+\n    scale_color_discrete(labels = c(\"Training\",\"Validation\"))+\n    labs(\n      color = \"Set\"\n    )+\n    xlab(\"Epoch\")+\n    ylab(\"RMSE\")+\n    ylim(0,3000)+\n    theme_minimal(base_size = 14)+\n    theme(\n      legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\nNote, that in the example above. No hyperparametertuning was performed. Hyperparametertuning with Keras and TensorFlow is certainly possible, (see e.g., {kerastuneR}). However, this would be beyond the scope of this exercises and will be left for the reader to explore on their own.\nTo evaluate the model on the test set, we can use the keras::evaluate function:\n\ntest_results &lt;- dnn_model %&gt;% keras::evaluate(\n  as.matrix(test_features),\n  as.matrix(test_labels)\n) \n\n34/34 - 0s - 2ms/step - RootMeanSquaredError: 378.0413 - loss: 142915.2500\n\ntest_results\n\n$RootMeanSquaredError\n[1] 378.0413\n\n$loss\n[1] 142915.2\n\n\nThe test error of \\(486.46\\) is better than the XGBoost test error (\\(543\\)) we observed in Session 06.\nWe can now use the neural network object dnn_model_reg to explore different interpretability methods. The package that provides these methods is {iml} which stands for “interpretable machine learning”.\n\nlibrary(iml)\n\nBefore creating a new Predictor object called mod by piping the model dnn_model_reg into the Predictor$new function with additional arguments data = train_features, y = train_labels.\n\ndata_train_nn_filtered &lt;- data_train_nn %&gt;%\n  filter(livingSpace&lt;=quantile(livingSpace,0.95))\n\ntrain_features_filtered &lt;- data_train_nn_filtered %&gt;%\n  dplyr::select(-c(\"scoutId\", \"baseRent\"))\n\ntrain_labels_filtered &lt;- data_train_nn_filtered %&gt;%\n  dplyr::select(\"baseRent\")\n\ncustom_predict &lt;- function(model, newdata) {\n  data_matrix &lt;- as.matrix(newdata)\n  results &lt;- predict(model, data_matrix)\n  return(as.numeric(results))\n}\n\npredictor &lt;- Predictor$new(\n  model = dnn_model,             \n  data = train_features,         \n  y = train_labels,              \n  predict.fun = custom_predict\n)\n\nThis predictor mod will be used throughout the remaining introduction.\nWe can use the LocalModel$new() function to create a (linear) local surrogate model for the second observation of the training features. By using the \"gower\" distance, the data points passed are weighted by their proximity to the instance to be explained. The argument k sets the number of features used for the approximation.\n\n(loc.mod &lt;- LocalModel$new(\n  predictor,                     # Use the wrapper we just made\n  x.interest = train_features[2, ],\n  dist.fun = \"gower\",\n  k = 15\n))\n\n100/100 - 0s - 2ms/step\n\n\nInterpretation method:  LocalModel \n\n\nAnalysed predictor: \nPrediction task: unknown \n\n\nAnalysed data:\nSampling from data.frame with 3174 rows and 112 columns.\n\n\nHead of results:\n                      beta  x.recoded      effect         x.original\ncellar          -0.8309040  0.6436676  -0.5348259  0.643667561507178\nlivingSpace    349.2577399  0.4548459 158.8584377  0.454845861676092\nlift             0.9869047  0.8059054   0.7953518  0.805905366050985\ncondition_01    24.6646997  0.4668963  11.5158571  0.466896302038034\ncondition_07     7.4945562  1.4937065  11.1946673    1.4937065061942\ninteriorQual_1  29.4387790 -0.7250461 -21.3444714 -0.725046081540302\n                      feature                     feature.value\ncellar                 cellar          cellar=0.643667561507178\nlivingSpace       livingSpace     livingSpace=0.454845861676092\nlift                     lift            lift=0.805905366050985\ncondition_01     condition_01    condition_01=0.466896302038034\ncondition_07     condition_07      condition_07=1.4937065061942\ninteriorQual_1 interiorQual_1 interiorQual_1=-0.725046081540302\n\n\nAfter creating the model, we can use the plot function to visualize the model and identify the feature that has the largest effect on the outcome variable baseRent.\n\nplot(loc.mod)+theme_minimal(base_size = 14)\n\n1/1 - 0s - 23ms/step\n\n\n\n\n\n\n\n\n\nAccording to the generated plot, the most influential feature for the second sample is livingSpace.\nFor the feature livingspace, we can create and plot feature effect plots where one plot is created using the method \"ice\" and the second plot created by using the method \"pdp\".\nRecall, that for ICE (Individual Conditional Expectation) plots, we choose one individual feature \\(X_j\\), vary its value between \\(\\min(x_j)\\) and \\(\\max(X_j)\\), while keeping every other feature constant. By evaluating the model for each value, we obtain new estimates that (hopefully) show some trend with the varied feature. By repeating this procedure for every observation in the dataset, we obtain a curve for every instance that describes how the estimate behaves for varying \\(X_j\\) between the smallest and largest observed value.\nA partial dependence plot can be obtained by considering the average of all curves for one feature.\n\nlibrary(patchwork)\n\n# ICE \np1_fe &lt;-  FeatureEffect$new(predictor,\n                            feature = \"livingSpace\",\n                            method = \"ice\")$plot()\n\n#Partial dependence plot\np2_fe &lt;- FeatureEffect$new(predictor,\n                           feature = \"livingSpace\",\n                           method = \"pdp\")$plot()\n\n\n(p1_fe|p2_fe) +\n    plot_layout(\n      axes = \"collect\"\n    )&\n    theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nThe ICE plot shows that for almost all observations, an increase in livingSpace corresponds to an increase in estimated base rent. The PDP plot shows, that on average, this is true as well.\nTo check if there are any observations that defeat this trend, we can analyse the points generated by the FeatureEffect function.\n\ndf_ice_filtered &lt;- p1_fe$data %&gt;%\n  group_by(.id) %&gt;%\n  filter(livingSpace == min(livingSpace) | \n        livingSpace == max(livingSpace)) %&gt;%\n  pivot_wider(names_from = livingSpace, values_from = .value) %&gt;%\n  rename(min_rent = 3, \n         max_rent = 4)\n\n\ndf_ice_filtered %&gt;% glimpse()\n\nRows: 3,174\nColumns: 4\nGroups: .id [3,174]\n$ .type    &lt;chr&gt; \"ice\", \"ice\", \"ice\", \"ice\", \"ice\", \"ice\", \"ice\", \"ice\", \"ice\"…\n$ .id      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ min_rent &lt;dbl&gt; 1203.3833, 1060.4132, 1162.8619, 961.0859, 929.2620, 1097.427…\n$ max_rent &lt;dbl&gt; 3036.582, 2539.832, 3292.610, 2438.019, 2698.038, 2176.416, 3…\n\n\nFirst, we group the points by the .id column which represents the index of the samples. Since not only the smallest and largest living space is used to predict a new base rent, we filter every value between. By using the pivot_wider function in line 5, we create a wider data set with two new columns displaying the base rent estimated for the smallest and largest living space respectively. Since the newly created columns have messy names (these are the normalized values for the smallest and largest living space), we change them to min_rent and max_rent.\nThis newly created dataset can then be used to check for any abnormalities:\nTo check whether there are any listings where the base rent decreases with increasing living space, the following snippet provides an answer:\n\ndf_ice_filtered %&gt;%\n  filter(min_rent-max_rent &gt;=0)\n\n# A tibble: 4 × 4\n# Groups:   .id [4]\n  .type   .id min_rent max_rent\n  &lt;chr&gt; &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 ice     621    1055.     958.\n2 ice    1709    1059.    1041.\n3 ice    2792    1369.    1342.\n4 ice    3147       0        0 \n\n\nSurprisingly there is one listing where an increase in living space yields a decreasing base Rent! Using the following snippet, we can visualize our finding:\n\nid_neg &lt;- 621\np1_fe$data %&gt;%\n  filter(.id!=id_neg) %&gt;%\n  ggplot(aes(x=livingSpace,y=.value, group = .id))+\n  geom_line(color=\"gray60\")+\n  geom_line(data=(filter(p1_fe$data,.id==id_neg)),\n            aes(x=livingSpace,y=.value),\n            color = \"red\",\n            linewidth=1.5)+\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\nAt this point, further analysis could be conducted to find out why that is the case. However, this is beyond the scope of this exercise.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "09_NN.html#exercises",
    "href": "09_NN.html#exercises",
    "title": "9  Neural Networks",
    "section": "9.3 Exercises",
    "text": "9.3 Exercises\n\n9.3.1 Theoretical Exercises\nThe notation used for the following exercise will be somewhat different from the notation in the lecture. However, I will try to explain every notational aspect as clearly as possible.\nConsider the neural network depicted below.\n\n\n\n\n\nHere,\n\n\\((x_1, x_2)^\\top\\in \\mathbb{R}^2\\) denotes an input vector\n\\(\\{h_i^{(1)}\\}_{i=1}^3\\) denote a hidden layer with each neuron defined by \\[ h_i^{(1)} := \\sigma_1\\left(\\sum_{j=1}^{2}w_{ij}^{(1)} x_j + b^{(1)}_i\\right)\\] where \\(\\sigma_1:\\mathbb{R}\\to\\mathbb{R}\\) denotes an activation function, \\(w_{i1}^{(1)},w_{i2}^{(1)}, b^{(1)}_i\\in\\mathbb{R},i=1,...,3\\) denote weights and a bias for the first layer.\n\\(y_1\\) is the output of the neural network that is defined as \\[\ny_1 = \\sigma_2\\left(\\sum_{i=1}^{3}w_{i1}^{(2)}h_i^{(1)}+b_i^{(2)}\\right),\n\\tag{9.1}\\]\nwhere \\(\\sigma_2:\\mathbb{R}\\to\\mathbb{R}\\) denotes another activation function, and \\(w_{i1}^{(2)}, b^{(2)}_i\\in\\mathbb{R},i=1,...,3\\) denote weights and a bias for the second/final layer.\n\n\nExercise 9.1 Assume that \\(\\sigma_1 = \\sigma_2 = \\mathrm{ReLU}\\), i.e., \\(x\\mapsto \\max\\{0,x\\}\\). Set up the model equation for this neural network by expanding the term \\(y_1\\).\n\n\nExercise 9.2 In this exercise, we want to implement and visualize the neural network of Exercise 9.1.\n\nWrite a function nn that represents the neural network of the previous exercise. It should take a vector \\(x\\in\\mathbb{R}^2\\), a weight matrix \\(w_1\\in\\mathbb{R}^{3\\times 2}\\) used to calculate \\(h_i^{(1)}\\), a weight vector \\(w_2\\in\\mathbb{R}^3\\) used to calculate the term \\(\\sum_{i=1}^{3}w_{i1}^{(2)}h_i^{(1)}\\) and two bias terms \\(b_1,b_2\\in\\mathbb{R}\\) as an input. The return value should be the output of the neural network we used in the previous exercises.\nGiven the following weights, biases, and input matrix x, calculate the return value and plot the results using the geom_tile() function where the return value of the nn function is set to be the fill parameter.\n\n\n\n\n\n\nTip\n\n\n\nYou will have to use the lapply function to be able to apply the input matrix x to the function nn.\n\n\n\n\n\nExercise 9.3 We can generalize the neural network above in a way that the hidden layer has \\(n\\in\\mathbb{N}\\) instead of \\(3\\) neurons. Modify Equation 9.1, such that the hidden layer now has \\(n\\in\\mathbb{N}\\) neurons! What are the benefits and drawbacks of adding additional neurons? Name one each.\n\n\nExercise 9.4 Explain in your own words the difference between global interpretability models and local interpretability models.\n\n\nExercise 9.5 Explain in your own words the meaning of “post-hoc” interpretation mehtods.\n\n\nExercise 9.6 Explain in your own words the difference between individual conditional expectation (ICE) for a feature and partial dependence on a feature.\n\n\n\n9.3.2 Neural Networks for Classification\nThe dataset for this exercise will be the Credit Card Customers data set that we already used in previous exercises. You can either download it again using the provided link or the button below.\n\nDownload BankChurners\n\nRecall that the data set consists of 10,127 entries that represent individual customers of a bank including but not limited to their age, salary, credit card limit, and credit card category.\nThe goal is to find out whether a customer will stay or leave the bank given the above features.\nThe following training, validation and test split should be used for training the models of the subsequent exercises.\n\ncredit_info &lt;- read.csv(\"data/BankChurners.csv\")\n\ncredit_info &lt;- credit_info %&gt;%\n  mutate(\n    Attrition_Flag = if_else(Attrition_Flag==\"Attrited Customer\",\n                             1,\n                             0)\n)\n\nset.seed(121)\nsplit &lt;- initial_split(credit_info, strata = Attrition_Flag)\ndata_train_ci &lt;- training(split)\ndata_test_ci &lt;- testing(split)\n\nPreprocessing of the data is handled by the following recipe.\n\nlevels_income &lt;- c(\"Less than $40K\",\"$40K - $60K\",\n                   \"$60K - $80K\",\"$80K - $120K\",\"$120K +\")\n\nlevels_education &lt;- c(\"Uneducated\", \"High School\",\"College\",\n                      \"Graduate\",  \"Post-Graduate\", \"Doctorate\")\n\nrec_ci &lt;- recipe(Attrition_Flag ~., data = data_train_ci) %&gt;%\n  update_role(CLIENTNUM, new_role = \"ID\") %&gt;%\n  step_mutate_at(all_nominal_predictors(),\n               fn = ~if_else(.%in% c(\"Unknown\",\"unknown\"),NA,.)\n  )%&gt;%\n  step_string2factor(Income_Category,\n                     levels = levels_income,\n                     ordered = TRUE) %&gt;%\n  step_string2factor(Education_Level,\n                     levels = levels_education,\n                     ordered = TRUE) %&gt;%\n  step_ordinalscore(all_ordered_predictors()) %&gt;%\n  step_unknown(all_factor_predictors()) %&gt;%\n  step_impute_knn(all_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_corr(all_predictors()) %&gt;%\n  step_normalize(all_predictors())\n\n\nExercise 9.7 Given the recipe and data split, create the training and testing features and labels that will later be used for training a neural network.\n\n\nExercise 9.8 For this exercise set the internal R seed to 24 and tensorflow::set_random_seed(2) to obtain reproducible results.\n\nWrite a function build_model with arguments n_input, h1,h2,n_output,d1,d2, where\n\nn_input denotes the dimension of the input,\nh_i \\(i=1,2\\) denotes the dimensions of two hidden layers,\nn_output denotes the dimension of the output, and\nd_i \\(i=1,2\\) denotes the dropout rate of the two hidden layers.\n\nGiven these inputs, the function should create a Keras sequential model consisting of three dense layers with two dropout layers in between having a dropout rate of d1 and d2 respectively. The first and second dense layer should be equipped with the 'relu' activation function and the last layer with the sigmoid activation function. The function build_model should then return the specified model.\nCreate an instance of a keras sequential model using the function described with arguments\n\nn_input = ncol(train_features),\nh_1 = 30, h_2 =  4,\nn_output = 1, and\nd_1 = 0.2, d_2 = 0.\n\nCompile the model with loss set to binary_crossentropy, optimizer set to optimizer_adam(5e-4, weight_decay = 1e-6) and metrics= 'Accuracy'. Then, train the model using a validation split size of \\(10\\%\\) and \\(100\\) epochs using the fit() function.\nFinally, evaluate the model on the test data.\n\n\n\nExercise 9.9 Use the following Code snippet to generate predictions for the test data and construct a confusion matrix based on the predictions. Then, calculate the sensitivity, precision and accuracy for the model.\n\ntest_pred &lt;- dnn_model %&gt;%\n  predict(\n    as.matrix(test_features)\n  ) %&gt;%\n  as_tibble() %&gt;%\n  mutate(\n    pred_class = if_else(V1&gt;0.5,\"Positive\",\"Negative\") %&gt;% factor(),\n    truth = if_else(test_labels == 1,\"Positive\",\"Negative\") %&gt;% factor()\n  )",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "09_NN.html#solutions",
    "href": "09_NN.html#solutions",
    "title": "9  Neural Networks",
    "section": "9.4 Solutions",
    "text": "9.4 Solutions\n\nSolution 9.1 (Exercise 9.1). \\[\\begin{align*}\n  y_1 &= &&\\sigma_2\\left(\\sum_{i=1}^{3}w_{i1}^{(2)}h_i^{(1)}+b_i^{(2)}\\right)\\\\\n      &= &&\\max\\left\\{\\sum_{i=1}^{3}w_{i1}^{(2)}h_i^{(1)}+b_i^{(2)},0\\right\\}\\\\\n      &= &&\\max\\left\\{\\sum_{i=1}^{3}w_{i1}^{(2)}\\sigma_1\\left(\\sum_{j=1}^{2}w_{ij}^{(1)} x_j + b_i^{(1)}\\right)+b_i^{(2)},0\\right\\}\\\\\n      &= &&\\max\\left\\{\\sum_{i=1}^{3}w_{i1}^{(2)}\\max\\left\\{\\sum_{j=1}^{2}w_{ij}^{(1)} x_j + b_i^{(1)},0\\right\\}+b_i^{(2)},0\\right\\}\\\\\n      &=&&\\max\\left\\{\\sum_{i=1}^{3}w_{i1}^{(2)}\\max\\left\\{w_{i1}^{(1)} x_1 + w_{i2}^{(1)} x_2 + b_i^{(1)},0\\right\\}+b_i^{(2)},0\\right\\} \\\\\n      &=&&\\max\\left\\{\\right.\\\\\n      & && w_{11}^{(2)}\\max\\left\\{w_{11}^{(1)} x_1 + w_{12}^{(1)} x_2 + b^{(1)}_1,0\\right\\} + b_1^{(2)}\\\\\n      & && w_{21}^{(2)}\\max\\left\\{w_{21}^{(1)} x_1 + w_{22}^{(1)} x_2 + b^{(1)}_2,0\\right\\} + b_2^{(2)}\\\\\n      & && w_{31}^{(2)}\\max\\left\\{w_{31}^{(1)} x_1 + w_{32}^{(1)} x_2 + b^{(1)}_3,0\\right\\} + b_3^{(2)}\\\\\n      & && \\left.,0\\right\\}\n\\end{align*}\\]\n\n\nSolution 9.2 (Exercise 9.2). \n\n\n\nnn &lt;- function(x, w1, w2, b1, b2){\n  h = pmax(w1%*%x+b1,0)\n  y = pmax(w2%*%h+b2,0)\n  return(y[1])\n}\n\n\n\nw1 &lt;- matrix(c(0.2,0.2,0.3,0.3,0.2,0.2), ncol = 2)\nw2 &lt;- matrix(c(-0.3,0.2,0.3), nrow = 1)\nb1 &lt;- 0.2\nb2 &lt;- 0.3\n\nx1 &lt;- seq(-1,1,length.out = 100)\nx2 &lt;- seq(-1,1,length.out = 100)\n\nx &lt;- expand.grid(x1,x2) %&gt;% as.matrix()\n\n\ny &lt;- apply(x, 1, function(row) nn(row, w1, w2, b1, b2))\n\ncbind(x, y) %&gt;%\n  as_tibble() %&gt;% set_names(c(\"x1\",\"x2\",\"y\")) %&gt;%\n  ggplot(aes(x=x1,y=x2,fill = y)) +\n  geom_tile() + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nSolution 9.3 (Exercise 9.3). \\[\ny_1 = \\sigma_2\\left(\\sum_{i=1}^{n}w_{i1}^{(2)}h_i^{(1)}+b_2\\right)\n\\]\nAdvantage The model is more flexible and can therefore better estimate the training data.\nDisadvantage: The model is prone to over fitting.\n\n\nSolution 9.4 (Exercise 9.4). \n\nThe goal of local interpretability is to explain the model’s behavior for a specific input, or a single observation in general.\nThe goal of global interpretability models is to explain the model’s behavior on the whole input data.\n\n\n\nSolution 9.5 (Exercise 9.5). Post-hoc intepretation refers to the process of interpreting the model’s behavior subsequent to model training. The goal is to make use of the predifined structure of the model without modifying the training process itself.\n\n\nSolution 9.6 (Exercise 9.6). ICE is used to model the relationship between a single input feature and the model’s predictions. Partial dependence is a modification of ICE, as it simply takes the average of ICE over the whole sample.\n\n\nSolution 9.7 (Exercise 9.7). \n\ndata_train_nn &lt;- rec_ci %&gt;%\n  prep() %&gt;%\n  bake(data_train_ci)\n\ntrain_features &lt;- data_train_nn %&gt;%\n  select(-c(CLIENTNUM,Attrition_Flag))\n\ntrain_labels &lt;- data_train_nn %&gt;%\n  select(Attrition_Flag)\n\ndata_test_nn &lt;- rec_ci %&gt;%\n  prep() %&gt;%\n  bake(data_test_ci) \n\ntest_features &lt;- data_test_nn %&gt;%\n  select(-c(CLIENTNUM,Attrition_Flag))\n\ntest_labels &lt;- data_test_nn %&gt;%\n  select(Attrition_Flag)\n\n\n\nSolution 9.8 (Exercise 9.8). \n\nbuild_model &lt;- function(n_input, h1,h2,n_output,d1,d2) {\n  model &lt;- keras_model_sequential(input_shape = c(n_input)) %&gt;%\n    layer_dense(h1, activation = 'relu') %&gt;%\n    layer_dropout(rate = d1) %&gt;% \n    layer_dense(h2, activation = 'relu') %&gt;%\n    layer_dropout(rate = d2) %&gt;%\n    layer_dense(n_output, activation = 'sigmoid') \n  model\n}\n\n\nset.seed(24)\ntensorflow::set_random_seed(2)\n\ndnn_model &lt;- build_model(ncol(train_features), 30, 4, 1, 0.2, 0)\ndnn_model &lt;- dnn_model %&gt;%\n    compile(\n    loss = 'binary_crossentropy',\n    optimizer = optimizer_adam(5e-4, weight_decay = 1e-6),\n    metrics='Accuracy'\n  )\n\nhistory &lt;- dnn_model %&gt;% fit(\n  as.matrix(train_features),\n  as.matrix(train_labels),\n  validation_split = 0.1,\n  verbose = 1,\n  epochs = 100\n)\n\n\ntest_results &lt;- dnn_model %&gt;% keras::evaluate(\n  as.matrix(test_features),\n  as.matrix(test_labels)\n) \n\n80/80 - 0s - 1ms/step - Accuracy: 0.8961 - loss: 0.5887\n\ntest_results\n\n$Accuracy\n[1] 0.8961295\n\n$loss\n[1] 0.5886608\n\n\n\n\nSolution 9.9 (Exercise 9.9). \n\ntest_pred &lt;- dnn_model %&gt;%\n  predict(\n    as.matrix(test_features)\n  ) %&gt;%\n  as_tibble() %&gt;%\n  mutate(\n    test_labels=test_labels$Attrition_Flag,\n    pred_class = if_else(V1&gt;0.5,\"Positive\",\"Negative\") %&gt;% factor(),\n    truth = if_else(test_labels == 1,\"Positive\",\"Negative\") %&gt;% factor()\n  )\n\n80/80 - 0s - 1ms/step\n\ncm_nn &lt;- test_pred %&gt;% conf_mat(estimate = pred_class,truth=truth)\n\ntable(test_pred$pred_class,test_pred$truth)\n\n          \n           Negative Positive\n  Negative     2107      245\n  Positive       18      162\n\ncm_tib &lt;- as_tibble(cm_nn$table)%&gt;% \n  mutate(\n    Prediction = factor(Prediction,\n                        levels = rev(levels(factor(Prediction)))),\n    Truth = factor(Truth)\n)\n\ncm_tib %&gt;% ggplot(aes(x = Prediction, y = Truth,fill = n)) +\n    geom_tile( colour = \"gray50\")+\n    geom_text(aes(label = n))+\n    scale_fill_gradient(low = \"white\", high = \"lightgreen\")+\n    theme_minimal()+\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe metrics are then given by\n\\[\\begin{align*}\n   \\mathrm{Accuracy} &= \\frac{173+2111}{173+2111+14+234} = 0.902\\\\\n   \\mathrm{Recall} &= \\frac{173}{173+234}=0.4250\\\\\n   \\mathrm{Precision} &= \\frac{173}{173+14}=0.925\n\\end{align*}\\]\nWhile the accuracy and precision of the model are relatively high, these metrics should be chosen carefully when the data set is unbalanced. Especially the precision is worrisome, since the model only correctly identifies roughly \\(42\\%\\) of the customers that might want to leave the bank.\nAnother aspect we have not considered: The classification threshold is set to \\(0.5\\), which might not be optimal. Consider the following ROC curve was produced by the DNN model.\n\ntest_pred %&gt;%\n  roc_curve(truth = truth,V1) %&gt;%\n  ggplot(aes(x=1-specificity,y=sensitivity))+\n  geom_line(linewidth=1.5)+\n  coord_equal()+\n  theme_minimal(base_size=12)\n\n\n\n\n\n\n\n\nThe optimal threshold value with respect to the ROC curve is the value that maximizes both, the sensitivity and specificity of the model. In simpler terms, the optimal threshold value is the one closest to the top left corner. To find this value, we can simply calculate the euclidean distance of all the points on the ROC curve to the top left corner and select the threshold of the one with minimal distance to the top left corner.\n\ntest_pred %&gt;%\n  roc_curve(truth = truth,V1) %&gt;%\n  mutate(\n    dist = sqrt((1-specificity)^2+(sensitivity-1)^2)\n  ) %&gt;%\n  filter(dist == min(dist))\n\n# A tibble: 1 × 4\n  .threshold specificity sensitivity  dist\n       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1      0.991       0.779       0.856 0.264\n\n\nTo visualize the point on the ROC curve that corresponds to the optimal threshold, consider the following code snippet:\n\ntest_pred %&gt;%\n  roc_curve(truth = truth,V1) %&gt;%\n  ggplot(aes(x=1-specificity,y=sensitivity))+\n  geom_line(linewidth=1.5, color = \"gray60\")+\n  geom_point(data = tibble(x=1-0.784,y=0.841), aes(x=x,y=y), color = \"red\", size = 3)+\n  coord_equal()+\n  theme_minimal(base_size=12)\n\n\n\n\n\n\n\n\nUsing this threshold produces the following confusion matrix:\n\n\n80/80 - 0s - 760us/step\n\n\n\n\n\n\n\n\n\nWhile the accuracy of the model with the new threshold is lower (\\(90.2\\%\\) vs. \\(83.1\\%\\)), the new model identifies people who are at risk of leaving the bank a lot better.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "10_clustering.html",
    "href": "10_clustering.html",
    "title": "10  Unsupervised Learning",
    "section": "",
    "text": "10.1 Introduction\nIn the remaining exercise sessions, we will consider another major subfield of machine learning: unsupervised learning. In particular, we will consider clustering with hierarchical methods, the k-means algorithm, and SBSCAN. We will also consider dimension reduction algorithms such as principal component analysis and autoencoders which are a common preprocessing step.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "10_clustering.html#introduction",
    "href": "10_clustering.html#introduction",
    "title": "10  Unsupervised Learning",
    "section": "",
    "text": "10.1.1 Dimension reduction\nMany of the data sets we encountered so far have a dimension \\(\\gg 2\\), i.e. there are a lot more than two features. We can therefore not directly visualize all features and the response variable in one figure. One solution is to build a facet plot (similar to Exercise 1.15) that displays every possible combination of variables. However, this is increasingly difficult: Consider the Credit Info data set we frequently use. There are \\(n=10\\) variables we usually use and to display every variable combination, \\(\\begin{pmatrix} 10 \\\\ 2\\end{pmatrix} = 45\\) subplots would be required. The idea of dimension reduction aims to solve this problem. Instead of considering every feature individually, dimension reduction usually merges feature values based on some rule set. Principal component analysis combines features linearly in a way that the resulting linear combinations explain most of the variability in the data set. Autoencoders on the other hand aim to map the features into a lower dimensional subspace (latent space) and rebuild the true features based on the features in the latent space.\n\n10.1.1.1 Principal Component Analysis (PCA)\nHeuristically speaking, PCA assumes that in a \\(n\\) dimensional (feature) space, not every variable is equally important in terms of variability. Consider a feature matrix \\(X\\in\\mathbb{R}^{n\\times p}\\) (\\(p\\) features and \\(n\\) observations) with normalized features. To find the features that explain the most variance, the following steps are performed:\n\nCenter the features such that \\(\\hat{\\mathbb{E}(X_j)} = \\frac{1}{n}\\sum_{i=1}^n x_{ij} = 0\\) for \\(j=1,...,p\\).\nCalculate the empirical covariance matrix \\(X^\\prime X =: \\Sigma\\in\\mathbb{R}^{p\\times p}\\) that measures the joint variability of each pair of features.\nCalculate the singular value decomposition of \\(\\Sigma\\):\n\nSolve the system \\(\\mathrm{det}(\\Sigma - \\lambda I_p)= 0\\) where \\[\\begin{align}\n  I_p &= \\begin{pmatrix}\n            1 & 0 & ... & 0\\\\\n            0 & 1 & ... & 0\\\\\n            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n            0 & 0 & ... & 1\n        \\end{pmatrix} \\in \\mathbb{R}^{p\\times p} \\text{ and}\\\\\n  \\lambda &\\in\\mathbb{R}.\n\\end{align}\\] for \\(\\lambda\\). Here, we assume that there are \\(p\\) unique solutions.1.\nReorder the solutions \\(\\lambda_j,\\, j=1,...,p\\) descending such that \\(\\lambda_{(1)}\\geq...\\geq \\lambda_{(q)}\\).\nFor each ordered solution \\(\\lambda_{(j)},\\, j=1,...,p\\) that solves \\(\\mathrm{det}(\\Sigma - \\lambda_{(j)} I_p)= 0\\), solve the system \\[\\begin{align}\n(\\Sigma - \\lambda_{(j)} I_p)\\gamma_{(j)} = 0,\n      \\end{align}\\] where \\(\\gamma_{(j)} \\in \\mathbb{R}^p\\) for \\(j=1,...,p\\).\nNormalize the resulting vectors to obtain the principal components of the data. Note, that the normalizing constant of each vector \\(\\gamma_{(j)}\\) is given by \\(\\frac{1}{\\|\\gamma_{(j)}\\|}\\).\n\nBy choosing the first \\(k\\leq p\\) of the \\(p\\) ordered principal components, we can map the feature matrix \\(X\\) to the \\(k\\)-dimensional subspace by multiplying \\(X\\) with the respective matrix \\((\\gamma_{(1)},...,\\gamma_{(k)})\\in \\mathbb{R}^{p\\times k}\\). The relative amount of variance explained by the first \\(k\\) principal components is then given by the fraction \\[\\begin{equation}\n  \\frac{1}{\\sum_{j=1}^p \\lambda_{(j)}}\\sum_{j=1}^k \\lambda_{(j)}.\n\\end{equation}\\]\n\n\n\n10.1.1.2 PCA in R\nWe will need the following libraries throughout the session:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(tensorflow)\nlibrary(keras3)\nlibrary(ggtext)\nlibrary(patchwork)\n\nLet us consider the Bank Churners or Credit Info data set that we already used in previous exercises. You can either download it again using the provided link or the button below.\n\nDownload BankChurners\n\nRecall that the data set consists of 10,127 entries that represent individual customers of a bank including but not limited to their age, salary, credit card limit, and credit card category.\nThe goal is to reduce the dimensionality of the data set while leaving out the variable Attrition_Flag. By doing so, we might be able to get a better intuition on the distribution of attrited customers within the reduced data set.\nThe following code snippet takes care of the preprocessing:\n\ncredit_info &lt;- read.csv(\"data/BankChurners.csv\")\n\n\nlevels_income &lt;- c(\"Less than $40K\",\"$40K - $60K\",\n                   \"$60K - $80K\",\"$80K - $120K\",\"$120K +\")\n\nlevels_education &lt;- c(\"Uneducated\", \"High School\",\"College\",\n                      \"Graduate\",  \"Post-Graduate\", \"Doctorate\")\n\ncredit_info &lt;- credit_info %&gt;%\n  select(-c(CLIENTNUM)) %&gt;%\n  mutate(\n    across(where(~(is.factor(.)|is.character(.))),\n                 ~if_else(.%in% c(\"Unknown\",\"unknown\"),NA,.)\n           ),\n    Income_Category = factor(\n      Income_Category,\n      levels = levels_income,\n      ordered = TRUE),\n    Education_Level = factor(\n      Education_Level,\n      levels = levels_education,\n      ordered = TRUE),\n  )\n\n\nrec_ci &lt;- recipe(Attrition_Flag~., data =credit_info ) %&gt;%\n  step_novel(all_nominal_predictors())%&gt;%\n  step_unknown(all_nominal_predictors(), all_ordered_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors())%&gt;%\n  step_ordinalscore(all_ordered_predictors())%&gt;%\n  step_impute_mean(all_numeric_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\ncredit_info_prepped &lt;- rec_ci %&gt;%\n  prep() %&gt;%\n  bake(credit_info)\n\ncredit_info_prepped %&gt;% glimpse()\n\nRows: 10,127\nColumns: 35\n$ Customer_Age             &lt;dbl&gt; -0.16539741, 0.33355391, 0.58302958, -0.78908…\n$ Dependent_count          &lt;dbl&gt; 0.5033433, 2.0430978, 0.5033433, 1.2732205, 0…\n$ Months_on_book           &lt;dbl&gt; 0.38460189, 1.01066492, 0.00896407, -0.241461…\n$ Total_Relationship_Count &lt;dbl&gt; 0.7639049, 1.4072367, 0.1205731, -0.5227587, …\n$ Months_Inactive_12_mon   &lt;dbl&gt; -1.3270705, -1.3270705, -1.3270705, 1.6413972…\n$ Contacts_Count_12_mon    &lt;dbl&gt; 0.4923795, -0.4115957, -2.2195459, -1.3155708…\n$ Credit_Limit             &lt;dbl&gt; 0.446599851, -0.041364610, -0.573669472, -0.5…\n$ Total_Revolving_Bal      &lt;dbl&gt; -0.473398843, -0.366648718, -1.426787891, 1.6…\n$ Avg_Open_To_Buy          &lt;dbl&gt; 0.488946675, -0.008485569, -0.445636329, -0.7…\n$ Total_Amt_Chng_Q4_Q1     &lt;dbl&gt; 2.6233649, 3.5631169, 8.3668007, 2.9426981, 6…\n$ Total_Trans_Amt          &lt;dbl&gt; -0.9596592, -0.9163874, -0.7409451, -0.951711…\n$ Total_Trans_Ct           &lt;dbl&gt; -0.9738471, -1.3572734, -1.9111113, -1.911111…\n$ Total_Ct_Chng_Q4_Q1      &lt;dbl&gt; 3.833813303, 12.607950374, 6.807527542, 6.807…\n$ Avg_Utilization_Ratio    &lt;dbl&gt; -0.77584393, -0.61624523, -0.99710576, 1.7595…\n$ Attrition_Flag           &lt;fct&gt; Existing Customer, Existing Customer, Existin…\n$ Gender_M                 &lt;dbl&gt; 1.0599033, -0.9433891, 1.0599033, -0.9433891,…\n$ Education_Level_1        &lt;dbl&gt; -0.82186794, 0.09034782, 0.09034782, -0.82186…\n$ Education_Level_2        &lt;dbl&gt; 0.1910951, -1.0079258, -1.0079258, 0.1910951,…\n$ Education_Level_3        &lt;dbl&gt; 0.5632953, 0.1640448, 0.1640448, 0.5632953, -…\n$ Education_Level_4        &lt;dbl&gt; -1.7867754, 0.7826945, 0.7826945, -1.7867754,…\n$ Education_Level_5        &lt;dbl&gt; 1.4912555, -0.9493902, -0.9493902, 1.4912555,…\n$ Education_Level_6        &lt;dbl&gt; -0.7679845, -0.7679845, -0.7679845, -0.767984…\n$ Education_Level_7        &lt;dbl&gt; -0.1089545, 1.2429924, 1.2429924, -0.1089545,…\n$ Marital_Status_Married   &lt;dbl&gt; 1.077285, -0.928168, 1.077285, -0.928168, 1.0…\n$ Marital_Status_Single    &lt;dbl&gt; -0.7984674, 1.2522756, -0.7984674, -0.7984674…\n$ Marital_Status_unknown   &lt;dbl&gt; -0.2825949, -0.2825949, -0.2825949, 3.5382846…\n$ Income_Category_1        &lt;dbl&gt; 0.07513271, -0.95655404, 0.59097609, -0.95655…\n$ Income_Category_2        &lt;dbl&gt; -1.0612546, 1.0246428, -1.3219918, 1.0246428,…\n$ Income_Category_3        &lt;dbl&gt; 1.083891390, -1.087321766, -0.001715188, -1.0…\n$ Income_Category_4        &lt;dbl&gt; -0.06492072, 0.42190053, 1.15213241, 0.421900…\n$ Income_Category_5        &lt;dbl&gt; -1.75727006, -0.38759283, -0.04517352, -0.387…\n$ Income_Category_6        &lt;dbl&gt; 1.4461457, 0.1380203, -1.8241677, 0.1380203, …\n$ Card_Category_Gold       &lt;dbl&gt; -0.1076388, -0.1076388, -0.1076388, -0.107638…\n$ Card_Category_Platinum   &lt;dbl&gt; -0.04448181, -0.04448181, -0.04448181, -0.044…\n$ Card_Category_Silver     &lt;dbl&gt; -0.2407818, -0.2407818, -0.2407818, -0.240781…\n\n\nWe now have a preprocessed data set credit_info_prepped that can be applied to different dimension reduction algorithms.\nTo fit a PCA model, we have to pass the preprocessed data into the prcomp function. The return value is a list containing the standard deviations of the principal components (the square roots of the singular values of the covariance matrix.) and the matrix of variable loadings.\n\npca_fit &lt;- credit_info_prepped %&gt;%\n  select(-Attrition_Flag) %&gt;%\n  prcomp() \n\nWe can then use the augment function to create the reduced dataset. To access the first two principal components, we can use the select function.\n\ndata_reduced &lt;- pca_fit %&gt;%\n  augment(credit_info_prepped) %&gt;%\n  select(c(.fittedPC1,.fittedPC2))\n\n\n\nCode\ntitle_text_p1 &lt;- \"Visualisation of the first two principal components\"\nsubtitle_text_p1 &lt;- \"The &lt;span style='color:#0000FF;'&gt;blue&lt;/span&gt;\n                     dots represent customers with positive attrition flag\"\n\ntitle_text_p2 &lt;- \"Visualisation of the explained variance &lt;br&gt;\n                  per principal component\"\n\n\npca_fit_pos &lt;- data_reduced %&gt;%\n  mutate(Attrition_Flag = credit_info_prepped$Attrition_Flag) %&gt;%\n  filter(Attrition_Flag == \"Attrited Customer\") \n  \npca_fit_neg &lt;- data_reduced %&gt;%\n  mutate(Attrition_Flag = credit_info_prepped$Attrition_Flag) %&gt;%\n  filter(Attrition_Flag != \"Attrited Customer\") \n\np1 &lt;- pca_fit_neg %&gt;%\n  ggplot(aes(.fittedPC1,.fittedPC2)) +\n  geom_point(size = 2, color = \"gray70\", alpha = 0.5)+\n  geom_point(data=pca_fit_pos,\n             aes(x=.fittedPC1,y=.fittedPC2),\n             color=\"blue\",\n             size = 2,\n             alpha = 0.5)+\n  labs(\n    title = title_text_p1,\n    subtitle = subtitle_text_p1,\n    x = \"first PC\",\n    y = \"second PC\"\n    )+\n  theme_minimal(base_size = 10)+\n  theme(plot.subtitle = element_markdown())\n  \n\np2 &lt;- pca_fit %&gt;% tidy(matrix = \"eigenvalues\") %&gt;%\n  ggplot(aes(PC, percent)) +\n  geom_col(fill = \"lightblue\") +\n  scale_y_continuous(\n    labels = scales::percent_format(),\n    expand = expansion(mult = c(0, 0.01))\n  ) +\n  labs(title = title_text_p2)+\n  theme_minimal(base_size=10)+\n  theme(plot.title=element_markdown())\n  \np1|p2 \n\n\n\n\n\n\n\n\n\nIt is evident from the figures above that PCA is not the best choice for this data set. Considering the figure on the left, the general point cloud looks quite “random” and there does not seem to be any sort of trend.\nFurthermore, the figure on the right implicates, that less than \\(20\\%\\) of the variance in the data set is explained by the first two principal components.\n\n\n10.1.1.3 Autoencoders\nAn autoencoder consists of two feed forward neural networks, namely an encoder and decoder. As the names suggest, the encoder applies a mapping to some data which in this case reduces the dimension. On the other hand, the decoder aims to rebuild the encoded data to obtain the data that has originially been passed into the encoder. Figure 10.1 displays a general autoencoder architecture.\n\n\n\n\n\n\nFigure 10.1\n\n\n\nOn the left hand side, the encoder network is displayed. The input layer \\(I^E\\) consists of \\(N_I\\) neurons that is passed into \\(K_E\\) hidden layers. Each hidden layer \\(H_1^E,...,H_{K_E}^E\\) consists of \\(N_1,...,N_{K_E}\\) neurons. The output layer of the encoder consists of \\(N_L\\) neurons, where \\(L\\) denotes the dimension of the (feature) latent space. It is important that \\(L&lt;N_I\\) to ensure that the data is indeed compressed. The output \\(O^E\\) of the encoder is directly passed into the input layer \\(I^D\\). Note that the dimension of the encoder output is equal to the dimension of the decoder input and that there are no cross connection between the neurons, meaning that \\(O_i^E\\) only maps to \\(I_i^D\\). The architecture of the decoder is the same as the architecture as the encoder but mirrored.\nIn simple terms, an autoencoder are two neural networks stitchted together, where the output approximates the input and the section where the networks are stitched together has fewer neurons than the input/output layers.\n\n\n10.1.1.4 Autoencoders in R\nWe can use the {keras} and {tensorflow} framework to specify an autoencoder.\nSimilar to the neural networks defined in Session 09, we can create functions that aid us in building the autoencoder.\nThe encoder network in this example consists of an input layer with dimension n_imp, two hidden layers with dimension h1 and h2, activation function layers (ReLU) for the hidden layers, and two dropout layers with dropout probabilities d1 and d2 respectively that are applied to the output of the hidden layers.\nInstead of returning a keras_sequential model, we can also specify a model by passing the input and output into the keras_model function.\n\nencoder_spec &lt;- function(n_input,h1,h2,h3,d1,d2,d3,n_output){\n  \n  enc_input &lt;- layer_input(shape = n_input)\n\n  enc_output &lt;- enc_input %&gt;% \n    layer_dense(h1, activation = 'relu', input_shape = n_input) %&gt;%\n    layer_batch_normalization() %&gt;%\n    layer_dropout(rate = d1) %&gt;% \n    layer_dense(h2, activation = 'relu') %&gt;%\n    layer_batch_normalization() %&gt;%\n    layer_dropout(rate = d2) %&gt;%\n    layer_dense(h3, activation = 'relu') %&gt;%\n    layer_batch_normalization() %&gt;%\n    layer_dropout(rate = d3) %&gt;%\n    layer_dense(n_output) \n  \n  keras_model(enc_input, enc_output)\n \n}\n\nThe decoder specification function has the same input as the encoder specification function. Why we choose to do that can be seen in the auto encoder specification function.\n\ndecoder_spec &lt;- function(n_input,h1,h2,h3,d1,d2,d3,n_output){\n  \n  dec_input &lt;- layer_input(shape = n_input)\n  \n  dec_output &lt;- dec_input %&gt;% \n    layer_dense(h1, activation = 'relu', input_shape = n_input) %&gt;%\n    layer_batch_normalization() %&gt;%\n    layer_dropout(rate = d1) %&gt;% \n    layer_dense(h2, activation = 'relu') %&gt;%\n    layer_batch_normalization() %&gt;%\n    layer_dropout(rate = d2) %&gt;%\n    layer_dense(h3, activation = 'relu') %&gt;%\n    layer_batch_normalization() %&gt;%\n    layer_dropout(rate = d3) %&gt;%\n    layer_dense(n_output, activation = 'relu') \n \n  keras_model(dec_input, dec_output)\n  \n}\n\nThe function aen_spec specifies an autoencoder with input dimension n_inp, four hidden layers (h1,h2,h2,h1), and a dense layer specifying the dimension of the latent space n_latent. Inside the function, we first specify the input aen_input of the autoencoder as an input layer with dimension n_inp. Then, we define an encoder using the previously defined encoder_spec function. Note, that we defined the encoder using the &lt;&lt;- symbol. This is not a typo! The &lt;&lt;- operator allows us to access the specified encoder outside the aen_spec function once the autoencoder has been trained.\nWe then specify the decoder architecture where the output dimension (latent dimension) of the encoder is passed as the input dimension of the decoder. Additionally the dimension of the hidden layer are mirrored, meaning that the first hidden layer of the decoder has dimension h2 and the second hidden layer has the dimension d1. When mirroring the dimensions of the hidden layers, we need to adapt the drop out rates accordingly. The output dimension of the decoder is given by the dimension of the input.\nThe return value of the autoencoder specification is a keras model with input specified by an input layer and output specified as the sequence aen_input %&gt;% encoder() %&gt;% decoder().\n\naen_spec&lt;- function(n_input,h1,h2,h3,d1,d2,d3,n_latent){\n  \n aen_input &lt;- layer_input(shape = n_input)\n \n encoder &lt;&lt;- encoder_spec(n_input,h1,h2,h3,d1,d2,d3,n_latent)\n decoder &lt;- decoder_spec(n_latent,h3,h2,h1,d3,d2,d1,n_input)\n \n aen_output &lt;- aen_input %&gt;%\n  encoder() %&gt;% \n  decoder()\n   \n keras_model(aen_input, aen_output)\n \n}\n\nThe training procedure of the autoencoder is the same as for any other neural network we previously considered:\n\nCreate training data.\nUse the autoencoder specification function to create an untrained autoencoder.\nCompile the model to prepare it for the training process.\nPass the compiled model into the fit function to train the model.\n\n\nset.seed(6)\ntensorflow::set_random_seed(6)\n\ntrain_features &lt;- credit_info_prepped %&gt;%\n  select(-Attrition_Flag)\n\n\naen &lt;- aen_spec(n_input = ncol(train_features),\n                h1 = 256,\n                h2 = 128,\n                h3 = 32,\n                d1 = 0.2,\n                d2 = 0.1,\n                d3 = 0.1,\n                n_latent = 2)\n    \ned_nn &lt;- aen %&gt;%\n        compile(\n        loss = 'mse',\n        optimizer = optimizer_adam(1e-3)\n      )\n\n\nhistory_aen &lt;- ed_nn %&gt;% fit(\n      as.matrix(train_features),\n      as.matrix(train_features),\n      validation_split = 0,\n      epochs = 50\n    )\n\nWe can access the trained encoder directly using the obejct name (encoder). To create the lower dimensional representation of the data, we have to pass the data set as a matrix into the predict function.\n\ndata_enc &lt;- encoder %&gt;% predict(\n  as.matrix(\n    train_features\n  )\n) %&gt;%\n  as_tibble()\n\nWe can then plot the results in a similar fashion as with PCA:\n\n\nCode\ndata_enc &lt;- data_enc %&gt;%  \n  mutate(groups = credit_info$Attrition_Flag)\n\ndata_enc_pos &lt;- data_enc %&gt;%\n  filter(groups==\"Attrited Customer\") \n\ndata_enc %&gt;%\n  filter(groups==\"Existing Customer\") %&gt;%\n  ggplot()+\n  geom_point(aes(x=V1,y=V2),\n             alpha = 0.5,\n             size=2,\n             color = \"gray70\")+\n  geom_point(data = data_enc_pos,\n             aes(x=V1,y=V2, color = groups),\n             size=2,\n             color = \"blue\")+\n  labs(title = \"Visualisation of the samples in the two dimensional latent space\",\n       subtitle = subtitle_text_p1)+\n  theme_minimal()+\n  theme(\n    plot.subtitle = element_markdown()\n  )\n\n\n\n\n\n\n\n\n\nWhile we do not have a direct comparisson in explained variance, we can still see that the resulting lower dimensional data set seems more refined. However, we can use the resulting MSE to build a proxy \\(R^2\\): Since our data is \\(z\\)-normalized, i.e. \\(\\hat{\\mathbb{E}(X_j)}=0\\) and \\(\\hat{sd(X_j)}=1\\), the MSE measures the average deviation of the true values in “variance” units. A MSE of \\(1\\) implies that the autoencoder roughly explains none of the variance, meaning that the empiraical means are likely returned. A MSE of \\(0\\) implies that the autoencoder roughly all of the variance in the data is explained.\nMathematically, this can be explained as follows: In MLR \\(R^2\\) is defined as \\[\\begin{equation}\n  R^2 = 1-\\frac{SS_{\\text{residuals}}}{SS_{\\text{total}}}.\n\\end{equation}\\]\nwhere \\(SS_{\\text{residual}}\\) denotes the sum of squared residuals and \\(SS_{\\text{total}}\\) denotes the total sum of squares, i.e., the variance of the whole dataset. Since we \\(z\\)-normalized the data, \\(SS_{\\text{total}}=1\\)$, implying that \\(R^2=1-SS_{\\text{residual}}=1-MSE\\).\n\n\n\n10.1.2 Clustering\nUsing the lower dimensional representations of the data from the previous sections, we can apply some clustering algorithms to find out more about the data.\nIn simple terms, clustering aims to find subgroups in the data where members are similar to each other. How these subgroups are found is a bit more complex. There are a multitude of algorithms and metrices that can be used to describe how “far” different smaples are apart from each other and how they should be grouped.\nIn this session, we will consider the hard clustering algorithm \\(k\\)-means and density-based clustering algorithm DBSCAN. Both algorithms have their advantages and disadvanteges and as with any other machine learning model we considered so far, it is advisable to apply both algorithms and compare their performance to make a decision on which model to choose.\nEvaluating clustering methods is also not trivial. Since there is no target feature like in regression and classification, metrics like MSE or Accuracy do not make sense in this context. To measure how well a clustering algorithm performs, we can describe how homogeneous the clusters are.\nThe Silhouette Score measures the separation distance between clusters by taking values between \\([-1,1]\\). If an object is close to its own cluster and far from other clusters, the Silhouette Score is closer to \\(1\\). On the other hand, if an observation has a low score, it is an indicator that the sample does not fit with the assigned cluster. Reasons for poor scores could be too many or too few clusters.\nTo calculate the silhoutte score for each sample, we first have to calculate the mean distance between the considered sample \\(i\\) and every other data point in the same cluster:\n\\[\\begin{equation*}\n  a(i) := \\frac{1}{|C_I|-1}\\sum_{j\\in C_I, i\\neq j}d(i,j)\n\\end{equation*}\\]\nThen, we have to calculate the smallest mean distance of the considered sample \\(i\\) to all points in any other cluster:\n\\[\\begin{equation*}\n  b(i) :=\\min_{J\\neq I}{\\frac{1}{|C_{J}|}}\\sum_{j\\in C_{J}}d(i,j)\n\\end{equation*}\\]\nThe silhouette score for a sample \\(i\\) is then defined as:\n\\[\\begin{equation*}\n  s(i)={\\frac {b(i)-a(i)}{\\max\\{a(i),b(i)\\}}},\n\\end{equation*}\\]\nunder the assumption that \\(|C_I|&gt;1\\).\nFinding the optimal number of clusters has to be done manually with algorithms like \\(k\\)-means, whereas density based clustering algorithms determine the number of clusters on their own.\nThe {tidyclust} library includes algorithms like k-means and \\(k\\)-medoids. To apply density-based clustering algorithms like DBSCAN, we can use the {dbscan} library.\nFor this introduction, we will only use the low dimensional dataset produced by the autoencoder.\n\ndata_clust &lt;- data_enc %&gt;% \n  select(-groups)\n\ndata_clust %&gt;% ggplot(aes(x=V1,y=V2))+\n  geom_point(alpha = 0.5, size = 1.5)+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n10.1.2.1 \\(k\\)-means in R\nA \\(k\\)-means model can be trained in {tidymodels} fashion using the {tidyclust} library.\n\nlibrary(tidyclust)\n\nTo specify and tune a \\(k\\)-means model, the k_means function has to be called and the number of clusters has to be set to tune(). The engine \"stats\" is the default engine. The nstart argument specifies how many times the algorithm should choose random starting values.\n\nkmeans_spec &lt;- k_means(num_clusters = tune()) %&gt;%\n  set_engine(\"stats\", nstart = 5)\n\nrec_kmeans &lt;- recipe(~., data = data_clust)\n\nfolds &lt;- vfold_cv(data_clust)\n\nkmeans_wf &lt;- workflow() %&gt;%\n  add_recipe(rec_kmeans) %&gt;%\n  add_model(kmeans_spec)\n\nInstead of using the tune_grid function, the tune_cluster function can be used to tune the number of clusters. As for the metrics, thesse_ratio` function determins the ratio between the within-cluster-sum (wss) and sum of squared errors (sse).\n\ntune_res &lt;- kmeans_wf %&gt;%\n  tune_cluster(\n    grid = 10,\n    resamples = folds,\n    metrics =cluster_metric_set(sse_ratio)\n  )\n\nWe can plot the sse_ratio and determine the optimal number of clusters by the considering each angle enclosed by three points. The point that is enclosed in the smalles angle between two other points can be chosen as a candidate.\n\ntune_res %&gt;% collect_metrics() %&gt;%\n  filter(.metric == \"sse_ratio\") %&gt;%\n  ggplot(aes(x = num_clusters, y = mean)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal(base_size = 13) +\n  ylab(\"mean WSS/TSS ratio, over 5 folds\") +\n  xlab(\"Number of clusters\") +\n  scale_x_continuous(breaks = 1:10)\n\n\n\n\n\n\n\n\nGiven the plot above, it is not entirely clear which point is enclosed in the smallest angle. However, based on the information given, we can try num_clusters = 3. This is also referred to as the Elbow Method\nTo visualize the clusters, we can use the extract_cluster_assignment() function on the final fit and bind the clusters to the data as a new column.\n\nkmeans_spec_final &lt;- k_means(num_clusters = 3) %&gt;%\n  set_engine(\"stats\", nstart = 5)\n\nres_clust &lt;- kmeans_wf %&gt;%\n  update_model(kmeans_spec_final) %&gt;%\n  fit( data = data_clust)\n\nres_clust %&gt;%\n  extract_cluster_assignment() %&gt;%\n  cbind(data_clust) %&gt;%\n  ggplot(aes(x=V1,y=V2, color = .cluster)) +\n  geom_point(alpha = 0.5,size=1.5)+\n  theme_minimal(base_size = 13)+\n  coord_equal()\n\n\n\n\n\n\n\n\nGiven the clusters, we could now analyze each cluster on their own, however this is also kept as an exercise for the reader :-).\nTo build a Silhouette plot, consider the following code snippet:\n\ndists &lt;- data_clust %&gt;%\n  as.matrix() %&gt;%\n  dist()\n\nsilhouette(res_clust, dists = dists) %&gt;%\n  arrange(cluster,sil_width) %&gt;%\n  ggplot(aes(x=1:10127,y=sil_width, fill = cluster))+\n  geom_col()+\n  labs(x = \"sample number\",\n       y = \"Silhouette Score\")+\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\n\n10.1.2.2 DBSCAN in R\nFitting a DBSCAN model in R is also straightforward. The {dbscan} library contains functions that can make fitting a DBSCAN model easier. The {dbscan} algorithm has unfortunately not yet been incorporated in the {tidyclust} framework, so tuning the hyperparameters can be challenging.\n\nlibrary(dbscan)\n\nSince the dbscan() function requires the parameters eps and minPts as arguments, choosing their values without relying on tuning requires additional analyses.\nThe kNNdist() function calculates the \\(k\\)-nearest neighbor distances for a given dataset. Choosing \\(k=2\\) indicates, that the function returns the distance of every point to its k nearest neighbor. A larger distance between points and their respective neighbors indicates that a data point might be an outlier. We can visualize the distances with the following code snippet:\n\nkNNdist(data_clust,2) %&gt;%\n  as_tibble() %&gt;%\n  arrange(value) %&gt;%\n  mutate(x=1:nrow(.)) %&gt;%\n  ggplot(aes(x=x,y=value))+\n    geom_line()+\n    theme_minimal()\n\n\n\n\n\n\n\n\nThe point of the biggest increase in incline is usually referred to as the knee-point. This point can be used for the parameter eps. The parameter eps describes the radius of the neighborhood of every point.\nThe second parameter minPts describes how many points need to be in the \\(\\varepsilon\\) neighborhood in order for it to be core point.\n\ndbscan_res &lt;- dbscan(data_clust, eps = 0.65, minPts = 4)\n\ndbscan_res %&gt;%\n  pluck(\"cluster\") %&gt;%\n  as_factor() %&gt;%\n  cbind(data_clust) %&gt;%\n  as_tibble() %&gt;%\n  rename(.cluster=1) %&gt;%\n  ggplot(aes(x=V1,y=V2, color = .cluster)) +\n  geom_point()+\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "10_clustering.html#exercises",
    "href": "10_clustering.html#exercises",
    "title": "10  Unsupervised Learning",
    "section": "10.2 Exercises",
    "text": "10.2 Exercises\n\nlibrary(tidyclust)\n\n\n10.2.1 Dimension Reduction\n\nExercise 10.1 Given the covariance matrix \\(\\Sigma\\) of a sample \\(x_1,x_2,x_3\\in\\mathbb{R}^3\\)\n\\[\n\\Sigma = \\begin{pmatrix}\n            2 & 0 & 1\\\\\n            0 & 2 & 0\\\\\n            0 & 0 & 3\n         \\end{pmatrix},\n\\] calculate the principal components of this sample.\n\nConsider the following data set:\n\npen &lt;- palmerpenguins::penguins %&gt;%\n  select(-year) %&gt;%\n  na.omit()\n\nThe data set comprises various specific characteristics of a total of \\(K=333\\) penguins. Calling the glimpse() function on the data set yields the following overview:\n\npen %&gt;% glimpse()\n\nRows: 333\nColumns: 7\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe…\n\n\n\nExercise 10.2 Write a recipe that for a clustering pipeline that converts all nominal features into dummy variables and normalizes the underlying data.\nUse this recipe to create a preprocessed data set.\n\n\nExercise 10.3  \n\nUsing the functions described in Section 10.1.1.4, train an autoencoder with an architecture of your choice to map the transformed dataset on a two dimensional subspace.\nPlot the resulting two-dimensional dataset using a scatter plot.\n\n\n\nExercise 10.4  \n\nRepeat Exercise 10.3 but instead of training an autoencoder, reduce the dimension using PCA.\nHow many principal components are needed to explain at least \\(70\\%\\) of the variance in the dataset?\n\n\n\n\n10.2.2 Clustering\n\nExercise 10.5 Heuristically, describe the iterative nature of the k-means algorithm.\n\n\nExercise 10.6 Suppose you’re given the following Scree-Plot. Decide for an optimal number of clusters and justify your choice.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "10_clustering.html#solutions",
    "href": "10_clustering.html#solutions",
    "title": "10  Unsupervised Learning",
    "section": "10.3 Solutions",
    "text": "10.3 Solutions\n\nSolution 10.1 (Exercise 10.1). To calculate principal components, we first need to find the eigenvalues of \\(\\Sigma\\). The eigenvalues \\(\\lambda\\) are the solutions to the characteristic equation \\(\\text{det}(A - \\lambda I) = 0\\), where \\(I\\) is the identity matrix, and \\(\\text{det}\\) denotes the determinant.\nFor the given matrix \\(\\Sigma\\), the characteristic equation is given by: \\[\\begin{equation}\n  \\text{det}(\\Sigma - \\lambda I) = \\begin{vmatrix}\n              2 - \\lambda & 0 & 1\\\\\n              0 & 2 - \\lambda & 0\\\\\n              0 & 0 & 3 - \\lambda\n  \\end{vmatrix} = (2 - \\lambda)(2 - \\lambda)(3 - \\lambda)\n\\end{equation}\\]\nSetting this equation to zero and solving for \\(\\lambda\\), we find three eigenvalues:\n\\[\\begin{align*}\n  \\lambda_1 &= 2 \\\\\n  \\lambda_2 &= 2 \\\\\n  \\lambda_3 &= 3\n\\end{align*}\\]\nNext, we need to find the eigenvectors for each eigenvalue by solving the system of linear equations \\((\\Sigma - \\lambda I) \\nu = 0\\), where \\(\\nu\\) is the eigenvector corresponding to eigenvalue \\(\\lambda\\).\nFor \\(\\lambda = 2\\):\nNote, that for \\(\\lambda = 2\\) we need two eigenvectors, since the characteristic polynomial has a root of degree two at \\(\\lambda = 2\\). By substituting \\(\\lambda = 2\\) into \\((\\Sigma  - \\lambda I)\\nu = 0\\) and solving for \\(\\nu_{1,2}\\) we obtain:\n\\[\\begin{equation*}\n(\\Sigma - 2I)\\nu = \\begin{pmatrix}\n            0 & 0 & 1\\\\\n            0 & 0 & 0\\\\\n            0 & 0 & 1\n\\end{pmatrix} \\nu = 0.\n\\end{equation*}\\]\nThe general solution is:\n\\[\\begin{equation*}\n\\nu_1 = \\begin{pmatrix}\n            x_1\\\\\n            x_2\\\\\n            0\n\\end{pmatrix} \\quad\n\\nu_2 = \\begin{pmatrix}\n            x_3\\\\\n            x_4\\\\\n            0\n\\end{pmatrix},\n\\end{equation*}\\]\nwhere \\(x_1,x_2, x_3, x_4\\) are arbitrary real constants.\nFor \\(\\lambda = 3\\):\nSubstitute \\(\\lambda = 3\\) into \\((\\Sigma - \\lambda I)\\nu=0\\) and solve for \\(\\nu_3\\): \\[\\begin{equation*}\n  (\\Sigma - 3I)\\nu_3 = \\begin{pmatrix}\n              -1 & 0 & 1\\\\\n              0 & -1 & 0\\\\\n              0 & 0 & 0\n  \\end{pmatrix} \\nu_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\0 \\end{pmatrix}\n\\end{equation*}\\]\nThe general solution is:\n\\[\\begin{equation*}\n  \\nu_3 = \\begin{pmatrix}\n              x_5\\\\\n              0\\\\\n              x_6\n  \\end{pmatrix}\n\\end{equation*}\\]\nwhere \\(x_5\\) and \\(x_6\\) are arbitrary real constants with \\(x_5=x_6\\).\nWe have to ensure, that the vectors are orthonormal, i.e. \\(\\|\\nu_i\\| = 1\\) for \\(i=1,2,3\\) and \\(\\langle \\nu_i, \\nu_j\\rangle = 0\\) if \\(i\\neq j\\).\nSetting \\(x_1=...=x_5=1\\) and \\(x_6=-1\\) yields: \\[\\begin{equation*}\n  \\nu_1 =\\begin{pmatrix}\n              1\\\\\n              0\\\\\n              0\n  \\end{pmatrix}, \\quad\n  \\nu_2 = \\begin{pmatrix}\n              0\\\\\n              1\\\\\n              0\n  \\end{pmatrix} \\quad\n  \\nu_3 = \\begin{pmatrix}\n              -1\\\\\n              0\\\\\n              1\n  \\end{pmatrix}\n\\end{equation*}\\]\nThe given vectors \\(\\nu_1,\\nu_2\\) are already normalized but not orthogonal to vector \\(\\nu_3\\), i.e. \\(\\langle \\nu_1,\\nu_3\\rangle = -1 \\neq 0\\). To orthogonalize the three vectors, we have to employ an algorithm like Gram-Schmidt. This yields the principal components\n\\[\\begin{equation*}\n  \\tilde\\nu_1 =\\begin{pmatrix}\n              1\\\\\n              0\\\\\n              0\n  \\end{pmatrix}, \\quad\n  \\tilde\\nu_2 = \\begin{pmatrix}\n              0\\\\\n              1\\\\\n              0\n  \\end{pmatrix} \\quad\n  \\tilde\\nu_3 = \\begin{pmatrix}\n             0\\\\\n              0\\\\\n              1\n  \\end{pmatrix}\n\\end{equation*}\\]\nwhich are normal and orthogonal to each other.\n\n\nSolution 10.2 (Exercise 10.2). \n\nrec_pen &lt;- recipe(~.,data=pen) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_predictors())\n\npen_prepped &lt;- rec_pen %&gt;% prep() %&gt;% bake(pen)\n\n\n\nSolution 10.3 (Exercise 10.3). \n\n\n\nset.seed(6)\ntensorflow::set_random_seed(6)\n\naen &lt;- aen_spec(n_input = ncol(pen_prepped),\n                h1 = 256,\n                h2 = 128,\n                h3 = 64,\n                d1 = 0.2,\n                d2 = 0,\n                d3 = 0,\n                n_latent = 2)\n\ned_nn &lt;- aen %&gt;%\n        compile(\n        loss = 'mse',\n        optimizer = optimizer_adam(1e-4)\n      )\n\n\nhistory_aen &lt;- ed_nn %&gt;% fit(\n      as.matrix(pen_prepped),\n      as.matrix(pen_prepped),\n      validation_split = 0,\n      epochs = 100\n    )\n\n\n\ndata_enc &lt;- encoder %&gt;% predict(\n  as.matrix(\n    pen_prepped\n  )\n) %&gt;%\n  as_tibble()\n\n11/11 - 0s - 14ms/step\n\ndata_enc %&gt;%\n  ggplot(aes(x=V1,y=V2))+\n  geom_point(alpha = 0.5, size = 2)+\n  labs(\n    title = \"Two-dimensional representation of the penguins data set\"\n  )+\n  theme_minimal(base_size = 13)+\n  coord_fixed()\n\n\n\n\n\n\n\n\n\n\n\nSolution 10.4 (Exercise 10.4). \n\npca_fit &lt;- pen_prepped %&gt;%\n  prcomp() \n\ndata_reduced &lt;- pca_fit %&gt;%\n  augment(pen_prepped) %&gt;%\n  select(c(.fittedPC1,.fittedPC2))\n\n\np1 &lt;- data_reduced %&gt;%\n  ggplot(aes(.fittedPC1,.fittedPC2)) +\n  geom_point(size = 2, alpha = 0.5)+\n  labs(\n    title = \"Visualization of the first two &lt;br&gt; Principal Components\",\n    x = \"first PC\",\n    y = \"second PC\"\n    )+\n  theme_minimal(base_size = 11)+\n  theme(plot.title = element_markdown())\n\np2 &lt;- pca_fit %&gt;% tidy(matrix = \"eigenvalues\") %&gt;%\n  ggplot(aes(PC, percent)) +\n  geom_col(fill = \"#aab5ee\") +\n  scale_y_continuous(\n    labels = scales::percent_format(),\n    expand = expansion(mult = c(0, 0.01))\n  ) +\n  labs(\n    title = \"Visualisation of the explained variance &lt;br&gt;\n             per principal component\")+\n  theme_minimal()+\n  theme(plot.title = element_markdown())\n  \np1|p2 \n\n\n\n\n\n\n\n\n\npca_fit %&gt;% tidy(matrix = \"eigenvalues\")\n\n# A tibble: 9 × 4\n     PC std.dev percent cumulative\n  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1     1   2.03  0.456        0.456\n2     2   1.46  0.237        0.693\n3     3   1.19  0.157        0.85 \n4     4   0.845 0.0794       0.929\n5     5   0.488 0.0264       0.956\n6     6   0.414 0.0190       0.975\n7     7   0.315 0.011        0.986\n8     8   0.301 0.0101       0.996\n9     9   0.192 0.00409      1    \n\n\nTo explain at least \\(70\\%\\) of the variance, we need to consider the first three principal components.\n\n\nSolution 10.5 (Exercise 10.5). The iterative nature of the k-means algorithm finds its origin in the calculation of the means in each step: Initially, we set a predetermined number of centers randomly in the dataset and assign points to each center (cluster) for which the distance is minimal. Once every point has been assigned we calculate the mean of each cluster and set it as the new center. This iterative process is repeated until the algorithm has either converged (no new cluster assignments after a step), or a predetermined number of steps has been performed.\n\n\nSolution 10.6 (Exercise 10.6). Using the elbow method, we should choose 6 clusters, as the angle enclosing the 5th and 7th cluster at this point is the smallest.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "10_clustering.html#footnotes",
    "href": "10_clustering.html#footnotes",
    "title": "10  Unsupervised Learning",
    "section": "",
    "text": "If some of the features are perfectly correlated, there are less than \\(p\\) unique solutions↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "11_TextMining.html",
    "href": "11_TextMining.html",
    "title": "11  Text Mining",
    "section": "",
    "text": "11.1 Introduction\nIn this final exercise we will shorty consider different text mining methodologies.\nTextmining is becoming evermore important due to the abundance of available data and the ability to process it using modern machine learning architectures. Source for aggregated data sets are provided by Organisations like Common Crawl which have made it their goal to not only archive but also distribute all publicly available website on the internet. A corpus that focuses on distributing books, research papers, and all sorts of other literature is Anna’s Archive. However, the data published on this site usually violates local copyright laws which is why it should be used with caution.\nFor this exercise, we will use a publicly available dataset sourced from non other than the white house itself: whitehouse.gov releases every executive order placed by the POTUS in the past year.\nOur goal will be to analyse some of the wordings and get a feeling of the general sentiment.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Text Mining</span>"
    ]
  },
  {
    "objectID": "11_TextMining.html#proprocessing",
    "href": "11_TextMining.html#proprocessing",
    "title": "11  Text Mining",
    "section": "11.2 Proprocessing",
    "text": "11.2 Proprocessing\nBefore we dive into the exercises, make sure you have the jsonlite-package installed. This library allows us to read .json-files. These files are commonly used in databases and web applications since they allow for an intuitive storage of data that is not necessarily tabular.\n\ninstall.packages(\"jsonlite\")\n\n\nlibrary(jsonlite)\nlibrary(tidyverse)\nlibrary(tidytext)\n\nWe can then read the dataset white_house detailed_executive_order_dump.json.\n\ndata_wh &lt;- fromJSON(\"data/white_house_detailed_executive_order_dump.json\")\n\nThe Data is already in a tibble format which makes it easier to work with:\n\ndata_wh %&gt;% glimpse()\n\nRows: 144\nColumns: 5\n$ headline      &lt;chr&gt; \"ADDRESSING STATE AND LOCAL FAILURES TO REBUILD LOS ANGE…\n$ date          &lt;chr&gt; \"January 27, 2026\", \"January 23, 2026\", \"January 20, 202…\n$ url           &lt;chr&gt; \"https://www.whitehouse.gov/presidential-actions/2026/01…\n$ page_found_on &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ text          &lt;chr&gt; \"Presidential Actions\\n\\nADDRESSING STATE AND LOCAL FAIL…\n\n\nThe tibble contains five columns and 144 entries, meaning that the white house has published information about 144 different executive orders.\n\nThe column headline contains the headlines of the executive order.\ndate contains the date at which an execuitve order was published.\nurl contains the link to the article on the whitehouse webpage.\npage_found_on describes on which page index the article has been found.\nLastly, text contains the statement of the executive order encoded in a long string.\n\nAs a first preprocessing step, we convert the date column into a suitable datatype, since the type &lt;chr&gt; makes it hard to filter for specific date ranges.\n\ndata_wh &lt;- data_wh %&gt;% mutate(date = mdy(date)) \n\nSince each entry in the text column currently contains a long string, the next step is to split the strings at each line break.\n\ndata_wh %&gt;%\n  select(text) %&gt;%\n  slice(1) \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text\n1 Presidential Actions\\n\\nADDRESSING STATE AND LOCAL FAILURES TO REBUILD LOS ANGELES AFTER WILDFIRE DISASTERS\\n\\nExecutive Orders\\n\\nJanuary 27, 2026\\n\\nBy the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered:\\n\\nSection\\n\\n1\\n\\n.\\n\\nPurpose\\n\\n.  One year ago, the California State and Los Angeles city and county governments failed to contain wildfires that ravaged Los Angeles and consumed nearly 40,000 acres of homes and businesses.  The State and local governments failed to engage in responsible forest management systems out of a misguided commitment to naturalist and climate policies, which increased the severity of the fires.  They failed to maintain water distribution and reservoir systems so that these systems would be available and fully functional in case of emergency. They then failed to quickly communicate evacuation warnings and failed to act decisively or cohesively to contain the fire once it started burning.  In fact, Mayor Karen Bass was not in Los Angeles to respond to the crisis because she was traveling abroad.  This marked one of the greatest failures of elected political leadership in American history, from enabling the wildfires to failing to manage them, and it continues today with the abject failure to rebuild.\\n\\nWhile the Biden Administration made big promises, debris removal did not actually begin until my Administration, through Executive Order 14181 of January 24, 2025 (Emergency Measures to Provide Water Resources in California and Improve Disaster Response in Certain Areas), initiated the fastest debris-removal operation in United States history.  The Federal Government completed hazardous-materials sweeps and cleared over 9,500 properties of over 2.6 million tons of debris in just 6 months.\\n\\nBut since then, American families and small businesses affected by the wildfires have been forced to continue living in a nightmare of delay, uncertainty, and bureaucratic malaise as they remain displaced from their homes, often without a source of income, while State and local governments delay or prevent reconstruction by approving only a fraction of the permits needed to rebuild.\\n\\nThe Federal Government has approved numerous individual relief claims to provide financial support directly to owners of homes and businesses and help survivors repair, rebuild, return home, reopen their businesses, and restore their communities.  But many homeowners and businesses have been unable to use these funds as they navigate overly burdensome, confusing, and inconsistent permitting requirements, duplicative permitting reviews, procedural bottlenecks, and administrative delays at the city, county, and State levels.  Elected leaders have refused to take even the minimum action necessary to allow many of these survivors to move forward and rebuild their lives — the ultimate tragic failure of the State of California and City of Los Angeles to live up to their moral and legal obligations to their citizens.  As a result, despite the Federal Government expeditiously clearing debris and doing its part to support survivors, the actions of State and local authorities have ensured that the vast majority of the tens of thousands of homes and businesses destroyed in the wildfires have not yet been rebuilt a year later.\\n\\nIn furtherance of the Presidential Declaration of a Major Disaster for the State of California (FEMA-4856-DR), immediate and decisive Federal action is required to ensure that Federal disaster assistance is delivered and utilized swiftly, effectively, and without obstruction by State and local governments, to accomplish the purposes for which it is allocated, as well as to support the American people who have been devastated by the Pacific Palisades and Eaton Canyon wildfires.\\n\\nSec\\n\\n.\\n\\n2\\n\\n.\\n\\nPolicy\\n\\n.  It is the policy of my Administration that federally funded reconstruction projects for homes and businesses in the wildfire-impacted neighborhoods of the Pacific Palisades and Eaton Canyon areas proceed with the maximum speed consistent with public safety, and that Federal assistance not be frustrated by unnecessary, duplicative, or obstructive permitting requirements that prevent families and businesses from rebuilding.\\n\\nSec\\n\\n.\\n\\n3\\n\\n.\\n\\nPreempting State Permitting Obstacles\\n\\n.  (a)  The Secretary of Homeland Security (Secretary), acting through the Administrator of the Federal Emergency Management Agency (FEMA), and the Administrator of the Small Business Administration (SBA) shall each consider promulgating regulations to advance the policies of this order.  In particular, the Administrator of the SBA and the Secretary, through the Administrator of FEMA, shall consider promulgating regulations that:\\n\\n(i)   preempt State or local permitting processes, and other similar pre-approval requirements, that each agency has found to have unduly impeded the timely use of Federal emergency-relief funds by homeowners, businesses, or houses of worship in rebuilding such structures following a disaster; and\\n\\n(ii)  replace preempted State or local permitting regimes, or other similar pre-approval requirements, with a requirement that builders self-certify to a Federal designee from each agency that they have complied with all applicable substantive State and local health and safety standards with respect to the structure proposed to be rebuilt using Federal emergency-relief funds.\\n\\n(b)  The Secretary, through the Administrator of FEMA, and the Administrator of the SBA shall each publish proposed regulations under subsection (a) of this section, if any, within 30 days of the date of this order and final regulations within 90 days of the date of this order.  Each agency head shall further consider whether notice and comment is unnecessary under 5 U.S.C. 553 or any other statute.\\n\\n(c)  The Secretary, through the Administrator of FEMA, shall continue to review all repairs and construction activities conducted under this order for compliance with applicable health, safety, and other substantive standards.\\n\\nSec\\n\\n.\\n\\n4\\n\\n.\\n\\nExpediting Federal Response\\n\\n.  (a)  The Federal Government has already taken action to expedite administrative processes related to water delivery, as detailed in Executive Order 14181 of January 24, 2025 (Emergency Measures to Provide Water Resources in California and Improve Disaster Response in Certain Areas).  In addition, the heads of relevant executive departments and agencies (agencies) shall seek to use all authorities available under Federal environmental, historic preservation, natural resource laws, or other similar laws, including the National Environmental Protection Act (42 U.S.C. 4321\\n\\net seq.\\n\\n), the Endangered Species Act (16 U.S.C. 1531\\n\\net seq\\n\\n.), and the National Historic Preservation Act (54 U.S.C. 300101\\n\\net seq.\\n\\n), to expedite waivers, permits, reviews, consultations, or approvals with respect to homes, businesses, or other such structures proposed to be rebuilt using Federal emergency-relief funds that are required to facilitate Federal response and recovery actions that will advance the policy of this order, consistent with applicable law.\\n\\n(b)  The heads of relevant agencies shall take steps to ensure that the process for evaluating and issuing such waivers, permits, reviews, consultations, or approvals shall be limited to the minimum scope and duration required to expeditiously advance the policy of this order and implement Individual Assistance and Hazard Mitigation Grant Programs while ensuring public health and safety.\\n\\n(c)  The heads of relevant agencies shall each designate a senior official from their agency to ensure timely execution of these actions without delay.\\n\\nSec\\n\\n.\\n\\n5\\n\\n.\\n\\nLegislation\\n\\n.  Within 90 days of the date of this order, the Secretary, through the Administrator of FEMA, and the Administrator of the SBA, in consultation with the Assistant to the President for Domestic Policy and the White House Director of Legislative Affairs, shall submit to the President, through the Director of the Office of Management and Budget, legislative proposals that enable FEMA and SBA to address situations in which States or local governments are not enabling timely recovery after disasters, including through appropriate regulation.\\n\\nSec\\n\\n.\\n\\n6\\n\\n.\\n\\nAccountability for Use of Taxpayer Dollars\\n\\n.  (a)  The Secretary, through the Administrator of FEMA, shall:\\n\\n(i)   within 30 days of the date of this order, determine what amount, if any, of the nearly $3 billion in unspent Hazard Mitigation Grant Program (HMGP) funding granted to California, which was awarded to mitigate hazards, including the threat of future wildfires to the citizens of California, was awarded arbitrarily, capriciously, or contrary to law; and\\n\\n(ii)  within 60 days of the date of this order, conduct a Federal audit of California’s use of HMGP funding, including of whether funded projects were completed as approved and on time, whether projected risk reduction matched actual outcomes, and whether California used Federal funding in a way that demonstrably mitigated the impact of future wildfires on its citizens.\\n\\n(b)  Within 30 days of the completion of the audit described in subsection (a)(ii) of this section, the Secretary, through the Administrator of FEMA, shall make administrative determinations in light of the audit’s findings and recommendations, and shall enforce such determinations by, where appropriate, imposing future grant conditions, initiating recoupment or recovery actions in accordance with applicable law, or deploying oversight and technical assistance to expedite the administration and use of HMGP funds for individuals, families, and small businesses, to implement this order.\\n\\nSec\\n\\n.\\n\\n7\\n\\n.\\n\\nGeneral Provisions\\n\\n.  (a)  Nothing in this order shall be construed to impair or otherwise effect:\\n\\n(i)   the authority granted by law to an executive department or agency, or the head thereof; or\\n\\n(ii)  the functions of the Director of the Office of Management and Budget related to budgetary, administrative, or legislative proposals.\\n\\n(b)  This order shall be implemented consistent with applicable law and subject to the availability of appropriations.\\n\\n(c)  This order is not intended to, and does not, create any right or benefit, substantive or procedural, enforceable at law or in equity by any party against the United States, its departments, agencies, or entities, its officers, employees, or agents, or any other person.\\n\\n(d)  The costs for publication of this order shall be borne by the Department of Homeland Security.\\n\\nDONALD J. TRUMP\\n\\nTHE WHITE HOUSE,\\n\\nJanuary 23, 2026.\n\n\nA linebreak is indicated by the \\n symbol, so using the str_split()-function allows us to seperate the long string into a list of strings where each element contains exactly one line.\n\nstr_list &lt;- data_wh %&gt;%\n  select(text) %&gt;%\n  sapply(str_split,pattern=\"\\n\") \n\nThe list str_list now contains a list of length 144 where each entry contains another list of strings, i.e. the statement split by line breaks.\nTo combine this list of strings with the original dataset, we can simply remove the column text and add this newly created column str_list using the cbind()-function. As a last step, we remove the columns headline, url and page, since we are only interested in the dates and texts so far.\n\nwords_wh &lt;- data_wh %&gt;%\n  select(-text) %&gt;%\n  cbind(.,str_list) %&gt;% \n  select(-c(headline,url,page_found_on)) \n\nThe next step is to extract the relevant tokens from the text-column. As of now, each entry of the text-column is still a list, which is why the first step of extracting the word tokens is converting the lists into individual elements. Since each element is an individual line, the dataframe now contains as many rows as there are lines of text in the whole dataset.\n\nwords_wh &lt;- words_wh %&gt;%\n  unnest(text) \n\nwords_wh %&gt;% glimpse()\n\nRows: 25,038\nColumns: 2\n$ date &lt;date&gt; 2026-01-27, 2026-01-27, 2026-01-27, 2026-01-27, 2026-01-27, 2026…\n$ text &lt;chr&gt; \"Presidential Actions\", \"\", \"ADDRESSING STATE AND LOCAL FAILURES …\n\n\nWe now want to extract the word tokens using the unnest_tokens function. The output argument specifies the name of the resulting column containing the tokens. The input-argument specifies which column of the data frame contains the raw data. Lastly the token-argument specifies what kind of token shall be extracted. We will stick to words for now.\n\nwords_wh &lt;- words_wh %&gt;%\n  unnest_tokens(\n    output = word,\n    input = text,\n    token = \"words\"\n    )\n\nwords_wh %&gt;% glimpse()\n\nRows: 199,433\nColumns: 2\n$ date &lt;date&gt; 2026-01-27, 2026-01-27, 2026-01-27, 2026-01-27, 2026-01-27, 2026…\n$ word &lt;chr&gt; \"presidential\", \"actions\", \"addressing\", \"state\", \"and\", \"local\",…\n\n\nThe new dataset contains roughly 200k words, of which a majority are so-called stop words like \"and\", \"or\", and \"which\". Since these words only offer little information, we want to filter them directly. Luckily the stop_words dataset contains a list of all commonly used stop words. We can use the anti_join()-function to remove these from the word-column.\n\ndata(\"stop_words\")\n\nwords_wh_filtered &lt;- words_wh %&gt;%\n  anti_join(stop_words)\n\nglimpse(words_wh_filtered)\n\nRows: 105,029\nColumns: 2\n$ date &lt;date&gt; 2026-01-27, 2026-01-27, 2026-01-27, 2026-01-27, 2026-01-27, 2026…\n$ word &lt;chr&gt; \"presidential\", \"actions\", \"addressing\", \"local\", \"failures\", \"re…\n\n\nThe words_wh_filtered dataset now contains only roughly 105k words, meaning we reduced the size by approximately 50%.\nSay, we want to get an overview of the most used nouns in the dataset. We can use another dataset called parts_of_speech which contains commonly used words and their respective type.\n\ndata(\"parts_of_speech\")\nglimpse(parts_of_speech)\n\nRows: 208,259\nColumns: 2\n$ word &lt;chr&gt; \"3-d\", \"3-d\", \"4-f\", \"4-h'er\", \"4-h\", \"a'\", \"a-1\", \"a-axis\", \"a-b…\n$ pos  &lt;chr&gt; \"Adjective\", \"Noun\", \"Noun\", \"Noun\", \"Adjective\", \"Adjective\", \"N…\n\n\nThe semi_join()-function behaves similiraly to the filter()-function, allowing us to only retain words in the word column that are present in the nouns of the parts_of_speech-dataset.\n\nnouns_wh_filtered &lt;- words_wh_filtered %&gt;%\n    semi_join(\n      parts_of_speech %&gt;%\n        filter(str_detect(pos,\"Noun\")), \n      by = \"word\"\n  )\n\nnouns_wh_filtered %&gt;% glimpse()\n\nRows: 51,116\nColumns: 2\n$ date &lt;date&gt; 2026-01-27, 2026-01-27, 2026-01-27, 2026-01-27, 2026-01-27, 2026…\n$ word &lt;chr&gt; \"local\", \"wildfire\", \"january\", \"authority\", \"president\", \"consti…\n\n\nLet us now visualize the most frequently used nouns. We define “most frequently” in terms of the \\(99\\%\\)-quantile, meaning that only words should be contained that appear in the \\(99\\%\\)-quantile of the frequency. Using a column-plot, we can neatly visualize the most frequently used nouns:\n\nnouns_wh_filtered %&gt;%\n  count(word, sort=TRUE) %&gt;%\n  filter(\n    n&gt;quantile(n,0.99)\n    ) %&gt;%\n  mutate(word = reorder(word,n)) %&gt;%\n  ggplot(aes(x=word,y=n))+\n  geom_col()+\n  coord_flip()+\n  labs(y=NULL)+\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Text Mining</span>"
    ]
  },
  {
    "objectID": "11_TextMining.html#frequency-tables",
    "href": "11_TextMining.html#frequency-tables",
    "title": "11  Text Mining",
    "section": "11.3 Frequency Tables",
    "text": "11.3 Frequency Tables\nNow that we have a broad overview of the most frequently used words, we now want to get a feeling for the importance of the different tokens.\nThe term frequency measures the relative frequency of a term within a certain document. In our example, we will consider each month a document. Furthermore, to reduce computational effort, we will still only use the most frequently used terms.:\n\nnouns_to_filter &lt;- nouns_wh_filtered %&gt;%\n  count(word, sort=TRUE) %&gt;%\n  filter(\n    n&gt;quantile(n,0.99)\n    ) %&gt;%\n  select(word)\n\nTo compute the term frequency, we have to proceed as follows:\n\nFilter for the most frequently used terms using the filter()-function. This is equivalent to using the semi_join()-statemtn.\nNext, we have have to extract the month from the date column using the month()-function.\nBy grouping for the month and word column we create groups for each words occurence in the respective month.\nUsing the summarise(n=n())-function returns the absolute frequency of each word in the respective month.\nTo create relative frequencies, we can simply use the mutate()-function. Since the data is still grouped, the sum()-function is also applied group-wise, meaning that the return value is the relative frequency with respect to the different months.\nThen, to create the frequency table, we use the pivot_wider()-function, that pivots the long table into a wider format. The wider format creates a column for each value specified with the names_from argument. Setting it to month implies, that the wider format will have 12 different columns, one for each month The argument values_from specifies how the different rows of the columns shall be filled. Setting it to rel_frequency yields the relative frequencies of each row in the respective month.\nAs a last step, we have to take care of the case when words only occur in a certain month. The wider table format currently treats these entries in the months where a word does not occur as NA values. Ideally, we would like to encode them with 0 to indicate that they did not occur in the specific month. We can do that by using the mutate()-function across every column and replace the value NA with 0.\n\n\ntbl_freq &lt;- nouns_wh_filtered %&gt;%\n  filter(word %in% pull(nouns_to_filter)) %&gt;%\n  mutate(month = month(date)) %&gt;%\n  group_by(month,word) %&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(rel_freq = n/sum(n)) %&gt;%\n  select(word,month,rel_freq) %&gt;%\n  pivot_wider(names_from = month, values_from = rel_freq) %&gt;%\n  mutate(across(everything(),~replace_na(.,0)))\n\ntbl_freq %&gt;% head(4)\n\n# A tibble: 4 × 13\n  word       `1`    `2`    `3`    `4`    `5`     `6`    `7`    `8`    `9`   `10`\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 act     0.0228 0      0.0145 0.0351 0.0180 0.0519  0.0224 0.0230 0.0589 0     \n2 action  0.0180 0      0.0135 0.0171 0.0150 0.00985 0.0202 0.0118 0.0161 0     \n3 agency  0.0516 0.0233 0.0518 0.0362 0.0553 0.0179  0.0229 0.0439 0.0152 0.107 \n4 americ… 0.0264 0.0388 0.0264 0.0252 0.0234 0.0188  0.0186 0.0292 0.0116 0.0194\n# ℹ 2 more variables: `11` &lt;dbl&gt;, `12` &lt;dbl&gt;\n\n\nInterestingly, neither the word act nor action seem to have been mentioned in the month “February”. In January, every 20-th noun in the most frequently used word list was agency, since it’s relative frequency is approximately 0.05.\nTo compute the tf-idf score which measures the importance of words in a specific document, we now have to calculate the inverse document frequency defined as\n\\[\\begin{equation*}\n  \\text{idf} = \\begin{pmatrix}\n      \\log\\left(\\frac{N}{\\text{df}_1}\\right),...,\\log\\left(\\frac{N}{\\text{df}_K}\\right)\n  \\end{pmatrix}\n\\end{equation*}\\]\nNote, that \\(\\text{idf}\\) is a vector with as many entries as words, i.e. \\(\\text{idf}_i =\\log(\\frac{N}{\\text{df}_i})\\) where \\(N\\) is the total number of documents.\nThe calculation of the inverse document frequency extends the computation of the document frequency. Since the first four steps are the same, we will continue in line 3.\n\nInstead of grouping by month and word, we now group by word and month, meaning, that there is a group for each word.\nSummarising the groups with respect to the occurances of each term, we obtain a tibble containing the frequency of each word in the respective month.\nThis tibble needs to be grouped by the word again, since we are interested in the number of months in which the respective word occured.\n\n\ninv_doc_freq &lt;- nouns_wh_filtered %&gt;%\n  filter(word %in% pull(nouns_to_filter)) %&gt;%\n  mutate(month = month(date)) %&gt;%\n  group_by(word,month) %&gt;%\n  summarise(n = n()) %&gt;%\n  group_by(word) %&gt;%\n  summarise(n_month=n())\n\ninv_doc_freq %&gt;% head()\n\n# A tibble: 6 × 2\n  word      n_month\n  &lt;chr&gt;       &lt;int&gt;\n1 act            10\n2 action         10\n3 agency         12\n4 american       12\n5 authority      12\n6 date           12\n\n\nNow, that we have obtained the number of months in which each word occurs, we can calculate the log-ratio by applying the mutate function:\n\ninv_doc_freq &lt;- inv_doc_freq %&gt;%\n  mutate(inv_doc_freq = log(12/n_month)) %&gt;%\n  select(word,inv_doc_freq)\n\ninv_doc_freq %&gt;% head()\n\n# A tibble: 6 × 2\n  word      inv_doc_freq\n  &lt;chr&gt;            &lt;dbl&gt;\n1 act              0.182\n2 action           0.182\n3 agency           0    \n4 american         0    \n5 authority        0    \n6 date             0    \n\n\nAs a last step, we have to compute the tf-idf weights by multiplying the resulting vector inv_doc_freq elementwise with each column of the termfrequency matrix. This is most easily done by using the * on the matrix representations of the respective data frames:\n\nm_tbl_freq &lt;- tbl_freq %&gt;%\n  select(-word) %&gt;%\n  as.matrix()\n\nm_inv_doc_freq &lt;- inv_doc_freq %&gt;%\n  select(-word) %&gt;%\n  as.matrix()\n\ntf_idf &lt;- (m_tbl_freq * as.vector(m_inv_doc_freq)) \n\nSince the matrix format lacks proper display properties, we can convert it back to a tibble and add the words as an initial column:\n\ntf_idf &lt;- tf_idf %&gt;%\n  as_tibble() %&gt;%\n  cbind(\n    inv_doc_freq %&gt;% select(word),\n    .\n        )\n\ntf_idf %&gt;% head()\n\n       word           1 2           3           4           5           6\n1       act 0.004158595 0 0.002645080 0.006398104 0.003285073 0.009467010\n2    action 0.003283101 0 0.002456145 0.003125791 0.002737561 0.001795467\n3    agency 0.000000000 0 0.000000000 0.000000000 0.000000000 0.000000000\n4  american 0.000000000 0 0.000000000 0.000000000 0.000000000 0.000000000\n5 authority 0.000000000 0 0.000000000 0.000000000 0.000000000 0.000000000\n6      date 0.000000000 0 0.000000000 0.000000000 0.000000000 0.000000000\n            7           8           9 10          11          12\n1 0.004080341 0.004189841 0.010734365  0 0.004188091 0.004356949\n2 0.003682259 0.002158403 0.002927554  0 0.003071267 0.002010900\n3 0.000000000 0.000000000 0.000000000  0 0.000000000 0.000000000\n4 0.000000000 0.000000000 0.000000000  0 0.000000000 0.000000000\n5 0.000000000 0.000000000 0.000000000  0 0.000000000 0.000000000\n6 0.000000000 0.000000000 0.000000000  0 0.000000000 0.000000000\n\n\nThe resulting dataframe contains for each word the \\(\\text{tf-idf}\\) weight in the respective document.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Text Mining</span>"
    ]
  },
  {
    "objectID": "11_TextMining.html#sentiment-analysis",
    "href": "11_TextMining.html#sentiment-analysis",
    "title": "11  Text Mining",
    "section": "11.4 Sentiment Analysis",
    "text": "11.4 Sentiment Analysis\n\ninstall.packages(\"textdata\")\nget_sentiments(\"afinn\")\n\nTBD",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Text Mining</span>"
    ]
  }
]