[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Mining √úbungsmaterial",
    "section": "",
    "text": "Vorwort\nDieses √úbungsmanuskript erg√§nzt das Vorlesungsskript zur Vorlesung Data Mining von Prof.¬†Dr.¬†Yarema Okhrin an der Universit√§t Augsburg. Da dieses Manuskript noch in Arbeit ist, sind √Ñnderungen unter Vorbehalt m√∂glich und wahrscheinlich.",
    "crumbs": [
      "Vorwort"
    ]
  },
  {
    "objectID": "index.html#warum-sollte-ich-mir-r-anschauen",
    "href": "index.html#warum-sollte-ich-mir-r-anschauen",
    "title": "Data Mining √úbungsmaterial",
    "section": "Warum sollte ich mir R anschauen?",
    "text": "Warum sollte ich mir R anschauen?\nIn fast allen √úbungen werden wir mit der (statistischen) Programmiersprache R arbeiten. Data Mining lebt von der Arbeit mit Daten ‚Äì sei es die explorative Datenanalyse (EDA) oder das Entwickeln von statistischen Modellen f√ºr Vorhersagen. R ist besonders f√ºr EDA geeignet, da es sich durch verschiedene Zusatzpakete (Libraries) wie das Tidyverse und Tidymodels auszeichnet. Zum erfolgreichen Abschlie√üen dieses Moduls ist es deshalb unerl√§sslich, dass man sich in der Vorlesung sowie der √úbung mit der Programmiersprache R auseinandersetzt. Sowohl die theoretischen Konzepte als auch die Anwendung sollen hierbei durch die verschiedenen √úbungen vertieft werden.",
    "crumbs": [
      "Vorwort"
    ]
  },
  {
    "objectID": "index.html#hilfe-mein-code-macht-nicht-das-was-ich-will",
    "href": "index.html#hilfe-mein-code-macht-nicht-das-was-ich-will",
    "title": "Data Mining √úbungsmaterial",
    "section": "Hilfe, mein Code macht nicht das, was ich will!",
    "text": "Hilfe, mein Code macht nicht das, was ich will!\nDebugging, auch bekannt als die Kunst, herauszufinden, warum sich der Code nicht wie erwartet verh√§lt, spielt beim Programmieren eine wesentliche Rolle. Auch nach jahrelanger √úbung werden (gelegentlich) Fehler beim Programmieren auftreten.\nEs gibt viele, mehr oder weniger effektive, Ans√§tze zur L√∂sung von programiertechnischen Problemen im Rahmen dieses Kurses. Die meisten Probleme werden von der Form\n\nIch habe den Code bei mir in ein R Skript kopiert und es funktioniert nicht.\n\nsein.\nDie meisten dieser Fehler bzw. Probleme lassen sich mithilfe der folgenden vier Ans√§tze l√∂sen. Wichtig an dieser Stelle ist zu erw√§hnen, dass die meisten Foren, Websites und Dokumentationen f√ºr R in englischer Sprache verfasst wurden, weshalb es sich lohnt, die Fehler in der englischen Form zu untersuchen.\n\nViele der h√§ufig auftretenden Fehlermeldungen wurden wahrscheinlich schon in Foren wie Stack Overflow diskutiert. Alternativ k√∂nnen die Suchergebnisse auch Websites wie DataCamp oder GeeksforGeeks enthalten. Diese Websites bieten f√ºr die meisten Probleme und Fehler eine schnelle L√∂sung, welche in der Regel einfach exakt wie vorgeschlagen in die eigenen Codeskripte √ºbernommen werden kann.\nVerwendung eines gro√üen Sprachmodells wie leChat. Wenn es um Fragen zur Fehlersuche oder zur Programmierung im Allgemeinen geht, sind Sprachmodelle hilfreich. Indem man eine Frage als Prompt eingibt, wird in den meisten F√§llen die Ausgabe der Sprachmodelle bereits eine zufriedenstellende Antwort liefern.\nDie interne ‚ÄûHilfe‚Äú-Funktion mag zwar als altmodisch gelten, ist aber ein √§u√üerst effektiver Ansatz zur Fehlersuche. Sie kann wertvolle Erkenntnisse liefern, wenn sich eine Funktion nicht wie erwartet verh√§lt. Wenn sich eine Funktion nicht wie erwartet verh√§lt, √∂ffnet die Eingabe von ?Funktionsname im Konsolen-Panel die entsprechende Hilfeseite. Der Abschnitt arguments auf der Hilfeseite erkl√§rt die verschiedenen Eingabeparameter einer Funktion. Der Abschnitt value beschreibt, was f√ºr einen R√ºckgabewert die Funktion hat. Beispiele f√ºr die Verwendung einer Funktion k√∂nnen im letzten Abschnitt der Hilfe Funktion gefunden werden.\nWenn jeder der oben genannten Schritte fehlschl√§gt, k√∂nnen auch Fragen im entsprechenden Digicampus-Kurs gestellt werden.",
    "crumbs": [
      "Vorwort"
    ]
  },
  {
    "objectID": "index.html#aufbau-des-manuskripts",
    "href": "index.html#aufbau-des-manuskripts",
    "title": "Data Mining √úbungsmaterial",
    "section": "Aufbau des Manuskripts",
    "text": "Aufbau des Manuskripts\nJedes Kapitel beginnt, je nach Komplexit√§t des Themas, mit einer mehr oder weniger technischen Zusammenfassung und Motivation. Nach diesen Zusammenfassungen folgen Beispiele, welche die Funktionen und Konzepte erg√§nzen. Die √úbungsaufgaben und L√∂sungen finden sich im Anschluss an diese Beispiele.\n\nIm ersten Kapitel werden wir die wichtigsten R-Konzepte wiederholen, welche in der √úbung und Vorlesung regelm√§√üig angewendet werden.\nIm zweiten Kapitel betrachten wir die einfache und die multiple lineare Regression. Wir gehen dabei auf die Annahmen ein, diskutieren, warum diese wichtig sind und wie man pr√ºft, ob diese Annahmen bei einem gegebenen Datensatz erf√ºllt sind, oder nicht.\nIm dritten Kapitel setzen wir uns mit essenziellen Techniken des Data Mining auseinander, welche direkt mit dem Entwickeln von Modellen verwandt sind. Es geht hierbei um das Aufteilen der Daten in Trainings- und Testdaten, was robustere Vorhersagen f√ºr zuvor ungesehene Auspr√§gungen erm√∂glicht. Au√üerdem f√ºhren wir die Imputation von fehlenden Daten sowie die effiziente Merkmalsselektion ein.\nDas vierte Kapitel besch√§ftigt sich mit Regressionsb√§umen. Wir lernen, wie man einen Regressionsbaum auf verschiedene Arten darstellen kann, f√ºhren f√ºr das Fitten eines Baumes den binary-splitting-Algorithmus ein und lernen, wie man mit Regressionsb√§umen in R umgeht. Zus√§tzlich diskutieren wir noch verschiedene Kriterien, welche beschreiben, wie gut ein gesch√§tztes Baummodell ist.\nAbschlie√üend setzen wir uns im Zuge der Regressionsaufgaben mit neuronalen Netzen auseinander. Wir gehen hierbei auf die grundlegenden Bausteine eines neuronalen Netzes ein, lernen, wie die Parameter eines einfachen neuronalen Netzes angepasst werden, und welche Rolle hierbei verschiedene Hyperparameter spielen.\nIm sechsten Kapitel setzen wir uns mit Klassifikationsaufgaben auseinander. Beginnend mit der logistischen Regression und Klassifikationsb√§umen und final mit neuronalen Netzen, diskutieren wir verschiedene Modellans√§tze. Wir besch√§ftigen uns au√üerdem intensiv mit der Herleitung verschiedener Metriken f√ºr Klassifikationsaufgaben und damit, wie diese zu interpretieren sind.\nNachdem wir mit dem Abschluss des sechsten Kapitels einen groben Einblick in das supervised Learning bekommen haben, wollen wir im finalen siebten Kapitel noch einen Einblick in das unsupervised Learning bekommen. Im Zuge des unsupervised Learnings setzen wir uns prim√§r mit dem Clustering auseinander. Wir diskutieren hierbei zuerst verschiedene Distanz- und √Ñhnlichkeitsma√üe und untersuchen dann hierarchische Clusterverfahren.",
    "crumbs": [
      "Vorwort"
    ]
  },
  {
    "objectID": "01_Einfuehrung.html",
    "href": "01_Einfuehrung.html",
    "title": "1¬† (Kurz)Einf√ºhrung in R",
    "section": "",
    "text": "1.1 Installieren von R und RStudio\nIn diesem Abschnitt wiederholen wir kurz die Installation von R und RStudio. Obwohl R (die Programmiersprache) mit einer vorinstallierten grafischen Benutzeroberfl√§che (GUI) installiert werden kann, werden wir stattdessen RStudio. RStudio kommt mit einigen Vorteilen gegen√ºber der internen R Gui, wie zum Beispiel das einsehen von Variablenwerten, Plots und Programmcode gleichzeitig. Es ist allerdings wichtig zu beachten, dass RStudio nur dann funktioniert, wenn R zuvor auch installiert wurde.\nNach der erfolgreichen Installation von R und RStudio sollte sich beim Start von RStudio ein Fenster √∂ffnen, das in etwa so aussieht wie das folgende (vgl. RStudio User Guide).\nDas Source Panel zeigt ein .R File namens ggplot2.R. .R Dateien sind das standard R Format in welchen Code geschrieben und gespeichert wird. Eine Variante die mehr √Ñsthetik besitzt sind sogenannte Quarto Markdown Files (Endung .qmd) welche erlauben Text und Code zu kombinieren. Zum Beispiel wurde dieses Manuskript komplett mit Quarto geschrieben üòÅ .\nNat√ºrlich muss man sich nicht zwingend in Quarto einlesen und kann die folgenden Aufgaben auch in normalen .R Skripten ausf√ºhren.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>(Kurz)Einf√ºhrung in R</span>"
    ]
  },
  {
    "objectID": "01_Einfuehrung.html#installieren-von-r-und-rstudio",
    "href": "01_Einfuehrung.html#installieren-von-r-und-rstudio",
    "title": "1¬† (Kurz)Einf√ºhrung in R",
    "section": "",
    "text": "R (f√ºr Windows) kann hier heruntergeladen werden.\nRStudio kann hier heruntergeladen werden.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKurzeinf√ºhrung Quarto\n\n\n\n\n\nQuarto ist in RStudio bereits vorinstalliert, weshalb das Anlegen von neuen Dokumenten sehr leicht ist.\n\nIn RStudio, klicke auf New File -&gt; Quarto Document... im File File tab \nWenn das New Quarto Document Fenster sich √∂ffnet kann man das Outputformat w√§hlen. Das HTML output ist sehr umg√§nglich und kann mit jedem Browser verwendet werden. Durch das ausw√§hlen des create Knopfes √∂ffnet sich dann ein neues Quarto Dokument. \nIm Source Panel wird nun ein Quarto-Beispieldokument angezeigt, welches bearbeitet werden kann. Eventuell muss noch rmarkdown installiert werden, um eine korrekte Darstellung erzeugen zu k√∂nnen (vgl. Markierung 1). Das Quarto-Dokument kann dann durch Anklicken der entsprechenden Abschnitte ge√§ndert werden. R-Code-Zellen k√∂nnen durch Klicken auf den gr√ºnen Pfeil ausgef√ºhrt werden (vgl. Markierung 2). Um neue R-Code-Zellen zwischen Abs√§tzen einzuf√ºgen, kann √ºber die die Registerkarte Einf√ºgen Ausf√ºhrbare Zelle -&gt; R (vgl. Markierung 3) eine neue Code-Zelle . \n\nF√ºr weitere Informationen siehe z.B. Quarto Guide.\n\n\n\n\n\n1.1.1 Arbeiten mit Arbeitspfaden und R Projekten\nNach dem √ñffnen von RStudio kann durch den Befehl getwd() im Konsolen Panel der Pfad des aktuellen Arbeitsverzeichnis ausgegeben werden. Das Arbeitsverzeichnis ist zum Beispiel dann wichtig, wenn wir neue Dateien anlegen, oder Datens√§tze einlesen wollen. Wenn der R√ºckgabewert des Befehls getwd() zum Beispiel \"C:/Users/lachlinu/Documents/Vorlesungen/Data_Mining/\" lautet, kann R auf jede Datei im Verzeichnis Data_Mining zugreifen. Eine M√∂glichkeit, das Arbeitsverzeichnis zu √§ndern, ist die Verwendung des Befehls setwd(). Hierbei ist wichtig, entweder den absoluten neuen Pfad, oder den relativen neuen Pfad anzugeben. Falls das Ziel ist, in das /Documents Verzeichnis zu wechseln, so kann man durch die Angabe des relativen Pfades setwd('../../../') das Verzeichnis /Documents ausw√§hlen, sofern das aktuelle Verzeichnis \"C:/Users/lachlinu/Documents/Vorlesungen/Data_Mining/\" ist. Die ../ Notation wird verwendet um das √ºbergeordnete Verzeichnis auszuw√§hlen. Alternativ kann man auch einfach den absoluten Pfad \"C:/Users/lachlinu/Documents\" in den setwd() Befehl einf√ºgen um das Verzeichnis entsprechend zu wechseln. Das manuelle √Ñndern des Verzeichnisses in jedem .R Skript kann schnell m√ºhsam werden, daher ist die Einrichtung eines Projekts eine praktischere Alternative. RStudio-Projekte erm√∂glichen die Erstellung eines individuellen Arbeitsverzeichnisses f√ºr mehrere Kontexte. Zum Beispiel k√∂nnen wir R nicht nur zum L√∂sen von Aufgaben f√ºr Data Mining verwenden, sondern auch f√ºr Abschlussarbeiten. Wenn man also zwei verschiedene Projekte einrichtet, kann man die Arbeitsverzeichnisse und Arbeitsbereiche f√ºr jedes Projekt individuell organisieren.\nDas folgende Beispiel illustriert wie man ein Projekt einrichtet und verwendet:\n\n√úber den File Tab, w√§hle New Project....\n\n\n\n\n\nDurch das w√§hlen von Existing Directory kann man durch den Explorer ein Verzeichnis w√§hlen welches bereits existiert und dann durch den Create Project button ein neues Projekt in diesem Verzeichnis anlegen.\n\n\n\n\n\nDas neue Projekt kann dann einfach angeklickt werden, wodurch sich ein neues RStudio Fenster √∂ffnen sollte.\n\n\n\n\n\n\nSobald sich √ºber das anklicken des Projekts ein neues RStudio Fenster ge√∂ffnet hat wird durch das Ausf√ºhrens des getwd() Befehls der Arbeitspfad des Projekts ausgegeben. Nun kann auch jede Datei in diesem Verzeichnis direkt verwendet werden.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>(Kurz)Einf√ºhrung in R</span>"
    ]
  },
  {
    "objectID": "01_Einfuehrung.html#einfache-r-befehle-und-funktionen",
    "href": "01_Einfuehrung.html#einfache-r-befehle-und-funktionen",
    "title": "1¬† (Kurz)Einf√ºhrung in R",
    "section": "1.2 Einfache R Befehle und Funktionen",
    "text": "1.2 Einfache R Befehle und Funktionen\nNach der erfolgreichen Installation von R und RStudio widmen wir uns nun ein paar grundlegenden Programmierkonzepten in R.\n\n1.2.1 Vektoren, Datens√§tze und Rechenoperationen\n\n1.2.1.1 Vektoren\nVariablen wie Vektoren, oder Arrays werden in R mit dem &lt;- erstellt. Falls zum Beispiel das Ziel ist, einen Vektor zu erstellen, welcher die Werte \\(1,2,3,4,5\\) enth√§lt, gibt es viele Verschiedene M√∂glichkeiten dies zu erreichen.\n\nx1 &lt;- c(1,2,3,4,5)\nx2 &lt;- 1:5\nx3 &lt;- seq(1,5)\n\nJeder der obigen Vektoren enth√§lt die Zahlen 1-5. x1 wurde durch die c() Funktion erstellt, welche einen Vektor entsprechend der √ºbergebenen Argumente erzeugt. x2 wurde mithilfe des : Operators erstellt, welcher einer ganzzahlige Sequenz der Zahlen beginnend mit der Zahl links des Operators bis zur Zahl rechts des Operators erzeugt. Die Erzeugung des Vektors x3 basiert auf der selben Idee wie die Erstellung des Vektors x2, allerdings wurde hier explizit die Funktion seq (Sequence) verwendet um die Zahlenfolge zu generieren. Die seq Funktion erlaubt im Gegensatz zur Verwendung des : Operators einige weitere Argumente wie zum Beispiel das Argument by, oder length.out.\nDer folgende Befehl erzeugt beispielsweise eine Sequenz von 1 bis 5 mit Schrittgr√∂√üe \\(0.5\\).\n\nseq(from = 1,to = 5,by = 0.5)\n\n[1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\n\nDurch das Spezifizieren von length.out kann man die L√§nge der Sequenz steuern. So wird im folgenden Beispiel eine Sequenz von Zahlen zwischen 1 und 5 erzeugt mit der L√§nge 20.\nDie Zuweisung von Werten in Funktionen erfolgt im obigen Beispiel mit dem Symbol =. Die Zuweisung neuer Variablen au√üerhalb von Funktionen kann auch mit dem = Symbol erfolgen, aber es wird selten verwendet und au√üer in einigen pathologischen F√§llen gibt es keinen Unterschied. Die meisten R-Benutzer bevorzugen jedoch die Zuweisung von Umgebungsvariablen mit &lt;-, was in Funktionsaufrufen nicht funktioniert.\n\nseq(1,5,length.out = 20)\n\n [1] 1.000000 1.210526 1.421053 1.631579 1.842105 2.052632 2.263158 2.473684\n [9] 2.684211 2.894737 3.105263 3.315789 3.526316 3.736842 3.947368 4.157895\n[17] 4.368421 4.578947 4.789474 5.000000\n\n\n\n\n\n\n\n\nTip\n\n\n\nDie Argumente einer Funktion m√ºssen nicht immer √ºbergeben werden. Im obigen Beispiel wurden die Benennungen \"from=\" und \"to=\" weggelassen. Das weglassen der Argumentnamen beim Aufruf der Funktion ist lediglich der Faulheit der programmierenden Person geschuldet. In den meisten F√§llen lohnt es sich Zwecks Lesbarkeit diese zu benennen.\n\n\nDer Abstand zwischen den einzelnen Folgenglieder der Folge ist hierbei gegeben durch\n\\[\n\\frac{5-1}{20-1} = \\frac{4}{19}.\n\\] Im Z√§hler ist steht die L√§nge des Intervalls welche durch obere Grenze - untere Grenze berechnet wird und im Nenner Anzahl der Folgeglieder - 1.\nDas Zugreifen auf Listenelemente wird durch eckige Klammern erm√∂glicht. M√∂chte man zum Beispiel auf das zweite Element des Vektors x2 zugreifen, so wendet man die folgende Notation an:\n\nx2[2]\n\n[1] 2\n\n\nInnerhalb der eckigen Klammer steht der Index, bzw. die Indizes welche zur√ºckgegeben werden sollen. So kann man auch zum Beispiel die ersten zwei Eintr√§ge des Vektors x2 durch folgende Notation ausw√§hlen:\n\nx2[1:2]\n\n[1] 1 2\n\n\n\n\n1.2.1.2 Erstellen von Datens√§tzen\nObwohl die meisten Datens√§tze, welche wir in der √úbung verwendet bereits erstellt wurden, ist es sehr sinnvoll sich mit der Erstellung von eigenen Datens√§tzen auseinander zu setzen. Besonders wenn man auf Basis bereits existierender Datens√§tze Teildatens√§tze erstellen will, ist diese Kenntnis hilfreich.\nEin Grundlegender Befehl zum erzeugen von Datens√§tzen ist die data.frame Funktion.\n\ndata&lt;- data.frame(\n  x = seq(1,10,length.out = 18),\n  y = letters[1:18],\n  z = unlist(\n        rep(\n          strsplit(\"The quick brown fox jumps over the lazy dog\",\n                   split = \" \"),\n          2)\n  )\n)\n\nDas Datensatz data enth√§lt drei Spalten: x,y,z welche mit verschiedenen Werten erzeugt wurden. Die Spalte x wurde mithilfe der seq Funktion mit 18 Werten bef√ºllt welche zwischen \\(1\\) und \\(10\\) liegen. Die Spalte y wurde mithilfe des vordefinierten letters Vektor bef√ºllt. Dieser enth√§lt die Buchstaben a-z. Hierbei wurden ebenso die 18 Eintr√§ge verwendet. In der letzten Spalte z wurden mehrere Funktionen verschachtelt auf einmal verwendet. Die innerste Funktion strsplit teilt einen gegebenen String (&lt;chr&gt;) Vektor in einzelne Komponenten auf. Die Teilung des langen Strings folgt durch das Argument split. Hier wurde als Split Argument das Leerzeichen \" \" √ºbergeben. Dadurch wird der Satz \"The quick brown fox jumps over the lazy dog\" an jedem Leerzeichen getrennt und eine Liste der einzelnen W√∂rter zur√ºckgegeben. Die rep (Repeat) Funktion wiederholt das √ºbergebene Argument und wiederholt dieses gem√§√ü der √ºbergebenen Zahl. Da der R√ºckgabewert der rep Funktion in diesem Fall eine verschachtelte Liste ist (eine Liste mit zwei Elementen, wobei jedes dieser beiden Elemente wieder selbst eine Liste ist), wir die Verschachtlung √ºber die unlist Funktion aufgehoben.\nDer Datensatz hat somit folgende Struktur:\n\ndata\n\n           x y     z\n1   1.000000 a   The\n2   1.529412 b quick\n3   2.058824 c brown\n4   2.588235 d   fox\n5   3.117647 e jumps\n6   3.647059 f  over\n7   4.176471 g   the\n8   4.705882 h  lazy\n9   5.235294 i   dog\n10  5.764706 j   The\n11  6.294118 k quick\n12  6.823529 l brown\n13  7.352941 m   fox\n14  7.882353 n jumps\n15  8.411765 o  over\n16  8.941176 p   the\n17  9.470588 q  lazy\n18 10.000000 r   dog\n\n\nDie Variablen eines Datensatzes k√∂nne √ºber verschiedene Methoden ausgew√§hlt und bearbeitet werden. Im n√§chsten Abschnitt werden hierf√ºr einige dieser Methoden vorgestellt.\n\n\n1.2.1.3 Rechenoperationen in R\nR wurde urspr√ºnglich als statistische Programmiersprache eingef√ºhrt, weshalb bereits viele Funktionen zum berechnen verschiedener Werte vordefiniert sind.\nArithmetische Rechenoperation funktionieren f√ºr Zahlenwerte, Zahlenvektoren und auch gemischten Vektoren.\n\n5+2\n\n[1] 7\n\nseq(1:5)*2\n\n[1]  2  4  6  8 10\n\nseq(1:5)^2\n\n[1]  1  4  9 16 25\n\ncos(seq(0,pi,length.out=10))\n\n [1]  1.0000000  0.9396926  0.7660444  0.5000000  0.1736482 -0.1736482\n [7] -0.5000000 -0.7660444 -0.9396926 -1.0000000\n\n\nObige Beispiele enthalten nur eine kleine Selektion an Methoden welche verwendet werden k√∂nnen, um verschiedene Rechenoperationen durchzuf√ºhren. R Funktioniert gewisserma√üen teilweise wie ein Taschenrechner, weshalb dieser Abschnitt auch nicht in der Tiefe ausgef√ºhrt werden muss.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>(Kurz)Einf√ºhrung in R</span>"
    ]
  },
  {
    "objectID": "01_Einfuehrung.html#arbeiten-mit-datens√§tzen",
    "href": "01_Einfuehrung.html#arbeiten-mit-datens√§tzen",
    "title": "1¬† (Kurz)Einf√ºhrung in R",
    "section": "1.3 Arbeiten mit Datens√§tzen",
    "text": "1.3 Arbeiten mit Datens√§tzen\nIm Zusammenhang mit Data Mining bilden Daten die Grundlage. Egal ob das Ziel ist das Wetter, k√ºnftige Aktienkurse oder die Grundmiete einer potenziellen Mietwohnung vorherzusagen - ohne hochwertige Daten versagen selbst die fortschrittlichsten Modelle. Die Realit√§t sieht jedoch so aus, dass Daten, die wir aus dem Internet, von Servern oder aus Excel Tabellen beziehen, oft alles andere als makellos sind. So k√∂nnen fehlende Werte beispielsweise als NA (Not Available), NaN (Not a Number), NULL oder einfach als leere Zeichenkette ‚Äû‚Äú kodiert sein. Aus diesem Grund ist es wichtig, dass die grundlegenden Datenmanipulationen in R nochmals diskutier werden.\n\n1.3.1 Daten Importieren\nViele Datens√§tze sind zwar bereits in R integriert, bzw. k√∂nnen √ºber Libraries eingelesen werden, allerdings sind einige der Datens√§tze welche wir in der √úbung verwenden nur durch externe Quellen verf√ºgbar. Die meisten Daten werden im csv oder txt Format zur Verf√ºgung gestellt. Es ist deshalb wichtig zu verstehen wie man diese Datens√§tze einliest bevor man tats√§chlich mit diesen arbeitet.\nF√ºr die folgenden √úbungen werden wir das Ramen Dataset untersuchen, welches entweder von der Data Science Plattform Kaggle oder dem untenstehenden Button heruntergeladen werden kann.\n\nDownload ramen Data\n\nDer Ramen Datensatz enth√§lt Informationen √ºber verschiedene Instant Ramen Gerichte, welche besonders beliebt unter Studierenden w√§hrend der Pr√ºfungszeit sind.\nDie Daten k√∂nnen dann zum Beispiel in einem Verzeichnis Daten gespeichert werden, welches im gleichen Verzeichnis liegt wie die Projektdatei. Um die Daten einzulesen verwendet man in diesem Fall die read.csv Funktion verwenden. Im folgenden Code Snippet wird der Ramen Datensatz aus dem Verzeichnis /data eingelesen und unter dem Namen data_ramen gespeichert.\n\ndata_ramen &lt;- read.csv(file = \"data/ramen-ratings.csv\")\n\n\n\n1.3.2 Einfache Datenmanipulation und Visualisierung\nF√ºr die meisten √úbungen werden wir das {tidyverse} verwenden, welches eine Sammlung von Libraries enth√§lt die das Arbeiten mit Daten stark vereinfachen. Um die Sammlung von Libraries zu installieren, kann der install.packages() Befehl verwendet werden:\n\ninstall.packages(\"tidyverse\")\n\nNach der Installation kann die Library dann mit dem library Befehl zur aktuellen R Session hinzugef√ºgt werden.\n\nlibrary(\"tidyverse\")\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nDas {tidyverse} enth√§lt eine vielzahl wichtiger Libraries, von welchen wir vor allem die folgenden drei verwenden werden:\n\n{ggplot2} f√ºr das Erstellen von Grafiken,\n{dplyr} f√ºr Datenmanipulation,\n{tibble} f√ºr das effiziente Speichern von Daten.\n\nEine sehr gute Einf√ºhrung in diese Library kann unter folgendem Link eingesehen werden. Der Entwickler hinter der Library hat ebenso ein kostenloses Buch namens R for Data Science geschrieben, welches eine sehr Umfangreiche Einf√ºhrung in die verschiedenen Libraries des {tidyverse} gibt.\nNach dem erfolgreichen Installieren und hinzuf√ºgen der Library k√∂nnen wir direkt die erste essentielle Funktion nutzen. Die glimpse() Funktion gibt einen kompakten √ºberblick √ºber den Inhalt und die beinhaltenden Datentypen des unterliegenden Datensatzes.\n\nglimpse(data_ramen)\n\nRows: 2,580\nColumns: 7\n$ Review.. &lt;int&gt; 2580, 2579, 2578, 2577, 2576, 2575, 2574, 2573, 2572, 2571, 2‚Ä¶\n$ Brand    &lt;chr&gt; \"New Touch\", \"Just Way\", \"Nissin\", \"Wei Lih\", \"Ching's Secret‚Ä¶\n$ Variety  &lt;chr&gt; \"T's Restaurant Tantanmen \", \"Noodles Spicy Hot Sesame Spicy ‚Ä¶\n$ Style    &lt;chr&gt; \"Cup\", \"Pack\", \"Cup\", \"Pack\", \"Pack\", \"Pack\", \"Cup\", \"Tray\", ‚Ä¶\n$ Country  &lt;chr&gt; \"Japan\", \"Taiwan\", \"USA\", \"Taiwan\", \"India\", \"South Korea\", \"‚Ä¶\n$ Stars    &lt;chr&gt; \"3.75\", \"1\", \"2.25\", \"2.75\", \"3.75\", \"4.75\", \"4\", \"3.75\", \"0.‚Ä¶\n$ Top.Ten  &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"‚Ä¶\n\n\nRows: 2,580 bedeutet, dass der Datensatz 2.580 Eintr√§ge hat, und Columns: 7 bedeutet, dass der Datensatz jeweils 7 Variablen enth√§lt. Die erste Spalte enth√§lt die Variablennamen, ihre Datentypen und einige Anfangswerte, wodurch der Datensatz eine klare Struktur erh√§lt. Wir k√∂nnen bereits sehen, dass au√üer einer Variablen (Review) jede andere Variable vom Typ chr ist, was f√ºr character oder string steht.\n\n1.3.2.1 Anpassen der Variablen\nBeim Aufrufen der glimpse() Funktion ist direkt aufgefallen, dass die Variablennamen teilweise inkonsistent sind. Die erste Spalte hat zum Beispiel den Namen \"Review..\". Um eine konsistente und einfache Namensgebung zu verwenden, k√∂nnen wir die janitor::clean_names Funktion verwenden.\n\n\n\n\n\n\nTip\n\n\n\nDie obige Notation wird verwendet, wenn man zus√§tzlich zum Funktionsname auch noch die Library der Funktion benennen will. Besonders bei der Vermeidung von sogenannten ‚Äònamespace conflicts‚Äô, bei welchen zwei Funktionen aus verschiedenen Libraries die gleichen Namen besitzen.\n\n\nAuch hier gilt: Eventuell muss die {janitor} Library zuerst installiert werden, bevor die Funktion verwendet werden kann. Die Installation kann analog wie im Beispiel zur {tidyverse} Library durchgef√ºhrt werden.\nWenn die Library dann erfolgreich installiert wurde, k√∂nnen wir die Daten mit einer angepassten Version √ºberschreiben:\n\ndata_ramen &lt;- janitor::clean_names(dat = data_ramen)\n\nNach dem Ab√§ndern der Daten und einem erneuten Aufrufen der glimpse() Funktion kann die √Ñnderung der Variablennamen eingesehen werden:\n\nglimpse(data_ramen)\n\nRows: 2,580\nColumns: 7\n$ review  &lt;int&gt; 2580, 2579, 2578, 2577, 2576, 2575, 2574, 2573, 2572, 2571, 25‚Ä¶\n$ brand   &lt;chr&gt; \"New Touch\", \"Just Way\", \"Nissin\", \"Wei Lih\", \"Ching's Secret\"‚Ä¶\n$ variety &lt;chr&gt; \"T's Restaurant Tantanmen \", \"Noodles Spicy Hot Sesame Spicy H‚Ä¶\n$ style   &lt;chr&gt; \"Cup\", \"Pack\", \"Cup\", \"Pack\", \"Pack\", \"Pack\", \"Cup\", \"Tray\", \"‚Ä¶\n$ country &lt;chr&gt; \"Japan\", \"Taiwan\", \"USA\", \"Taiwan\", \"India\", \"South Korea\", \"J‚Ä¶\n$ stars   &lt;chr&gt; \"3.75\", \"1\", \"2.25\", \"2.75\", \"3.75\", \"4.75\", \"4\", \"3.75\", \"0.2‚Ä¶\n$ top_ten &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"‚Ä¶\n\n\n\n\n\n1.3.3 Anpassen von Datens√§tzen\n\n1.3.3.1 Die mutateFunktion\nUm Anpassungen an den Daten vorzunehmen, bietet sich die dplyr::mutate Funktion perfekt an.\nDie mutate Funktion kann entweder neue Spalten generieren, oder vorhandene Spalten modifizieren. Im folgenden Beispiel wird die mutate Funktion daf√ºr verwendet die Variable stars vom Datentyp &lt;chr&gt; in &lt;dbl&gt; umzuwandeln und die Variable top_ten zu entfernen.\n\ndata_ramen &lt;- data_ramen %&gt;% \n  mutate(\n    stars = as.double(stars),\n    top_ten = NULL\n         )\n\nWarning: There was 1 warning in `mutate()`.\n‚Ñπ In argument: `stars = as.double(stars)`.\nCaused by warning:\n! NAs durch Umwandlung erzeugt\n\n\n\nIn der ersten Zeile der Codezelle wird angegeben, dass der Datensatz data_ramen durch eine ver√§nderte Version √ºberschrieben werden soll.\nDer urspr√ºngliche Datensatz data_ramen wird in der zweiten Zeile an die mutateFunktion √ºbergeben.\nIn der dritten Zeile wird als erstes Argument die Variable stars durch die Ausgabe der Funktion as.double mit Argument stars √ºberschrieben. Die Funktion as.double ist eine Funktion der {base} library.\nIn der vierten Zeile wird als weiteres Argument (separiert durch ein , aus der vorherigen Zeile) die den Variable top_ten entfernt, indem die Variable mit NULL √ºberschrieben wird.\n\nDas ausf√ºhren der vorherigen Code Zelle wirft folgende Warnung:\n\nWarning message: There was 1 warning in mutate(). i In argument: stars = as.double(stars). Caused by warning: ! NAs introduced by coercion\n\nDies liegt daran, dass in der Variable stars fehlende Werte vorhanden sind. Durch das Anwenden der as.double Funktion wurden die Fehlenden werte in NA Werte umgewandelt, was aber erst mal kein Problem darstellt.\n\n\n1.3.3.2 Der Pipe Operator %&gt;%\nIn der Code Zelle\n\ndata_ramen &lt;- data_ramen %&gt;% \n  mutate(...)\n\nwurde in der ersten Zeile der sogenannte Pipe Operator %&gt;% verwendet. Der Pipe Operator ist ein m√§chtiges Werkzeug aus der {magittr} Library welche erlaubt Verkettungen von Funktionen durchzuf√ºhren. Die Wortw√∂rtliche √ºbersetzung ‚ÄúRohr‚Äù steht hierbei sinnbildlich f√ºr den Vorgang dass auf der einen Seite (links) z.B. Daten √ºbergeben werden und auf der anderen Seite (rechts) nach den durchgef√ºhrten Transformationen ausgegeben werden. Die St√§rke des Operators liegt hierbei in der M√∂glichkeit viele Operationen sequentiell durchzuf√ºhren und dabei die Lesbarkeit und √úbersichtlichkeit zu bewahren.\nSo kann man zum Beispiel nach der initialen Transformation durch die mutate Funktion die na.omit Funktion auf die resultierenden Daten angewendet werden, um alle NA, bzw. NaN Werte zu entfernen.\n\ndata_ramen &lt;- data_ramen %&gt;% \n  mutate(\n    stars = as.double(stars),\n    top_ten = NULL\n         ) %&gt;%\n  na.omit()\n\nDurch die Anwendung des Pipe Operators muss auch der Datensatz im Obigen Beispiel nicht als Argument der Funktion √ºbergeben werden. Ohne den Pipe Operator k√∂nnte man das obige Beispiel auch wie folgt implementieren:\n\ndata_ramen &lt;- mutate(na.omit(data_ramen),\n                     stars = as.double(stars),\n                     top_ten = NULL)\n\n\n\n1.3.3.3 Filtern, Gruppieren und Zusammenfassen von Datasets\nIm vorherigen Abschnitt haben wir uns mit grundlegenden Funktionen f√ºr das Anpassen der Daten besch√§ftigt. Ein weiterer wichtiger Aspekt beim Umgang mit Daten ist die sogenannte Explorative Datenanalyse (EDA). Hierbei werden die unterliegenden Daten mithilfe von deskriptiven und induktiven Methoden untersucht. Wichtige Funktionen in diesem Kontext sind die filter, group_by und summarise Funktionen aus dem {dplyr} Package.\nAngenommen wir wollen f√ºr das data_ramen Dataset herausfinden, wie viele Marken (brands) mehr als 30 Sorten (varieties) an Instant Ramen Nudeln anbieten. Intuitiv macht es Sinn die Eintr√§ge des Datensatzes zu Gruppieren, wobei jede Marke einer Gruppe entspricht. Nach der Bildung von Gruppen erzeugt durch die Marken, kann dann f√ºr jede Gruppe die Anzahl der Eintr√§ge innerhalb der Gruppen berechnet werden. Abschlie√üend filtert man die resultierenden Ergebnisse nach dem Kriterium, dass mindestens 30 Eintr√§ge in einer Gruppe enthalten sein m√ºssen. Diese beschriebene Heuristik l√§sst sich mithilfe des folgenden Skripts umsetzen:\n\ndata_ramen %&gt;%\n  group_by(brand) %&gt;%\n  summarise(num_var = n()) %&gt;%\n  filter(num_var &gt; 30)\n\n# A tibble: 12 √ó 2\n   brand         num_var\n   &lt;chr&gt;           &lt;int&gt;\n 1 Indomie            53\n 2 Lucky Me!          34\n 3 Mama               71\n 4 Maruchan           76\n 5 Myojo              63\n 6 Nissin            381\n 7 Nongshim           98\n 8 Ottogi             45\n 9 Paldo              66\n10 Samyang Foods      51\n11 Vifon              33\n12 Vina Acecook       34\n\n\nFassen wir die Schritte im obigen Skript nochmal zusammen:\n\nIn der ersten Zeile wird der Datensatz data_ramen durch den Pipe Operator in die folgenden Funktionen √ºbergeben.\nIn der zweiten Zeile wird der group_by Befehl verwendet um die Eintr√§ge des Datensatzes nach der Variable brand zu Gruppieren.\n\n\n\n\n\n\nTip\n\n\n\nDurch das alleinige Aufrufen der group_by Funktion mit der entsprechenden Gruppierungsvariable als Argument, wird im Output bereits angezeigt wie viele verschiedene Gruppen erstellt wurden.\n\ndata_ramen %&gt;%\n  group_by(brand) %&gt;%\n  glimpse()\n\nRows: 2,577\nColumns: 6\nGroups: brand [355]\n$ review  &lt;int&gt; 2580, 2579, 2578, 2577, 2576, 2575, 2574, 2573, 2572, 2571, 25‚Ä¶\n$ brand   &lt;chr&gt; \"New Touch\", \"Just Way\", \"Nissin\", \"Wei Lih\", \"Ching's Secret\"‚Ä¶\n$ variety &lt;chr&gt; \"T's Restaurant Tantanmen \", \"Noodles Spicy Hot Sesame Spicy H‚Ä¶\n$ style   &lt;chr&gt; \"Cup\", \"Pack\", \"Cup\", \"Pack\", \"Pack\", \"Pack\", \"Cup\", \"Tray\", \"‚Ä¶\n$ country &lt;chr&gt; \"Japan\", \"Taiwan\", \"USA\", \"Taiwan\", \"India\", \"South Korea\", \"J‚Ä¶\n$ stars   &lt;dbl&gt; 3.75, 1.00, 2.25, 2.75, 3.75, 4.75, 4.00, 3.75, 0.25, 2.50, 5.‚Ä¶\n\n\nso wurden im obigen Beispiel 355 verschiedene Gruppen gebildet, woraus wir schlie√üen k√∂nnen, dass im Datensatz 355 verschiedene Marken enthalten sind (unter der Annahme, dass keine Duplikate enthalten sind).\n\n\nDer summarise Befehl wird dann verwendet, um verschiedene Deskriptive Statistiken auf den Gruppen anzuwenden. In diesem Fall wird √§hnlich wie bei der Verwendung der mutate Funktion eine neue Spalte num_vars erstellt welch in jeder Zeile den Output der Funktion n enth√§lt. Die dplyr::n Funktion hat als R√ºckgabewert die Gruppengr√∂√üe.\n\n\n\n\n1.3.4 Faktor Variablen\nEin wichtiger Datentyp, der sowohl ordinale (Daten mit einem gewissen Ordnungsbegriff) als auch nominale Daten verarbeiten kann, sind so genannte Faktor Variablen (Datentyp &lt;fct&gt;).\nBetrachten wir hierf√ºr den folgenden Synthetischen Datensatz mit sieben Personen mit entsprechenden Altersgruppen und Augenfarben.\n\ndata_example &lt;- tibble(\n names = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\", \"Frank\", \"Grace\"),\n age_groups = c(\"18-25\", \"&lt;18\", \"26-35\", \"36-45\", \"18-25\", \"60+\", \"26-35\"),\n eye_color = c(\"Blue\", \"Brown\", \"Green\", \"Hazel\", \"Brown\", \"Blue\", \"Green\")\n)\n\ndata_example \n\n# A tibble: 7 √ó 3\n  names   age_groups eye_color\n  &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;    \n1 Alice   18-25      Blue     \n2 Bob     &lt;18        Brown    \n3 Charlie 26-35      Green    \n4 Diana   36-45      Hazel    \n5 Eve     18-25      Brown    \n6 Frank   60+        Blue     \n7 Grace   26-35      Green    \n\n\n\n\n\n\n\n\nNote\n\n\n\nIm obigen Code Snippet wurde die tibble::tibble Funktion verwendet. Tibbles werden √§hnlich wie data.frame Objekte zum speichern von Datens√§tzen verwendet. Tibbles haben allerdings zum Beispiel den Vorteil, dass sie bei der Ausgabe in der Konsole √ºbersichtlicher dargestellt werden.\n\n\nDa die Variable age_group nur eine Altersspanne angibt, ist es nicht sinnvoll, sie als Zahl und nicht als Ordinalvariable zu kodieren. Mit der Funktion mutate k√∂nnen wir deshalb die Altersgruppen als Ordinalvariablen (ordered factors) kodiert werden. Dazu wird die Variable age_groups in einen Faktor mit Stufen und Label gesetzt. Die Stufen geben die Reihenfolge der Werte an, und die Label k√∂nnen zur Umbenennung dieser Kategorien verwendet werden.\n\ndata_example &lt;- data_example %&gt;%\n  mutate( \n    age_groups = factor(\n      age_groups,\n      levels = c(\"&lt;18\", \"18-25\", \"26-35\", \"36-45\", \"60+\"),\n      ordered = TRUE,\n      labels = c(\"child\",\"adult\",\"adult\",\"adult\",\"senior\")\n    )\n  )\n\n\n√Ñhnlich wie im vorherigen Beispiel sollten wir angeben, dass wir den Datensatz data_example mit einer ver√§nderten Version √ºberschreiben.\nIn der zweiten Zeile wird dann die mutate Funktion aufgerufen um in der dritten Zeile die Variable age_groups zu √ºberschreiben.\nDurch das Setzen von age_groups = factor(age_groups, ...) wird die Variable age_groups in einen (bisher ungeordneten) Faktor umgewandelt, der das setzen von Stufen (levels) und Labels erm√∂glicht.\nSo wird durch das Setzen von levels = c(\"&lt;18\", \"18-25\",...) eine Ordnung auf die Variable auferlegt.\nordered=TRUE gibt an, dass die Altersgruppen nach den im levelsVektor angegebenen Stufen aufsteigend sortiert sind.\nZu guter Letzt gibt labels = c(\"child\", \"adult\", ...) die Bezeichnungen an, welche die numerischen Altersgruppen ersetzen. Zum Beispiel wird \\(&lt;18\\) als \"child\" bezeichnet, die Bereiche 18-25, 26-35 und 36-45 als ‚Äúadult‚Äú, und \\(&gt;60\\) als \"senior\".\n\nAnalog kann auch die Variable eye_color in eine nominale Faktorvariable umgewandelt werden:\n\ndata_example &lt;- data_example %&gt;%\n  mutate(\n    eye_color = factor(eye_color)\n  )",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>(Kurz)Einf√ºhrung in R</span>"
    ]
  },
  {
    "objectID": "01_Einfuehrung.html#datenvisualisierung-mit-ggplot2",
    "href": "01_Einfuehrung.html#datenvisualisierung-mit-ggplot2",
    "title": "1¬† (Kurz)Einf√ºhrung in R",
    "section": "1.4 Datenvisualisierung mit {ggplot2}",
    "text": "1.4 Datenvisualisierung mit {ggplot2}\nEin weiterer wichtiger Aspekt des Data Minings ist die grafische Darstellung von Datens√§tzen. Die Beschreibung eines Algorithmus mit mathematischer Notation oder die Untersuchung eines Datensatzes mit Hilfe von deskriptiven und induktiven Statistiken allein kann eine Herausforderung f√ºr die Vermittlung von Botschaften darstellen. Obwohl R einige Basisfunktionen zur Erstellung von Grafiken bietet, wird in diesem Kurs haupts√§chlich die Bibliothek {ggplot2} verwendet. Eine umfassende Einf√ºhrung in {ggplot2} findet sich in Hadley Wickhams Buch Elegant Graphics for Data Analysis.\nF√ºr das folgende Beispiel wird der Datensatz data_ramen_ratings verwendet:\n\ndata_ramen_ratings &lt;- data_ramen %&gt;%\n  group_by(stars) %&gt;%\n  summarise(count=n())\n\ndata_ramen_ratings %&gt;% head()\n\n# A tibble: 6 √ó 2\n  stars count\n  &lt;dbl&gt; &lt;int&gt;\n1  0       26\n2  0.1      1\n3  0.25    11\n4  0.5     14\n5  0.75     1\n6  0.9      1\n\n\nHierbei wird ein neuer Datensatz erzeugt, welcher zwei Spalten enth√§lt. Die erste Spalte enth√§lt die verschiedenen Auspr√§gungen der Variable stars und die zweite Spalte count enth√§lt die Anzahl der Ramen, welche die entsprechende Bewertung in stars haben. Der head Befehl gibt hierbei die ersten 6 Eintr√§ge des neu erstellten Datensatzes aus.\nEine mit {ggplot2} erstellte Grafik besteht aus den folgenden drei Grundkomponenten:\n\nDie Daten selbst.\n\nggplot(data = data_ramen_ratings)\n\n\n\n\n\n\n\n\nBeachte, dass das Diagramm so noch keine Achsen, Ticks und Variablen anzeigt.\nEine Reihe von √Ñsthetik-Zuordnungen (aesthetics mappings), die beschreiben, wie Variablen in den Daten auf visuelle Eigenschaften abgebildet werden.\n\nggplot(aes(x=stars,y=count), data = data_ramen_ratings)\n\n\n\n\n\n\n\n\nDie aes Funktion gibt hierbei die √Ñsthetik-Zuordnung an. Im obigen Beispiel wird spezifiziert, dass die \\(x\\)-Achse die Variable stars abbildet und die \\(y\\)-Achse die Anzahl der Bewertungen.\nAls n√§chstes werden durch die geom-Layer die grafischen Komponenten hinzugef√ºgt, welche beschreiben wie die Daten in der Grafik repr√§sentiert werden.\n\nggplot(aes(x=stars, y=count), data = data_ramen_ratings)+\n  geom_col()\n\n\n\n\n\n\n\n\nNeben der ggplot Funktion wurde in der Obigen Zelle nun die Geometrie Column (S√§ule) hinzugef√ºgt. Um eine neue Geometrie hinzuzuf√ºgen wird nicht wie bei den vorheringen Beispielen der Pipe Operator, sondern ein + verwendet. Der Funktion geom_col (Geometry Column) wurde kein weiteres Argument hinzugef√ºgt. Man kann durch das Hinzuf√ºgen weiterer Argumente wie zum Beispiel fill (F√ºllung) die Farbe der S√§ulen ver√§ndern. So wird durch das Setzen von `fill=‚Äúred‚Äù die Farbe der S√§ulen auf rot gesetzt.\n\nggplot(aes(x=stars, y=count), data = data_ramen_ratings)+\n  geom_col(fill=\"red\")\n\n\n\n\n\n\n\n\nWeitere M√∂glichkeiten die Grafiken anzupassen und die Daten auf verschiedene Arten darzustellen werden in den folgenden √úbungen eingef√ºhrt.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>(Kurz)Einf√ºhrung in R</span>"
    ]
  },
  {
    "objectID": "01_Einfuehrung.html#√ºbungsaufgaben",
    "href": "01_Einfuehrung.html#√ºbungsaufgaben",
    "title": "1¬† (Kurz)Einf√ºhrung in R",
    "section": "1.5 √úbungsaufgaben",
    "text": "1.5 √úbungsaufgaben\n\n1.5.1 R als Taschenrechner\n\nAufgabe 1.1 Geben Sie folgende Ausdr√ºcke in die R-Console ein und erkl√§ren Sie jeweils das Ergebnis.\n\n2 + 3 * 4\n(2 + 3) * 4\n0.2 * 3 - 1.1\n0,2 * 3 - 1.1\n2^3^2\n(2^3)^2\nexp(1)\nlog(exp(1))\nsqrt(16)\n16^(1/2)\nSqrt(16)\n# Das ist ein Kommentar.\n\n\n\nAufgabe 1.2 Weise einer neuen Variable x den Wert \\(4\\) zu. Weise dann einer neuen Variable x2 den folgenden Wert zu:\n\\[\n\\sqrt{3x^2 + \\log\\left(\\frac{1}{e^x}\\right) + 5}\n\\]\n√úberlege was folgende Zeilen ausgeben und f√ºhre diese dann aus, um das Ergebnis zu √ºberpr√ºfen.\n\nx\nx2\nx + x2\nx.Produkt &lt;- x * x2\nx.Produkt\nx.Produkt &lt;- x.Produkt * x\nls()\nrm(x)\nx\n\n\n\nAufgabe 1.3 Kopiere den folgenden Code. Welche Ausgabe bewirkt der Code?\n\nZahl &lt;- 10\nkeine.Zahl &lt;- \"10\"\nZahl + 1\nkeine.Zahl + 1 \n\n\n\n\n1.5.2 Listen und Vektoren\n\nAufgabe 1.4 Erzeuge folgende Vektoren:\n\n\n[1] 5 6 7 8 9\n\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\n[1] -0.10 -0.08 -0.06 -0.04 -0.02  0.00\n\n\n[1] 10000 12500 15000 17500 20000\n\n\n [1] -3 -2 -1  0  1  2 -3 -2 -1  0  1  2 -3 -2 -1  0  1  2\n\n\n [1]  5.0  6.0  7.0  8.0  9.0 10.0 10.1 10.2 10.3 10.4 10.5\n\n\n\n\nAufgabe 1.5 Gegeben sind die Vektoren:\n\nx &lt;- 4:2\ny &lt;- seq(from=0.1, to=0.5, by=0.1)\n\nErkl√§re, was folgende Ausdr√ºcke ergeben und √ºberpr√ºfe das Ergebnis:\n\nx + y \nx * y\nx^3 + 1\n2*x - 3*y\nn &lt;- length(x + y)\nsum(x+y)/n\n\n\n\nAufgabe 1.6 Gegeben seien die beiden folgenden Vektoren:\\[-2ex]\n\nx &lt;- seq(from=0, to=100, by=2)\ny &lt;- 100:1\n\nSchreibe die Ergebnisse folgender Ausdr√ºcke auf und √ºberpr√ºfen anschlie√üend das Ergebnis in R:\n\nx[3]\ny[c(1,3,10)]\nx[1:4]\nx[x &gt; 91]\nx[x &gt; 20 & x &lt;= 30]\ny[y==5 | y&gt;95 | y&lt;3]\n\n\n\n\n1.5.3 Datens√§tze\nF√ºr die folgenden √úbungen verwenden wir wieder den data_ramen Datensatz aus Sektion Section 1.3.2.\nWende hierf√ºr die zuvor durchgef√ºhrten Datenmanipulationen auf den Datensatz an, so dass die Spalte top_ten entfernt wurde und die Spalte stars in den Datentyp &lt;dbl&gt; umgewandelt wurden.\n\nAufgabe 1.7 Finde heraus, aus welchen f√ºnf L√§nder die meisten Ramen stammen. Hinweis: Verwende die arrange Funktion in Kombination mit der desc Funktion um die Ergebnisse absteigend nach der Gr√∂√üe zu sortieren.\n\n\nAufgabe 1.8 Finde heraus, wie viele verschiedene Ramen Sorten in Deutschland (country == \"Germany\") verf√ºgbar sind.\n\n\n\n1.5.4 Grafiken\n\nAufgabe 1.9 Erkl√§re die Funktionen und Argumente des folgenden Code Snippets und erkl√§re was sich im Vergleich zum unbearbeiteten Datensatz inhaltlich ge√§ndert hat.\n\ndata_ramen_filtered &lt;- data_ramen %&gt;%\n  filter(style %in% c(\"Bowl\",\"Cup\",\"Pack\")) %&gt;%\n  mutate(style = if_else(\n    style==\"Pack\",\"Pack\",\"Container\"\n    ),\n    style= factor(style)\n  ) \n\n\n\nAufgabe 1.10 Beschreibe die folgende Grafik. Vergleiche dabei auch die beiden Gruppen untereinander.\n\n\n\n\n\n\n\n\n\n\n\nAufgabe 1.11 Verwende den Datensatz data_ramen_filtered um die Dichte der Variable stars mit {ggplot2} darzustellen.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>(Kurz)Einf√ºhrung in R</span>"
    ]
  },
  {
    "objectID": "01_Einfuehrung.html#l√∂sungen",
    "href": "01_Einfuehrung.html#l√∂sungen",
    "title": "1¬† (Kurz)Einf√ºhrung in R",
    "section": "1.6 L√∂sungen",
    "text": "1.6 L√∂sungen\n\nSolution 1.1 (Aufgabe¬†1.1). \n\n2 + 3 * 4\n\n[1] 14\n\n\nMit diesen Befehlen wird die Rechnung \\(2+3*4\\) durchgef√ºhrt.\n\n(2 + 3) * 4\n\n[1] 20\n\n\nIm Vergleich zur vorherigen Aufgabe werden hier Klammern gesetzt, um zuerst \\(2+3\\) zu berechnen, bevor das Ergebnis mit \\(4\\) multipliziert wird.\n\n0.2 * 3 - 1.1\n\n[1] -0.5\n\n\nNeben ganzzahligen Werten, kann R auch Rechnungen mit Dezimalzahlen durchf√ºhren.\n\n0,2 * 3 - 1.1\n\nError in parse(text = input): &lt;text&gt;:1:2: Unerwartete(s) ','\n1: 0,\n     ^\n\n\nBei der obigen Rechnung wird ein Fehler ausgegeben, da das Dezimaltrennzeichen in R nicht ,, sondern . ist.\n\n2^3^2\n\n[1] 512\n\n\nDie obige Code Zeile f√ºhrt die Rechnung \\(2^{3^2} = 2^9\\) aus.\n\n(2^3)^2\n\n[1] 64\n\n\nWie zuvor wird durch Klammern bewirkt, dass die erste Potenzierung zuerst durchgef√ºhrt wird.\n\nexp(1)\n\n[1] 2.718282\n\n\nDie exp(x) Funktion f√ºhrt die Rechnung \\(e^x\\) durch, wobei \\(e\\approx 2.718282\\) die eulersche Konstante ist.\n\nlog(exp(1))\n\n[1] 1\n\n\nDie log Funktion berechnet den nat√ºrlichen Logarithmus. Die Rechnung \\(\\log\\circ\\exp\\) ist somit die Identit√§tsfunktion, also \\(x\\mapsto x\\).\n\nsqrt(16)\n\n[1] 4\n\n\nDie sqrt(x) Funktion f√ºhrt die Rechnung \\(\\sqrt{x}\\) durch.\n\n16^(1/2)\n\n[1] 4\n\n\nDa \\(\\sqrt{16} = 16^{0.5} = 16^{\\frac{1}{2}}\\) ist das Ergebnis identisch zur vorherigen Code Zelle.\n\nSqrt(16)\n\nError in Sqrt(16): konnte Funktion \"Sqrt\" nicht finden\n\n\nHier wird ein Fehler ausgegeben, da R ist. Case Sensitivity f√ºhrt dazu, dass bei dem Eingeben von Funktionen auch auf Gro√ü- und Kleinschreibung geachtet werden muss.\n\n# Das ist ein Kommentar.\n\nKommentare in R werden durch das # Symbol intiiert. Kommentare werden nicht ausgewertet, so dass hinter dem # Symbol jegliche Folge von Zeichen stehen kann.\n\n\nSolution 1.2 (Aufgabe¬†1.2). \n\nx &lt;- 4\nx2 &lt;- sqrt(3*x^2+log(1/exp(x))+5)\n\n\nx\n\n[1] 4\n\nx2\n\n[1] 7\n\nx + x2\n\n[1] 11\n\nx.Produkt &lt;- x * x2\nx.Produkt\n\n[1] 28\n\nx.Produkt &lt;- x.Produkt * x\nls()\n\n [1] \"data\"                \"data_example\"        \"data_ramen\"         \n [4] \"data_ramen_filtered\" \"data_ramen_ratings\"  \"x\"                  \n [7] \"x.Produkt\"           \"x1\"                  \"x2\"                 \n[10] \"x3\"                  \"y\"                  \n\nrm(x)\n\nDurch die rm Funktion wird die Variable x entfernt und kann somit nicht mehr aufgerufen werden. Deshalb wird beim versuchten Aufruf in der folgenden Zeile eine Fehlermeldung ausgegeben.\n\nx\n\nError: Objekt 'x' nicht gefunden\n\n\n\n\nSolution 1.3 (Aufgabe¬†1.3). Die Variable keine.Zahl ist vom Datentyp &lt;char&gt; und kann somit nicht ohne weiteres wie ein tats√§chlicher Zahlenwert gehandhabt werden. Das f√ºhrt dazu, dass in der folgende Code Zelle eine Fehlermeldung ausgegeben wird.\n\nZahl &lt;- 10\nkeine.Zahl &lt;- \"10\"\nZahl + 1\n\n[1] 11\n\nkeine.Zahl + 1 \n\nError in keine.Zahl + 1: nicht-numerisches Argument f√ºr bin√§ren Operator\n\n\n\n\nSolution 1.4 (Aufgabe¬†1.4). \n\n5:9\n\n[1] 5 6 7 8 9\n\n10:1\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\nseq(from=-0.1, to=0, by=0.02)\n\n[1] -0.10 -0.08 -0.06 -0.04 -0.02  0.00\n\nseq(from=10000, to=20000, length.out=5)\n\n[1] 10000 12500 15000 17500 20000\n\nrep(-3:2, 3)\n\n [1] -3 -2 -1  0  1  2 -3 -2 -1  0  1  2 -3 -2 -1  0  1  2\n\nc(5:10, seq(from=10.1, by=.1, to=10.5))\n\n [1]  5.0  6.0  7.0  8.0  9.0 10.0 10.1 10.2 10.3 10.4 10.5\n\n\n\n\nSolution 1.5 (Aufgabe¬†1.5). Gegeben sind die Vektoren:\n\nx &lt;- 4:2\ny &lt;- seq(from=0.1, to=0.5, by=0.1)\n\nDa die Objekte x und y nicht die selbe L√§nge besitzen, werden in der folgenden Zelle die Eintr√§ge des k√ºrzeren Vektors bis zur L√§nge des l√§ngeren Vektors wiederholt. Anschlie√üend werden die einzelnen Eintr√§ge komponentenweise addiert.\n\nx + y \n\nWarning in x + y: L√§nge des l√§ngeren Objektes\n     ist kein Vielfaches der L√§nge des k√ºrzeren Objektes\n\n\n[1] 4.1 3.2 2.3 4.4 3.5\n\n\nIn der folgenden Code Zelle werden die Listeneintr√§ge von x analog erweitert und dann komponentenweise mit den Eintr√§gen von y multipliziert.\n\nx * y\n\nWarning in x * y: L√§nge des l√§ngeren Objektes\n     ist kein Vielfaches der L√§nge des k√ºrzeren Objektes\n\n\n[1] 0.4 0.6 0.6 1.6 1.5\n\n\nDurch den ^ Operator werden zuerst alle Elemente der Liste x potenziert und anschlie√üend mit dem Wert 1 addiert.\n\nx^3 + 1\n\n[1] 65 28  9\n\n\nAnalog wie in den vorherigen Teilaufgaben werden die konstanten Faktoren zuerst komponentenweise multipliziert, bevor die Liste x erweitert werden und von 3*y komponentenweise abgezogen werden.\n\n2*x - 3*y\n\nWarning in 2 * x - 3 * y: L√§nge des l√§ngeren Objektes\n     ist kein Vielfaches der L√§nge des k√ºrzeren Objektes\n\n\n[1] 7.7 5.4 3.1 6.8 4.5\n\n\nDie L√§nge der Liste x+y is gegeben durch die L√§nge des l√§ngsten Elements in der Summe. Also durch die L√§nge von y.\n\nn &lt;- length(x + y)\n\nWarning in x + y: L√§nge des l√§ngeren Objektes\n     ist kein Vielfaches der L√§nge des k√ºrzeren Objektes\n\n\nAnalog wie in den vorherigen Teilaufgaben wird die Summe von x und y berechnet, dann die Summe der einzelnen Komponenten und zum Schluss wird die Summe durch n geteilt.\n\nsum(x+y)/n\n\nWarning in x + y: L√§nge des l√§ngeren Objektes\n     ist kein Vielfaches der L√§nge des k√ºrzeren Objektes\n\n\n[1] 3.5\n\n\n\n\nSolution 1.6 (Aufgabe¬†1.6). \n\nx &lt;- seq(from=0, to=100, by=2)\ny &lt;- 100:1\n\nx[3]\n\n[1] 4\n\ny[c(1,3,10)]\n\n[1] 100  98  91\n\nx[1:4]\n\n[1] 0 2 4 6\n\nx[x &gt; 91]\n\n[1]  92  94  96  98 100\n\nx[x &gt; 20 & x &lt;= 30]\n\n[1] 22 24 26 28 30\n\ny[y==5 | y&gt;95 | y&lt;3]\n\n[1] 100  99  98  97  96   5   2   1\n\n\n\n\nSolution 1.7 (Aufgabe¬†1.7). \n\ndata_ramen %&gt;%\n  group_by(country) %&gt;%\n  summarise(n=n()) %&gt;%\n  arrange(desc(n)) %&gt;%\n  head(5)\n\n# A tibble: 5 √ó 2\n  country         n\n  &lt;chr&gt;       &lt;int&gt;\n1 Japan         352\n2 USA           323\n3 South Korea   307\n4 Taiwan        224\n5 Thailand      191\n\n\n\n\nSolution 1.8 (Aufgabe¬†1.8). \n\ndata_ramen %&gt;%\n  filter(country==\"Germany\") %&gt;%\n  nrow()\n\n[1] 27\n\n\n\n\nSolution 1.9 (Aufgabe¬†1.9). \n\ndata_ramen_filtered &lt;- data_ramen %&gt;%\n  filter(style %in% c(\"Bowl\",\"Cup\",\"Pack\")) %&gt;%\n  mutate(style = if_else(\n    style==\"Pack\",\"Pack\",\"Container\"\n    ),\n    style= factor(style)\n  ) \n\n\nIn der ersten Zeile wird ein neuer Datensatz data_ramen_filtered erzeugt indem der Datensatz data_ramen modifiziert wird.\nIn der zweiten Zeile wird die filter Funktion auf den Datensatz data_ramen angewandt. Der %in% Operator pr√ºft hierbei, ob in der Spalte style die Werte \"Bowl\",\"Cup\",\"Pack\" vorhanden sind und geben den Wert TRUE oder FALSE zur√ºck. D.h., es werden alle Eintr√§ge selektiert, welche den Style \"Bowl\",\"Cup\", oder \"Pack\" enthalten.\nIn der dritten Zeile wird die mutate Funktion auf die Variable style angewendet.\nDie Transformation erfolgt hierbei in der vierten und f√ºnften Zeile durch die if_else Funktion, welche die Kondition style==\"Pack\" √ºberpr√ºft. Falls der Style eines Eintrags \"Pack\" ist, so wird diesem wieder der Wert \"Pack\" zugeschrieben. Falls allerdings der Style \"Cup\" oder \"Bowl\" betr√§gt, so wird der Wert mit \"Container\" √ºberschrieben.\nIn der sechsten Zeile wird wird die Variable \"Style\" dann in eine Faktorvariable umgewandelt.\n\n\n\nSolution 1.10 (Aufgabe¬†1.10). Beschreibe die folgende Grafik. Vergleiche dabei auch die beiden Gruppen untereinander.\n\n\n\n\n\n\n\n\n\nDie Grafik zeigt zwei Boxplots f√ºr die Variable star. Auf der linken Seite wird ein Boxplot f√ºr alle Eintr√§ge welche den Style \"Container\" besitzen angezeigt und auf der rechten Seite ein Boxplot f√ºr alle Eintr√§ge welche den Style \"Pack\" besitzen. Es l√§sst sich an den Boxplots ablesen, dass die beiden Styles einen √§hnlichen Median bez√ºglich der Bewertung besitzen. Allerdings ist das \\(25\\%\\) Quartil beim Style \"Container\" etwas kleiner als beim Style \"Pack\". Beim Style \"Pack\" werden auf der anderen Seite aber mehr Ausrei√üer nach Unten abgebildet.\n\n\nSolution 1.11 (Aufgabe¬†1.11). Verwende den Datensatz data_ramen_filtered um die Dichte der Variable stars mit {ggplot2} darzustellen. Hinweis: Die Dichte einer Variable kann mithilfe der Funktion geom_density dargestellt werden.\n\ndata_ramen_filtered %&gt;%\n  ggplot(aes(x=stars))+\n  geom_density(fill=\"#69b3a2\", color=\"#69b3a2\")+\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>(Kurz)Einf√ºhrung in R</span>"
    ]
  },
  {
    "objectID": "02_Lineare_Regression.html",
    "href": "02_Lineare_Regression.html",
    "title": "2¬† Lineare Regression",
    "section": "",
    "text": "2.1 Der Palmer-Penguin-Datensatz\nDer Datensatz f√ºr diese und teilweise auch kommende √úbungen ist der sogenannte *Palmer-Penguins-Datensatz. Als Teil der {tidyverse}-Library ist der Datensatz bereits vorinstalliert und wir k√∂nnen diesen nach dem Hinzuf√ºgen der {tidyverse}-Library √ºber den Befehl palmerpenguins::penguins abrufen. Es ist wie immer ratsam, den Datensatz als neue Variable zu speichern:\nlibrary(tidyverse)\ndata_penguin&lt;-palmerpenguins::penguins\nDas Aufrufen der glimpse‚Å£-Funktion gibt wieder einen kompakten √úberblick √ºber die verschiedenen Variablen und Werte, welche diese annehmen.\ndata_penguin %&gt;% glimpse()\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse‚Ä¶\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ‚Ä¶\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ‚Ä¶\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186‚Ä¶\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ‚Ä¶\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male‚Ä¶\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007‚Ä¶\nWie der Name des Datensatzes andeutet, enth√§lt er Informationen √ºber verschiedene Pinguinarten. Konkret umfasst er \\(344\\) Beobachtungen von drei unterschiedlichen Pinguinarten, die auf verschiedenen Inseln des Palmer-Archipels in der N√§he der Antarktis leben. Die Daten wurden von Dr.¬†Kristen Gorman erhoben und dienen inzwischen als Standardbeispiel f√ºr vielf√§ltige Anwendungen in der Datenanalyse.\nNeben der Pinguinart (species) und der Beobachtungsinsel (island) sind folgende Messgr√∂√üen enthalten: die Schnabell√§nge in Millimetern (bill_length_mm), die Schnabeltiefe in Millimetern (bill_depth_mm), die Fl√ºgell√§nge in Millimetern (flipper_length_mm), das K√∂rpergewicht in Gramm (body_mass_g) sowie das Geschlecht (sex). Zus√§tzlich ist das Beobachtungsjahr (year) im Datensatz vorhanden, spielt aber f√ºr diese √úbung keine wesentliche Rolle.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "02_Lineare_Regression.html#sec-penguins",
    "href": "02_Lineare_Regression.html#sec-penguins",
    "title": "2¬† Lineare Regression",
    "section": "",
    "text": "2.1.1 Deskriptive Statistiken\nDie Funktion summary() in R erzeugt beim Aufruf eine deskriptive Zusammenfassung des √ºbergebenen Datensatzes. F√ºr quantitative Variablen werden dabei deskriptive Statistiken wie Minimum, Maximum, Median, arithmetisches Mittel und die Quartile ausgegeben. Bei nominalen oder kategorialen Variablen werden die einzelnen Auspr√§gungen und deren absolute H√§ufigkeiten dargestellt. Zus√§tzlich wird f√ºr jede Variable die Anzahl der fehlenden Werte (kodiert als NA) aufgef√ºhrt.\n\nsummary(data_penguin)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\nUm deskriptive Statistiken f√ºr verschiedene Subgruppen metrischer Variablen zu berechnen, bieten sich die in der vorherigen √úbung eingef√ºhrten Funktionen summarise() und group_by() aus der dplyr-Library an:\n\ndata_penguin %&gt;%\n  na.omit() %&gt;%\n  group_by(species) %&gt;%\n  summarise(n = n(),\n            \"1st Qu.\" = quantile(bill_length_mm, 0.25),\n            \"Median\" = median(bill_length_mm),\n            \"Mean\" = mean(bill_length_mm),\n            \"3rd Qu.\" = quantile(bill_length_mm, 0.25),\n            )\n\n\n\n\n\n\n\n\n\nspecies\nn\n1st Qu.\nMedian\nMean\n3rd Qu.\n\n\n\n\nAdelie\n146\n36.725\n38.85\n38.82397\n36.725\n\n\nChinstrap\n68\n46.350\n49.55\n48.83382\n46.350\n\n\nGentoo\n119\n45.350\n47.40\n47.56807\n45.350\n\n\n\n\n\n\n\nWir k√∂nnen diese Statistiken auch grafisch mit einem Boxplot darstellen:\n\ndata_penguin %&gt;%\n  na.omit() %&gt;%\n  ggplot(aes(x=species,y=bill_length_mm, fill = species))+\n  geom_boxplot()+\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\"))+\n  labs(\n    x = \"Pinguin Spezies\",\n    y = \"Schnabell√§nge in mm\"\n  )+\n  theme_minimal(base_size=14)+\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\nIm Vergleich zur Visualisierung in Aufgabe¬†1.10 implementierte die oben dargestellte Grafik folgende Anpassungen:\n\nDer Parameter fill in der Funktion aes() weist den grafischen Elementen F√ºllfarben zu.\nDie Funktion scale_fill_manual() definiert die F√ºllfarben manuell. Hier setzen wir die Farbwerte (values) auf c(\"darkorange\", \"purple\", \"cyan4\").\nMit der Funktion labs() und den Argumenten x und y passen wir die Beschriftungen der x- und y-Achse an.\nDie Funktion theme_minimal(base_size = 14) legt unter anderem einen wei√üen Hintergrund f√ºr die Grafik fest und setzt die Basis-Schriftgr√∂√üe auf 14pt.\nDie Anweisung theme(legend.position = \"none\") unterdr√ºckt die Legende der Grafik, was die √úbersichtlichkeit erh√∂ht.\n\nDer bewusste Einsatz von Farben, Formen und Texten in Grafiken vereinfacht das Lesen und Verarbeiten der enthaltenen Informationen ma√ügeblich. Die effektive visuelle Kommunikation von Daten ist eine essenzielle Kompetenz, die man in jedem Berufsfeld, welches Daten involviert, beherrschen muss.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "02_Lineare_Regression.html#die-einfache-lineare-regression",
    "href": "02_Lineare_Regression.html#die-einfache-lineare-regression",
    "title": "2¬† Lineare Regression",
    "section": "2.2 Die einfache lineare Regression",
    "text": "2.2 Die einfache lineare Regression\nDie lineare Regression erweist sich als ein vergleichsweise einfaches, aber √§u√üerst leistungsstarkes Werkzeug in der Statistik und im Data Mining. Ihre Vorteile umfassen die hervorragende Interpretierbarkeit der Koeffizienten ‚Äì sowohl hinsichtlich ihrer Werte als auch ihrer statistischen Signifikanz.\nIn der einfachen linearen Regression besteht das Ziel darin, eine abh√§ngige Variable (auch Zielvariable, Outcome oder Target genannt) \\(Y \\in \\mathbb{R}^K\\) durch die Gleichung\n\\[\nY = b_0 + b_1X + \\varepsilon\n\\tag{2.1}\\]\nzu approximieren. Hierbei ist \\(X \\in \\mathbb{R}^K\\) die unabh√§ngige Variable, \\(b_0, b_1 \\in \\mathbb{R}\\) sind die Regressionskoeffizienten (Achsenabschnitt und Steigung) und \\(\\varepsilon \\sim \\mathcal{N}(0, 1)\\) ist ein Fehlerterm, der als normalverteilt angenommen wird.\nGleichung Gleichung¬†2.1 bezeichnen wir auch als einfache lineare Regressionsgleichung oder Modellgleichung. Dabei sind \\(X\\) und \\(Y\\) Vektoren der Dimension \\(K \\in \\mathbb{N}\\), also \\(X = (x_1, \\ldots, x_K)\\) und \\(Y = (y_1, \\ldots, y_K)\\) mit \\(x_i\\in\\mathbb{R}\\) und \\(y_i\\in\\mathbb{R}\\) f√ºr alle \\(k=1,\\ldots K\\).\nWir k√∂nnen die Regressionsgleichung Gleichung¬†2.1 komponentenweise betrachten:\n\\[\ny_k = b_0 + b_1 x_k + \\varepsilon_k, \\qquad k=1,\\ldots,K\n\\]\n\\(K\\) entspricht hierbei wieder der Anzahl der Observationen.\n\nBeispiel 2.1 Betrachte folgenden synthetischen Datensatz:\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n0.56\n\n\n1\n0.73\n\n\n2\n-0.56\n\n\n3\n1.43\n\n\n4\n1.87\n\n\n5\n0.78\n\n\n6\n2.54\n\n\n7\n4.77\n\n\n8\n4.69\n\n\n9\n4.95\n\n\n10\n3.78\n\n\n\n\n\n\n\nDie folgende Grafik zeigt die Daten, visualisiert als Punktwolke:\n\n\n\n\n\n\n\n\n\nUm nun eine Regressionsgerade mit den Parametern \\(\\hat{b}_0\\) und \\(\\hat{b}_1\\) zu bestimmen, welche die abh√§ngige Variable \\(Y\\) bestm√∂glich approximiert, verwenden wir die folgenden Formeln zur Sch√§tzung der Koeffizienten:\n\\[\n\\hat b_1 = \\frac{\\sum_{k=1}^{K} x_k y_k - K \\bar{x} \\bar{y}}{\\sum_{k=1}^{K} x_k^2 - K \\bar{x}^2} \\qquad \\text{und}\\qquad \\hat b_0 = \\bar{y} - \\hat{b}_1 \\bar{x}\n\\]\nDabei repr√§sentieren \\(\\bar{x} = \\frac{1}{K}\\sum_{k=1}^K x_k\\) und \\(\\bar{y} = \\frac{1}{K}\\sum_{k=1}^K y_k\\) den Stichprobenmittelwert der unabh√§ngigen Variable \\(X\\) bzw\nF√ºr das vorherige Beispiel ergeben sich die gesch√§tzten Koeffizienten zu \\(\\hat{b}_1 = 0.4908\\) und \\(\\hat{b}_0 = 0.2253\\). Daraus resultiert die gesch√§tzte Regressionsgleichung mit eingesetzten Werten:\n\\[\n\\hat{y} =\\hat{b}_1x+\\hat{b}_0 =  0.5098x - 0.2273\n\\]\nWir k√∂nnen die Regressionsgerade dann mithilfe der geom_abline-Funktion visualisieren:\n\n\ndata_example %&gt;% ggplot(aes(x=x,y=y))+\n  geom_point(color=\"darkorange\",\n             size = 3)+\n  geom_abline(slope = 0.5098,\n              intercept = -0.2273,\n              linewidth = 1,\n              color = \"cyan4\")+\n  theme_minimal(base_size=14)\n\n\n\n\n\n\n\n\nIn Zeile 4 spezifizieren wir mit dem slope-Argument die Steigung, bzw. den Parameter \\(\\hat{b_1}\\) und in Zeile 5 durch das Argument intercept den Schnittpunkt mit der \\(y\\)-Achse, bzw. den Parameter \\(\\hat{b_0}\\). Die Argumente linewidth und color steuern die Dicke und Farbe der Linie.\n\n\n\n\n\n\n\nWichtig\n\n\n\nDie Notation der Sch√§tzgleichung unterscheidet sich von der Modellgleichung. Bei gesch√§tzten Parametern oder Werten verwendet man die Notation \\(\\hat{y},\\hat{b}_1,\\text{ und }\\hat{b}_0\\), um gesch√§tzte Werte darzustellen.\n\n\n\n2.2.1 Sch√§tzung der Modellparameter in R\nUm eine lineare Regression mit R durchzuf√ºhren, nutzen wir die {tidymodels}-Library. {tidymodels} erweitert das Konzept der {tidyverse}-Bibliothek um statistische Modellierung. Der Prozess zur Spezifikation eines linearen Modells gliedert sich hierbei, unabh√§ngig vom verwendeten Datensatz, in zwei separate Phasen:\n\nModellspezifikation\nSch√§tzung der Modellparameter\n\nDiese Phasen werden durch die Funktionen linear_reg() und fit() spezifiziert. Die Spezifikation des Modells erfolgt stets unabh√§ngig von den zugrundeliegenden Daten:\n\nlibrary(tidymodels)\n\nlm_spec &lt;- linear_reg()\n\nUm nun die Modellparameter f√ºr die Daten aus Example¬†2.1 zu sch√§tzen, kombinieren wir die Modellspezifikation mit der fit-Funktion:\n\nlm_fit &lt;- lm_spec %&gt;% fit(\n  data = data_example,\n  formula = y ~ x\n  )\n\n\nIn der ersten Zeile deklarieren wir ein neues Objekt namens lm_fit. Indem wir die zuvor definierte Modellspezifikation lm_spec an die Funktion fit() √ºbergeben, weisen wir fit() an, die Parameter f√ºr ein lineares Regressionsmodell zu sch√§tzen.\nIn der zweiten Zeile legen wir mithilfe des Arguments data den Datensatz fest, anhand dessen die Modellparameter \\(b_0\\) und \\(b_1\\) gesch√§tzt werden sollen.\nNach der Datenspezifikation m√ºssen wir noch die abh√§ngige und die unabh√§ngige Variable definieren. Dies geschieht mithilfe des Operators ~. Die Variable links des ~-Operators (y) ist die abh√§ngige Variable, und die Variable rechts des ~-Operators (x) ist die unabh√§ngige Variable.\n\nDas Objekt lm_fit enth√§lt nun unter anderem die gesch√§tzten Modellparameter \\(b_0\\) und \\(b_1\\). Es existieren verschiedene Wege, um auf diese zuzugreifen:\nDurch das Ausgeben des Objekts lm_fit erhalten wir neben den Koeffizienten \\(b_0\\) (Intercept) und \\(b_1\\) (x) beispielsweise auch die Formel, die zur Sch√§tzung der Modellparameter verwendet wurde:\n\nlm_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = y ~ x, data = data)\n\nCoefficients:\n(Intercept)            x  \n    -0.2273       0.5098  \n\n\nEine detaillierte √úbersicht der gesch√§tzten Modellparameter und deren statistischer Signifikanz erhalten wir durch folgenden Code-Abschnitt:\n\nlm_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  summary()\n\n\nCall:\nstats::lm(formula = y ~ x, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5418 -0.6913  0.1278  0.6881  1.4285 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.2273     0.5768  -0.394 0.702757    \nx             0.5098     0.0975   5.229 0.000543 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.023 on 9 degrees of freedom\nMultiple R-squared:  0.7523,    Adjusted R-squared:  0.7248 \nF-statistic: 27.34 on 1 and 9 DF,  p-value: 0.0005429\n\n\nDie Funktion extract_fit_engine() ruft das Attribut fit aus dem Objekt lm_fit ab. Das Attribut fit eines linearen Modells beinhaltet neben den gesch√§tzten Parametern zahlreiche weitere Informationen, die die Funktion summary() √ºbersichtlich zusammenfasst. Diese werden wir in den folgenden Abschnitten genauer betrachten.\n\n2.2.1.1 Residuen\nDer Abschnitt Residuals der Zusammenfassung beschreibt deskriptive Merkmale der Residuen, welche sich durch die Sch√§tzung der Modellparameter ergeben haben.\n\n\n\n\n\n\nWiederholung\n\n\n\nZur Erinnerung: F√ºr eine Observation \\((x_i,y_i)\\) mit gesch√§tztem Wert \\(\\hat{y}_i = \\hat{b}_1x_i+\\hat{b}_0\\) ist das Residuum \\(\\hat{\\varepsilon}_i\\) als \\(\\hat{\\varepsilon}_i = y_i-\\hat{y}_i = y_i - (\\hat{b}_1x_i+\\hat{b}_0)\\) definiert.\n\n\n\n\n...\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5418 -0.6913  0.1278  0.6881  1.4285 \n\nResidual standard error: 1.023 on 9 degrees of freedom\n...\n\n\nKonkret hei√üt das f√ºr das obige lineare Modell, dass etwa das kleinste Residuum den Wert \\(-1.5418\\) annimmt und \\(75\\%\\) der Residuen maximal den Wert \\(0.6881\\) besitzen.\nNeben den deskriptiven Merkmalen wird im unteren Teil der Ausgabe auch noch der Standardfehler der Residuen angegeben.\nDie gesch√§tzten Residuen f√ºr das gesch√§tzte Modell k√∂nnen auch grafisch dargestellt werden:\n\nestimates &lt;- lm_fit %&gt;%\n  predict(data_example) %&gt;%\n  pluck(1)\n\ndata_example %&gt;% ggplot(aes(x=x,y=y))+\n  geom_segment(aes(x=x,xend=x,y=y,yend=estimates),\n               linewidth = 1,\n               linetype = 2,\n               color = \"darkorange\")+\n  geom_point(color=\"darkorange\",\n             size = 3)+\n  geom_abline(slope = 0.5092,\n              intercept = -0.2253,\n              linewidth = 1,\n              color = \"cyan4\")+\n  theme_minimal(base_size=14)\n\n\n\n\n\n\n\n\nZuerst haben wir mithilfe der Funktion predict() die vorhergesagten Werte \\(\\hat{y} = \\hat{b}_0 + x\\hat{b}_1\\) berechnet. Da predict() ein Dataframe oder Tibble zur√ºckgibt, k√∂nnen wir die relevante Spalte mit purrr::pluck() als Vektor extrahieren.\nIm Vergleich zur vorherigen Visualisierung stellen wir die Residuen nun mit der Funktion geom_segment() dar. geom_segment() zeichnet gerade Linien zwischen den Punkten (x, y) und (xend, yend), wobei die Werte im Datensatz und die vorhergesagten Werte in der Liste estimates diese Endpunkte definieren. Um die Residuallinien hervorzuheben, haben wir ihre Breite (linewidth) angepasst, den Linientyp (linetype) auf ‚Äûgestrichelt‚Äú gesetzt und ihre Farbe (color) an die entsprechenden Datenpunkte angeglichen.\n\n\n2.2.1.2 Koeffizienten\nDer Coefficients Abschnitt beschreibt die gesch√§tzten Parameter und deren statistische Signifikanz.\n\n\n...\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.2273     0.5768  -0.394 0.702757    \nx             0.5098     0.0975   5.229 0.000543 ***\n...\n\n\n\nDie Spalte Estimate gibt hierbei den gesch√§tzten Wert f√ºr \\(b_0\\) und \\(b_1\\) an.\nDie Spalte Std. Error gibt f√ºr jeden gesch√§tzten Parameter den Standardfehler an.\nDie Spalte t value gibt den Wert der \\(t\\)-Statistik f√ºr den entsprechenden Parameter an.\nDie Spalte Pr(&gt;|t|) beschreibt den \\(p\\)-Wert des zugrundeliegenden \\(t\\)-Tests.\n\nZus√§tzlich werden rechts von der letzten Spalte noch Signifikanzcodes wiedergegeben:\n\n\n...\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n...\n\n\nDiese beschreiben, zu welchem Signifikanzniveau \\(\\alpha\\) ein Parameter signifikant ist. Falls also zum Beispiel bei der Variable x der Signifikanzcode '***' gegeben ist, dann bedeutet das, dass \\(0&lt;p&lt; 0.001\\).\n\n\n\n\n\n\nWiederholung\n\n\n\nDer \\(t\\)-Test f√ºr den Koeffizienten \\(b\\) einer Variable \\(x\\) einer linearen Regression lautet:\n\\[\nH_0:\\: b=0\\qquad \\text{vs.}\\qquad H_1:\\:b\\neq 0\n\\] Der Wert der \\(t\\)-Statistik ist durch \\(\\frac{\\hat{b}}{\\text{sd}_{\\hat{b}}}\\sqrt{K}\\) gegeben, wobei \\(\\text{sd}_{\\hat{b}}\\) den Standardfehler von \\(\\hat{b}\\) und \\(K\\) die Anzahl der Observationen beschreibt. Falls der Wert der \\(t\\)-Statistik im Bereich \\(B = (-\\infty,-x_{1-\\frac{\\alpha}{2}})\\cup(x_{1-\\frac{\\alpha}{2}},\\infty)\\) liegt, dann k√∂nnen wir \\(H_0\\) verwerfen. Hierbei verwenden wir die Approximation \\(x_{1-\\frac{\\alpha}{2}}\\approx z_{1-\\frac{\\alpha}{2}}\\), wobei \\(z_{1-\\frac{\\alpha}{2}}\\) das \\(1-\\frac{\\alpha}{2}\\) Quantil der Standardnormalverteilung ist.\nFalls wir \\(H_0\\) nicht verwerfen (z.¬†B. zum Niveau \\(\\alpha = 0.001\\)), k√∂nnen wir nicht mit ausreichender Sicherheit sagen, dass \\(\\hat{b}\\neq 0\\) und der Parameter somit einen Einfluss auf die abh√§ngige Variable \\(y\\) hat.\n\n\n\n\n\n2.2.2 Das G√ºtema√ü \\(R^2\\)\nNeben den Parametern, deren Signifikanz und den Residuen sind wir ebenso an dem G√ºtema√ü \\(R^2\\) interessiert. Das G√ºtema√ü \\(R^2\\) beschreibt in Worten gefasst, wie viel Prozent der Varianz der Daten wir durch unser Modell erkl√§ren k√∂nnen.\n\n\n...\n\nMultiple R-squared:  0.7523,    Adjusted R-squared:  0.7248 \n...\n\n\nFalls also, wie im obigen Output zu sehen ist, der \\(R^2\\) des Modells bei \\(0.7526\\) liegt, kann unser Modell ca. \\(75\\%\\) der Varianz in den Daten erkl√§ren.\nUm das G√ºtema√ü zu berechnen, wird folgende Formel herangezogen:\n\\[\nR^2  = \\frac{\\sum_{k=1}^K(\\hat{y}_k-\\bar{y})^2}{\\sum_{k=1}^K(y_k-\\bar{y})^2}\n\\tag{2.2}\\]\n\n2.2.2.1 Der \\(F\\)-Test\nDie letzte Zeile der Zusammenfassung des Modells zeigt die \\(F\\)-Statistik mit korrespondierendem \\(p\\)-Wert.\n\n\n...\n\nF-statistic: 27.34 on 1 and 9 DF,  p-value: 0.0005429\n...\n\n\nDer \\(F\\)-Test beschreibt, ob das Modell als gesamtes signifikant ist. Die zugrunde liegenden Hypothesen des \\(F\\)-Tests sind:\n\\[ H_0:\\: b_i = 0 \\text{ f√ºr alle } i,\\qquad\\text{ vs }\\qquad H_1:\\: b_i\\neq 0 \\text{ f√ºr mindestens ein } i\\]\nFalls wir \\(H_0\\) nicht verwerfen, k√∂nnen wir nicht mit hoher Sicherheit sagen, dass alle Regressoren keinen Einfluss auf die abh√§ngige Variable haben. Im Kontext der einfachen linearen Regression sind der \\(F\\)-Test und \\(t\\)-Test insofern √§quivalent, dass\n\\[\n\\verb|F-statistic| = (\\verb|t value|)^2\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "02_Lineare_Regression.html#die-multiple-lineare-regression",
    "href": "02_Lineare_Regression.html#die-multiple-lineare-regression",
    "title": "2¬† Lineare Regression",
    "section": "2.3 Die multiple lineare Regression",
    "text": "2.3 Die multiple lineare Regression\nDie multiple lineare Regression erweitert die Idee der einfachen linearen Regression durch das Hinzuf√ºgen von weiteren erkl√§renden Variablen.\nDie Modellgleichung lautet somit\n\\[\nY = b_0 + b_1X_1+...+b_JX_J+\\varepsilon,\n\\]\nfalls die Daten aus \\(J\\) verschiedenen Variablen bestehen. Hierbei gilt wieder, dass f√ºr jede Variable \\(X_j,\\:j=1,\\ldots,J\\) \\(K\\) Observationen vorhanden sind. Man geht hierbei davon aus, dass die \\(K\\) Observationen zuf√§llig von einer Gesamtpopulation gezogen wurden.\nSomit kann die Modellgleichung auch wieder komponentenweise beschrieben werden:\n\\[\ny_k = b_0 + b_1x_{k1}+...+b_Jx_{kJ}+\\varepsilon_k\n\\]\nZus√§tzlich kann die Modellgleichung auch in Matrixnotation dargestellt werden:\n\\[\nY = Xb+\\varepsilon\n\\tag{2.3}\\]\nWobei \\(Y = (y_1,...,y_k)^\\prime\\in\\mathbb{R}^{K\\times 1},\\, X = (1,x_1,...,x_J)\\in\\mathbb{R}^{K\\times (J+1)}, b = (b_0,b_1,...,b_J)^\\prime\\in\\mathbb{R}^{(J+1)\\times 1}\\) und \\(\\varepsilon \\sim \\mathcal{N(0,1)}\\).\nDas Sch√§tzen der Modellparameter \\(b_0,\\ldots,b_J\\) auf Basis gegebener Observationen wird bei der multiplen linearen Regression √ºblicherweise nicht mehr von Hand durchgef√ºhrt. Der einfachste Weg, die Modellparameter von Hand zu bestimmen, ist allerdings durch folgende Formel gegeben:\n\\[\nb = (X^\\prime X)^{-1}X^\\prime Y\n\\tag{2.4}\\]\nWobei \\(X^\\prime\\) die transponierte Matrix von \\(X\\) darstellt und \\((X^\\prime X)^{-1}\\) die inverse von \\((X^\\prime X)\\). Die Komplexit√§t in der Berechnung von \\(b\\) anhand Gleichung¬†2.4 liegt in der Berechnung der inversen Matrix.\n\n2.3.1 Multiple lineare Regression in R\nDas Sch√§tzen der Modellparameter im multidimensionalen Fall in R funktioniert fast analog wie im einfachen Fall.\nAls Beispiel verwenden wir hierf√ºr nun keinen synthetischen Datensatz mehr, sondern den data_penguin-Datensatz aus Section 2.1.\nAngenommen, die abh√§ngige Variable ist das Gewicht der Pinguine (body_mass_g). Wie im einfachen linearen Regressionsmodell wird zuerst das Modell durch die linear_reg-Funktion spezifiziert. Nach der Spezifikation werden dann die Parameter mithilfe der fit-Funktion gesch√§tzt. Das Argument formula hat rechts des ~-Operators einen Punkt stehen. Der .-Operator signalisiert, dass alle verbleibenden Variablen als unabh√§ngige Variablen verwendet werden sollen.\n\nmlr_spec &lt;- linear_reg()\nmlr_fit &lt;- mlr_spec %&gt;%\n  fit(data = data_penguin,\n      formula = body_mass_g ~.)\n\nAlternativ kann man statt des . auch alle Variablen wie folgt ausschreiben, wobei das in den meisten F√§llen aufgrund schlechter Lesbarkeit nicht zu empfehlen ist:\n\nmlr_fit &lt;- mlr_spec %&gt;%\n  fit(data = data_penguin,\n      formula = body_mass_g ~ species + island + bill_length_mm +\n        bill_depth_mm + flipper_length_mm + sex + year)\n\nAnalog zum eindimensionalen Fall kann mit den extract_fit_engine- und summary-Funktion eine Zusammenfassung erzeugt werden:\n\nmlr_fit_summary &lt;- mlr_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  summary()\n\nmlr_fit_summary\n\n\nCall:\nstats::lm(formula = body_mass_g ~ species + island + bill_length_mm + \n    bill_depth_mm + flipper_length_mm + sex + year, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-809.70 -180.87   -6.25  176.76  864.22 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       84087.945  41912.019   2.006  0.04566 *  \nspeciesChinstrap   -282.539     88.790  -3.182  0.00160 ** \nspeciesGentoo       890.958    144.563   6.163 2.12e-09 ***\nislandDream         -21.180     58.390  -0.363  0.71704    \nislandTorgersen     -58.777     60.852  -0.966  0.33482    \nbill_length_mm       18.964      7.112   2.667  0.00805 ** \nbill_depth_mm        60.798     20.002   3.040  0.00256 ** \nflipper_length_mm    18.504      3.128   5.915 8.46e-09 ***\nsexmale             378.977     48.074   7.883 4.95e-14 ***\nyear                -42.785     20.949  -2.042  0.04194 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 286.5 on 323 degrees of freedom\n  (11 Beobachtungen als fehlend gel√∂scht)\nMultiple R-squared:  0.8768,    Adjusted R-squared:  0.8734 \nF-statistic: 255.4 on 9 and 323 DF,  p-value: &lt; 2.2e-16\n\n\nDie Modellgleichung f√ºr das multiple lineare Regressionsmodell mit eingesetzten Zahlen lautet also\n\\[\n\\begin{aligned}\n\\verb|body_mass_g|_i = 84087.945 &+\\verb|speciesChinstrap|_i\\cdot-282.53+\\\\\n&\\verb|speciesGentoo|_i\\cdot890.958+ \\verb|islandDream|_i\\cdot-21.180+\\\\\n&\\verb|islandTorgersen|_i\\cdot-58.777 + \\verb|bill_length_mm|_i\\cdot18.964+ \\\\\n&\\verb|bill_depth_mm|_i\\cdot60.798    + \\verb|flipper_length_mm|_i\\cdot18.504+\\\\\n&\\verb|sexmale|_i\\cdot378.977+\\verb|year|_i\\cdot-42.785+\\varepsilon_i\n  \\end{aligned}\n\\tag{2.5}\\]\n\n\n2.3.2 Interpretation und Vergleich der Koeffizienten\nDie gesch√§tzten Koeffizienten im MLR-Modell lassen sich √§hnlich wie bei der einfachen linearen Regression interpretieren, da der Effekt der Koeffizienten auch linear ist.\nSo erwarten wir zum Beispiel im obigen Beispiel, dass das Gewicht eines Pinguins im Schnitt um 18.504\\(g\\) steigt, falls sich die L√§nge der Flossen um 1¬†mm erh√∂ht (\\(b_{\\verb|flipper_length_mm|} = 18.504\\)) und alle anderen Einflussfaktoren konstant bleiben.\nNeben dem direkten Einfluss der gesch√§tzten Koeffizienten k√∂nnen wir zwar die statistische Signifikanz der verschiedenen Parameter vergleichen, aber es ist ohne Weiteres nicht klar, welcher Koeffizient den gr√∂√üten Einfluss auf das Gewicht der Pinguine hat.\nUm herauszufinden, welche Variable den gr√∂√üten Einfluss besitzt, m√ºssen wir entweder die Variablen oder die Koeffizienten standardisieren. Ohne eine Standardisierung w√§re ein Vergleich der gesch√§tzten Koeffizienten nicht sinnvoll, da die Variablen teilweise in anderen Einheiten gemessen werden oder gar nicht metrisch sind. So macht etwa ein Vergleich der Parameter f√ºr die Schnabell√§nge und das Geschlecht keinen Sinn, da die Schnabell√§nge in \\(mm\\) gemessen wird und das Geschlecht nur die Auspr√§gungen \"male\" und \"female\" besitzt. Den standardisierten Koeffizienten f√ºr eine Variable \\(x\\) k√∂nnen wir mithilfe der Formel\n\\[\n\\hat{b}_{x,\\text{std}} = \\hat{b}_x\\frac{\\text{sd}_x}{\\text{sd}_y}\n\\] berechnen, wobei \\(\\text{sd}_x\\) f√ºr die Standardabweichung von \\(x\\) und \\(\\text{sd}_y\\) f√ºr die Standardabweichung von \\(y\\) steht.\nDurch die Standardisierung erreichen wir, dass die standardisierten Koeffizienten vergleichbar sind. Der gesch√§tzte Koeffizient mit dem gr√∂√üten standardisierten Wert hat dadurch den gr√∂√üten Einfluss auf die abh√§ngige Variable, w√§hrend der Koeffizient mit dem kleinsten standardisierten Wert den kleinsten Einfluss hat.\nDurch die lm.beta::lm.beta()-Funktion k√∂nnen wir die standardisierten Koeffizienten f√ºr ein lineares Modell in R bestimmen:\n\nmlr_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  lm.beta::lm.beta() %&gt;%\n  summary()\n\n\nCall:\nstats::lm(formula = body_mass_g ~ species + island + bill_length_mm + \n    bill_depth_mm + flipper_length_mm + sex + year, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-809.70 -180.87   -6.25  176.76  864.22 \n\nCoefficients:\n                    Estimate Standardized Std. Error t value Pr(&gt;|t|)    \n(Intercept)        8.409e+04           NA  4.191e+04   2.006  0.04566 *  \nspeciesChinstrap  -2.825e+02   -1.417e-01  8.879e+01  -3.182  0.00160 ** \nspeciesGentoo      8.910e+02    5.311e-01  1.446e+02   6.163 2.12e-09 ***\nislandDream       -2.118e+01   -1.271e-02  5.839e+01  -0.363  0.71704    \nislandTorgersen   -5.878e+01   -2.545e-02  6.085e+01  -0.966  0.33482    \nbill_length_mm     1.896e+01    1.288e-01  7.112e+00   2.667  0.00805 ** \nbill_depth_mm      6.080e+01    1.487e-01  2.000e+01   3.040  0.00256 ** \nflipper_length_mm  1.850e+01    3.221e-01  3.128e+00   5.915 8.46e-09 ***\nsexmale            3.790e+02    2.357e-01  4.807e+01   7.883 4.95e-14 ***\nyear              -4.278e+01   -4.320e-02  2.095e+01  -2.042  0.04194 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 286.5 on 323 degrees of freedom\n  (11 Beobachtungen als fehlend gel√∂scht)\nMultiple R-squared:  0.8768,    Adjusted R-squared:  0.8734 \nF-statistic: 255.4 on 9 and 323 DF,  p-value: &lt; 2.2e-16\n\n\nIm obigen Code haben wir nach der Erzeugung der standardisierten Koeffizienten auch noch die summary()-Funktion aufgerufen, um neben den standardisierten Koeffizienten auch noch eine √úbersicht √ºber die restlichen Parameter und Statistiken zu bekommen.\n\n2.3.2.1 Dummy-Encoding\nBeim Vergleichen der unabh√§ngigen Variablen mit dem R-Output werden zum Beispiel statt species die beiden neuen Variablen speciesChinstrap und speciesGentoo verwendet. Das liegt daran, dass diese Variable nominal ist (Auspr√§gungen \"Chinstrap\",\"Gentoo\" und \"Adelie\") und die Auspr√§gungen deshalb nicht einfach mit gesch√§tzten Koeffizienten multipliziert werden k√∂nnen, wie das im ordinalen oder metrischen Fall ist. Deshalb werden sogenannte Dummy-Variablen zur Berechnung der Parameter verwendet. Hierbei werden im Falle der Variable species \\(l-1 = 3-1 = 2\\) (mit \\(l\\) die Anzahl der verschiedenen Auspr√§gungen) neue Variablen angelegt. speciesChinstrap nimmt den Wert 1 an, falls ein Datenpunkt im Datensatz der Spezies Chinstrap angeh√∂rt, und ansonsten 0. Gleich wird auch mit speciesGentoo verfahren. Allerdings ist hier nirgends die Variable speciesAdelie spezifiziert! Das liegt daran, dass der Einfluss dieses Datenpunktes durch den Intercept, also \\(b_0\\), abgebildet wird, um Multikollinearit√§t zu vermeiden (siehe Section 2.3.3.5).\nDas gleiche Vorgehen wurde auch bei den Variablen island und sex verwendet.\n\n\n\n2.3.3 Modellannahmen\nUnter welchen Voraussetzungen macht es Sinn, ein lineares Modell zu verwenden, und wie wird gepr√ºft, ob diese erf√ºllt sind?\nDer Hintergrund dieser Voraussetzungen liegt im sogenannten Gauss-Markov-Theorem:\n\nTheorem 2.1 (Gauss-Markov) Gegeben sei ein lineares Modell wie in Gleichung¬†2.3 mit unbekannten Parametern \\(b = (b_0,\\ldots, b_J)\\). Angenommen, es gilt \\(\\varepsilon \\sim \\mathcal{N}(0,\\Sigma)\\) mit \\(\\Sigma = \\sigma^2 I_J\\in\\mathbb{R}^{J\\times J}\\), wobei \\(I_J\\) eine \\(J\\)-dimensionale Einheitsmatrix ist und \\(\\sigma^2&gt;0\\). Wenn zus√§tzlich die Matrix \\(X^\\prime X\\) invertierbar ist, dann gilt:\nDer Kleinste-Quadrate-Sch√§tzer f√ºr den wahren Parametervektor \\(b\\), gegeben durch Gleichung¬†2.4, ist ein minimalvarianter linearer erwartungstreuer Sch√§tzer f√ºr den wahren Parameter \\(b\\).\n\nEinfach gesagt bedeutet das:\n\nFalls die einzelnen St√∂rgr√∂√üen \\(\\varepsilon_i,\\: i=1,...,J\\) normalverteilt sind mit Erwartungswert \\(0\\) und Varianz \\(\\sigma^2&gt;0\\) und zus√§tzlich die Variablen \\(X_1,...,X_J\\) linear unabh√§ngig voneinander sind, dann erwarten wir, dass der Sch√§tzer \\(\\hat{b}\\) im Mittel dem wahren Parameter entspricht und es keinen anderen erwartungstreuen Sch√§tzer \\(\\tilde{\\hat{b}}\\) gibt, welcher eine bessere Modellg√ºte liefert.\n\nAus diesen Aussagen lassen sich folgende Bedingungen herleiten:\n\nDie Beziehung zwischen den unabh√§ngigen und der abh√§ngigen Variable muss tats√§chlich linear sein.\nDie Varianz der Residuen muss konstant sein (Homoskedastizit√§t).\nDie Residuen sind unabh√§ngig (keine Autokorrelation).\nDie Residuen sind normalverteilt.\n\n\n2.3.3.1 Lineare Beziehung\nEine grundlegende Bedingung ist, dass die Beziehung zwischen der abh√§ngigen Variable und den (metrischen) erkl√§renden Variablen tats√§chlich linear ist. Falls diese Bedingung nicht erf√ºllt ist, k√∂nnen die Effekte der verschiedenen unabh√§ngigen Variablen nicht richtig interpretiert werden. Um zu pr√ºfen, ob diese erf√ºllt ist, k√∂nnen die paarweise Punktewolken zwischen der abh√§ngigen und den unabh√§ngigen Variablen betrachtet werden.\nDas folgende Code-Snippet erzeugt drei Punktwolken, welche jeweils auf der x-Achse die abh√§ngige Variable body_mass_g und auf der y-Achse die metrischen unabh√§ngigen Variablen abbilden.\n\ndata_penguin %&gt;%\n  select(ends_with(\"mm\"),body_mass_g) %&gt;%\n  na.omit() %&gt;%\n  pivot_longer(-body_mass_g) %&gt;%\n    ggplot(aes(y=value,x=body_mass_g))+\n    geom_point()+\n    facet_grid(rows = vars(name) ,scales = \"free\")+\n    theme_minimal(base_size = 10)\n\n\n\n\n\n\n\n\nIn den ersten vier Zeilen wird der Datensatz f√ºr die Darstellung aufbereitet:\n\nIn Zeile 2 werden mithilfe der select- und ends_with-Funktion alle Variablen des Datensatzes data_penguin ausgew√§hlt, deren Name entweder mit \"mm\" endet oder body_mass_g ist. Durch diese Zeile werden alle metrischen Werte ausgew√§hlt.\nIn Zeile 3 werden alle Eintr√§ge entfernt, welche fehlende Werte beinhalten.\nDie pivot_longer-Funktion wandelt die √ºbergebenen Daten in eine verl√§ngerte Version um, welche weniger Spalten, aber mehr Zeilen enth√§lt:\n\n\n# A tibble: 6 √ó 3\n  body_mass_g name              value\n        &lt;int&gt; &lt;chr&gt;             &lt;dbl&gt;\n1        3750 bill_length_mm     39.1\n2        3750 bill_depth_mm      18.7\n3        3750 flipper_length_mm 181  \n4        3800 bill_length_mm     39.5\n5        3800 bill_depth_mm      17.4\n6        3800 flipper_length_mm 186  \n\n\nDurch das Spezifizieren von -body_mass_g wird erreicht, dass alle Variablen au√üer body_mass_g in zwei Spalten name und value zusammengefasst werden. Der verl√§ngerte Datensatz enth√§lt also nur noch die drei Spalten body_mass_g,name und value. In der Variable name sind die urspr√ºnglichen Variablennamen gespeichert und in value der dazugeh√∂rige Wert. Dieses Vorgehen erlaubt im n√§chsten Schritt das Erzeugen von mehreren Plots in Kombination mit der facet_grid-Funktion.\nDie facet_grid-Funktion erzeugt eine Matrix von Plots, wobei durch das Argument rows = vars(name) spezifiziert wird, dass die Auspr√§gungen der Variable names die Zeilen der Plotmatrix abbilden sollen.\n\nAn der obigen Grafik kann abgelesen werden, dass der Zusammenhang zwischen den unabh√§ngigen Variablen und body_mass_g linear ist, da ein linearer Trend erkennbar ist. Selbst bei der Variable bill_depth_mm, in welcher zwei distinkte Gruppen erkennbar sind, l√§sst sich jeweils ein linearer Trend feststellen.\n\n\n2.3.3.2 Konstante Varianz der Residuen\nUm die Residuen auf konstante Varianz zu pr√ºfen, verwenden wir √§hnlich wie im vorherigen Abschnitt ein visuelles Hilfsmittel: Indem wir die Residuen in Abh√§ngigkeit von den Vorhersagewerten als Punktwolke darstellen, k√∂nnen wir die Streuung um den Ursprung betrachten.\n\n\n\n\n\n\n\n\n\nIn der obigen Grafik ist die Varianz der Residuen vermutlich konstant.\nBegr√ºndung: Obwohl unter den Vorhersagewerten preds in den Bereichen [3750,4250] und [5250,5500] Punkte zu erkennen sind, welche als Ausrei√üer interpretiert werden k√∂nnten, sind die restlichen Punkte eher gleichm√§√üig um den Ursprung verteilt. Gegeben dieser Observation k√∂nnen wir die Vermutung rechtfertigen, dass die Streuung der Residuen konstant ist.\nUm eine Grafik wie die obige zu erzeugen, m√ºssen wir zuerst die Residuen und dann die Vorhersagen erzeugen.\nDie Residuen k√∂nnen wir √§hnlich wie die Zusammenfassung der Modellparameter erzeugen, indem wir zuerst das fit-Attribut des Modells aus dem Objekt mlr_fit mithilfe der extract_fit_engine-Funktion extrahieren. Nach der Extraktion k√∂nnen wir dann mit der residuals-Funktion die Residuen auslesen.\n\nresid&lt;-mlr_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  residuals()\n\nEin √§hnliches Vorgehen k√∂nnen wir auch bei den Vorhersagen anwenden. Allerdings m√ºssen wir die Vorhersagen erst mithilfe der predict-Funktion erzeugen. Um Vorhersagen f√ºr alle Observationen zu erhalten, reicht es, den gesamten Datensatz als Argument zu √ºbergeben. Durch die na.omit-Funktion werden dann noch alle fehlenden Werte entfernt.\n\npreds &lt;- mlr_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  predict(data_penguin) %&gt;%\n  na.omit()\n\nNach dem Erzeugen der Residuen und der Vorhersagewerte k√∂nnen wir diese in einem neuen Datensatz speichern und direkt an die ggplot-Funktion √ºbergeben:\n\ntibble(\n  predictions = preds,\n  residuals = resid\n) %&gt;%\n  ggplot(aes(x=preds,y=resid))+\n  geom_point()+\n  theme_minimal(base_size=14)\n\nNeben den Residuen ist es manchmal auch sinnvoll, die Quadratwurzel des Betrags der standardisierten Residuen zu betrachten.\n\n\n\nQuelle\n\n\nDie gesch√§tzten Residuen \\(\\hat\\varepsilon = \\hat{y}-y\\) werden standardisiert, indem man durch eine empirische Standardabweichung teilt.\nSo ergibt sich\n\\[\n\\hat{\\varepsilon}_{\\text{std}} = \\frac{\\hat{\\varepsilon}}{\\text{sd}_{\\hat\\varepsilon}}\n\\]\nDie empirische Standardabweichung wird hierbei durch den Term\n\\[\n\\text{sd}_{\\hat{\\varepsilon}} = \\hat{\\sigma}\\sqrt{(K-1)/K}\n\\] mit\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{K-J}\\sum_{k=1}^K\\hat{\\varepsilon}^2_k\n\\] gesch√§tzt.\nDie Motivation f√ºr standardisierte Residuen ist, dass, obwohl unser Modell Homoskedastizit√§t mit einem i.¬†i.¬†d.-Fehlerterm mit fester Varianz \\(\\varepsilon_k\\sim \\mathcal{N}(0,1)\\) annimmt, die Verteilung der Residuen \\(\\varepsilon_k\\) nicht i.¬†i.¬†d.¬†sein kann, da die Summe der Residuen immer genau null ist.\nEinfach gesagt bedeutet das: Beim Darstellen der nicht standardisierten Residuen sind die Abweichungen schwieriger zu erkennen.\nFolgendes Beispiel illustriert diesen Effekt:\n\n\n\n\n\n\n\n\n\nFalls die Daten in der rechten Spalte ebenso homoskedastisch w√§ren, m√ºsste die Trendlinie in der letzten Grafik auch horizontal sein. Anhand der anderen beiden Plots l√§sst sich kaum erkennen, ob Heteroskedastie vorliegt.\nIm Beispiel des Pinguin-Datensatzes sieht die Darstellung der Residuen wie folgt aus:\n\n\n\n\n\n\n\n\n\nIn den Residuen wird ein kleiner Abw√§rtstrend sichtbar, welcher allerdings so gering ist, dass f√ºr das gegebene Modell keine Heteroskedastie vermutet werden sollte.\nDie obige Grafik wurde durch folgenden Code erzeugt:\n\nresid&lt;-mlr_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  rstudent()\n\ntibble(\n  predictions = preds,\n  residuals = sqrt(abs(resid))\n) %&gt;%\n  ggplot(aes(x=preds,y=residuals))+\n  geom_point()+\n  geom_smooth(method = \"lm\", formula = y~x, se = FALSE)+\n  theme_minimal(base_size=14)\n\n√Ñhnlich wie bei der vorherigen Grafik wurde mithilfe von ggplot eine Punktewolke durch die Residuen und Vorhersagen erzeugt. Der Unterschied liegt hierbei in den Zeilen 3, 7 und 11.\n\nIn Zeile 3 wurde nicht die residuals()-Funktion verwendet, sondern die rstudent. Diese erzeugt die standardisierten Residuen, wie sie im vorherigen Abschnitt beschrieben wurden.\nIn Zeile 7 wird die Quadratwurzel des Betrags der standardisierten Residuen durch die Funktionen sqrt() und abs() berechnet.\nIn Zeile 11 wird mithilfe der geom_smooth-Funktion eine Trendlinie hinzugef√ºgt. Das Argument method = \"lm\" sorgt daf√ºr, dass die Trendlinie linear ist. Durch formula = y~x wird die Formel der Trendlinie spezifiziert. Die Notation mit y und x bezieht sich hierbei auf die Variablen y und x welche in den Aesthetics in Zeile 9 √ºbergeben wurden. Das Argument se=FALSE steuert, dass keine Konfidenzstreifen um die Trendlinien hinzugef√ºgt werden.\n\n\n\n2.3.3.3 Autokorrelation\nUm zu √ºberpr√ºfen, ob die Residuen unabh√§ngig voneinander sind, kann man ebenso als visuelles Hilfsmittel eine Punktewolke wie bei der √úberpr√ºfung der Homoskedastizit√§tsannahme erstellen.\n\n\n\n\n\n\n\n\n\nIn der obigen Grafik √ºberpr√ºfen wir allerdings nicht nur, ob die Trendlinie horizontal ist, sondern ob diese linear ist. Beim Erzeugen der Trendlinie wurde n√§mlich nicht wie zuvor method = \"lm\" verwendet, sondern method = \"loess\". \"loess\" steht in diesem Kontext f√ºr locally estimated scatterplot smoothing. Wichtig hierbei ist lediglich zu wissen, dass ein LOESS-Modell auch nichtlineare Zusammenh√§nge zwischen der abh√§ngigen und den unabh√§ngigen Variablen abbilden kann.\nDie Trendlinie in der obigen Grafik ist leicht nichtlinear, folgt allerdings keinem erkennbaren Trend, sodass wir hier eine leichte Korrelation der Residuen unterstellen k√∂nnen.\n\n\n\n2.3.3.4 Normalit√§t der Residuen\nDie letzte Annahme des Gauss‚ÄìMarkov‚ÄìTheorems l√§sst sich durch verschiedene visuelle Hilfsmittel √ºberpr√ºfen.\n\n2.3.3.4.1 QQ-Plots\nH√§ufig wird ein sogenannter QQ-Plot (Quantil-Quantil-Plot) verwendet, um zu √ºberpr√ºfen, ob die unterliegende Verteilung einer Normalverteilung entspricht. Hierbei werden den empirischen Quantilen (der unterliegenden Residuen) die theoretischen Quantile gegen√ºbergestellt. Falls die Punkte hierbei sehr nah an der erzeugten Geraden liegen, so geht man davon aus, dass die Quantile der empirischen Verteilung gleich der Normalverteilung sind und somit die Residuen ebenso normalverteilt sind.\n\nresid &lt;- mlr_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  residuals() %&gt;%\n  tibble(residuals = .)\n\nresid %&gt;% ggplot(aes(sample=residuals))+\n  geom_qq()+\n  geom_qq_line()+\n  theme_minimal(base_size=14)\n\n\n\n\n\n\n\n\nDa die empirischen Quantile der Residuen relativ nah an der erzeugten Geraden liegen, kann man davon ausgehen, dass diese normalverteilt sind.\nUm die obige Grafik zu erzeugen, werden wieder zuerst die Residuen in den Codezeilen 1-4 erzeugt. Hierbei werden die Residuen allerdings in einem Tibble bzw. Data Frame gespeichert, da der R√ºckgabewert der residuals()-Funktion den Datentyp &lt;dbl&gt; besitzt. Die ggplot-Funktion setzt allerdings voraus, dass die Daten als Data Frame oder Tibble √ºbergeben werden. Das Argument residuals = . sorgt daf√ºr, dass im Tibble eine neue Spalte residuals erzeugt wird, welche als Werte den Output der residuals()-Funktion √ºbergeben bekommt.\nUm dann den QQ-Plot zu erzeugen, muss man in den Aestethics der ggplot-Funktion das Argument sample setzen (anstatt x und y wie gewohnt). Nachdem das Sample auf residuals gesetzt wurde, k√∂nnen dann mithilfe der geom_qq() und der geom_qq_line()-Funktion die Punkte und die Gerade erzeugt werden.\n\n\n\n\n\n\nWiederholung\n\n\n\nUm f√ºr eine gegebene Stichprobe \\(x_1,\\ldots,x_K\\) die empirischen Quantile zu berechnen, wird die Stichprobe zuerst sortiert (Ordnungsstatistik). Die sortierte Stichprobe wird durch die Notation \\(x_{(1)},\\ldots,x_{(K)}\\) mit \\(x_{(1)}\\leq x_{(2)}\\leq\\ldots \\leq x_{(K)}\\) dargestellt. F√ºr \\(p\\in(0.1)\\) ist dann das empirische \\(p\\)-Quantil definiert als\n\\[\n\\tilde x_p = \\begin{cases}\n              x_{(\\lfloor K\\cdot p\\rfloor+1)} \\quad \\text{f√ºr }K\\cdot p\\notin\\mathbb{Z}\\\\\n              \\frac{x_{(K\\cdot p)}+x_{(K\\cdot p+1)}}{2}\\text{f√ºr }K\\cdot p\\in\\mathbb{Z}\\\\\n              \\end{cases}\n\\]\n\n\n\n\n2.3.3.4.2 Vergleich der Histogramme mit der Dichte einer Normalverteilung\nNeben den Quantilen bietet sich auch die Betrachtung des Histogramms der Residuen an:\n\nresid %&gt;% \n  ggplot(aes(x=residuals))+\n  geom_histogram()+\n  theme_minimal(base_size = 14)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nUm das Histogramm, welches durch die geom_histogram-Funktion erzeugt wurde, mit der Dichte einer Normalverteilung vergleichen zu k√∂nnen, m√ºssen wir zuerst die y-Achse so anpassen, dass diese nicht mehr die absoluten H√§ufigkeiten, sondern die relativen H√§ufigkeiten darstellt. Durch das Setzen von y=after_stat(density) in den Aesthetics der geom_histogram‚Å£-Ebene wird dies erreicht.\n\nresid %&gt;% \n  ggplot(aes(x=residuals))+\n  geom_histogram(aes(y= after_stat(density)))+\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nNach dem Festlegen der Darstellung der H√§ufigkeit kann als N√§chstes die Dichte einer Normalverteilung hinzugef√ºgt werden. Die stat_function()-Funktion erleichter das Hinzuf√ºgen der Dichte einer Normalverteilung um ein vielfaches, da hier vorerst nur das Argument fun=dnorm √ºbergeben werden muss.\n\nresid %&gt;% \n  ggplot(aes(x=residuals))+\n  geom_histogram(aes(y= after_stat(density)))+\n  stat_function(fun = dnorm, color = \"red\", linewidth = 1.5)+\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nProblematisch ist allerdings, dass die Dichte einer Standardnormalverteilung (\\(\\mathcal{N}(0,1)\\)) abgebildet wird, f√ºr welche gilt \\(f(x)\\approx 0,\\: |x|&gt;4\\). Die rote Linie in der obigen Grafik stellt die Dichte der Standardnormalverteilung dar. F√ºr das Histogramm gilt allerdings, dass selbst f√ºr \\(|\\verb|residuals|| &gt; 500\\) noch Observationen erkennbar sind. Es m√ºssen also entweder die Werte der Residuen skaliert werden, oder die Parameter der Normalverteilung angepasst werden. Um die Parameter der Normalverteilung anzupassen, verwendet man das args Argument. Die Funktion dnorm besitzt die Argumente mean und sd, was f√ºr den Mittelwert und die Standardabweichung der Normalverteilung steht. Indem man das Argument mean auf den Mittelwert der Residuen und sd auf die Standardabweichung der Residuen setzt, sollte sich also die Dichte einer Normalverteilung erzeugen lassen, welche wir mit dem Histogramm vergleichen k√∂nnen!\nUm die beiden Argumente mean und sd zu √ºbergeben, kann man diese zuerst separat berechnen und dann als Liste list(mean = ..., sd = ...) √ºbergeben:\n\nmean_res &lt;- mlr_fit %&gt;%\n  extract_fit_engine()%&gt;%\n  residuals() %&gt;%\n  mean()\n\nsd_res &lt;- mlr_fit %&gt;%\n  extract_fit_engine()%&gt;%\n  residuals() %&gt;%\n  sd()\n\nresid %&gt;% \n  ggplot(aes(x=residuals))+\n  geom_histogram(aes(y= after_stat(density)))+\n  stat_function(fun = dnorm,\n                color = \"red\",\n                linewidth = 1.5,\n                args = list(mean = mean_res,\n                            sd = sd_res))+\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nWie an der obigen Grafik zu erkennen ist, passt die dargestellte Dichte gut zu dem erzeugtem Histogramm. Wir k√∂nnen also auch daraus schlie√üen, dass die Residuen vermutlich normalverteilt sind.\n\n\n\n2.3.3.5 Multikollinearit√§t\nEine weitere Bedingung, welche zwar nicht durch das Gauss‚ÄìMarkov Theorem vorausgesetzt wird, aber trotzdem wichtig ist, ist die der nichtvorhandenen Multikollinearit√§t der erkl√§renden Variablen.\nMultikollinearit√§t ist wie folgt definiert:\nFalls f√ºr einen Datensatz mit erkl√§renden Variablen \\(X_1,\\ldots,X_J\\in\\mathbb{R}^K\\) eine Teilmenge \\(X_i,X_{i+1},\\ldots,X_j\\) mit \\(1\\leq i&lt;j+1\\leq J\\) existiert, sodass\n\\[\nX_i = \\sum_{n=i}^{j} \\alpha_n X_{n+1}, \\quad \\alpha_1,...,\\alpha_n\\in\\mathbb{R}\\setminus\\{0\\}\n\\tag{2.6}\\]\ndann sind die Variablen \\(X_i,X_{i+1},...,X_j\\) multikollinear. Die rechte Seite der Gleichung¬†2.6 wird auch Linearkombination der Vektoren \\(X_{i+1},...,X_{j+1}\\) genannt. Die Vektoren \\(X_i\\) und \\(X_{i+1},...,X_{j+1}\\) sind somit linear abh√§ngig. Falls keine Konstanten \\(\\alpha_1,\\ldots,\\alpha_n\\in\\mathbb{R}\\setminus\\{0\\}\\) existieren, sodass in Gleichung¬†2.6 Gleichheit gilt, linearer Unabh√§ngigkeit der Vektoren.\n\nBeispiel 2.2 Seien \\[\\begin{equation*}\n      X_1 = \\begin{pmatrix}\n        1 & 0 & 0\\\\\n        0 & 1 & 0\\\\\n        0 & 0 & 1\\\\\n        0 & 1 & 1\\\\\n      \\end{pmatrix} \\quad\n      X_2 = \\begin{pmatrix}\n        1 & 0 & 1\\\\\n        1 & 0 & 1\\\\\n        0 & 1 & 1\\\\\n        0 & 1 & 1\\\\\n      \\end{pmatrix}.\n    \\end{equation*}\\]\n\nDie Spalten von \\(X_1\\) sind paarweise linear unabh√§ngig und linear unabh√§ngig.\nDie Spalten von \\(X_2\\) sind paarweise linear unabh√§ngig, aber nicht linear unabh√§ngig, da\n\\[\\begin{align*}\n  x_1 = \\begin{pmatrix}  1\\\\1\\\\0\\\\0\\end{pmatrix} =  -1 \\cdot\\begin{pmatrix}   0\\\\0\\\\1\\\\1\\end{pmatrix}   + \\begin{pmatrix}  1\\\\1\\\\1\\\\1\\end{pmatrix}.\n\\end{align*}\\] Der erste Spaltenvektor l√§sst sich also als Linearkombination der anderen beiden Spaltenvektoren darstellen.\n\n\nIn den meisten F√§llen sind die Variablen in einem Datensatz allerdings nicht multikollinear wie in Gleichung¬†2.6, sondern nur approximativ kollinear, d.h.\n\\[\nX_i \\approx \\sum_{n=i}^{j} \\alpha_n X_{n+1}, \\quad \\alpha_1,...,\\alpha_n\\in\\mathbb{R}\\setminus\\{0\\}.\n\\]\nUm zu √ºberpr√ºfen, ob die unabh√§ngigen Variablen eines Datensatzes linear abh√§ngig sind, wird nicht wie im obigen Beipsiel versucht eine passende Linearkombination zu finden, sondern es wird der Spaltenrang der Feature-Matrix bestimmt.\nDer Spaltenrang einer Matrix \\(X\\) ist definiert als die maximale Anzahl an linear unabh√§ngigen Spaltenvektoren in \\(X\\).\n\nBeispiel 2.3 ¬†\n\nDie Matrix \\(X_1\\) aus Example¬†2.2 hat Spaltenrang \\(3\\), da die drei Spalten linear unabh√§ngig sind.\nDie Matrix \\(X_2\\) aus Example¬†2.2 hat Spaltenrang \\(2\\), da nur \\(2\\) der drei Spalten linear unabh√§ngig sind.\n\n\n\n\n\n\n\n\nWichtig\n\n\n\nEine Quadratische Matrix \\(X\\in\\mathbb{R}^{n\\times n}\\) ist invertierbar, falls sie vollen Rang hat, i.e., alle Spalten sind linear unabh√§ngig. Falls \\(X\\) vollen Rang besitzt, dann existiert eine Matrix \\(X^{-1}\\in\\mathbb{R}^{n\\times n}\\) (die Inverse (Matrix) von \\(X\\)) sodass \\(X\\cdot X^{-1} = I_n\\) mit\n\\[\\begin{equation}\n  I_n = \\begin{pmatrix}\n    1 & 0 & ... & 0\\\\\n    0 & 1 & ... & 0\\\\\n    \\vdots  & \\vdots & \\ddots & \\vdots\\\\\n    0 & 0 & ... & 1\n  \\end{pmatrix}\n\\end{equation}\\]\ngilt.\n\n\n\n\n2.3.3.6 Das Problem mit Multikollinearit√§t\nWarum ist Multikollinearit√§t nun ein Problem? Kurz zusammengefasst:\nMultikollinearit√§t f√ºhrt dazu, dass die Regressionsparameter nicht mehr korrekt interpretiert werden k√∂nnen und im schlimmsten Fall, dass das MLR Modell als ganzes nicht mehr interpretierbar ist.\nDa in Gleichung¬†2.4 die Matrix \\((X^\\prime X)\\) invertiert wird um die optimalen Parameter \\(b_0,...,b_J\\) sch√§tzen zu k√∂nnen, muss diese die voraussetzung der Invertierbarkeit erf√ºllen. Wie im vorherigen Abschnitt beschrieben ist die Invertierbarkeit einer quadratischen Matrix allerdings nur dann m√∂glich, falls diese vollen Spaltenrang hat, also die Spalten alle unabh√§ngig von einander sind. Falls die Spalten abh√§ngig voneinander und somit (multi)kollinear sind, k√∂nnen die Parameter nicht mehr gesch√§tzt werden. Auch falls die Spalten approximativ linear abh√§nngig sind, f√ºhrt das ebesno zu Problem bei der Berechnung.\nDas folgende Beispiel illustriert diese Effekte.\n\nBeispiel 2.4 Der Datensatz data_kantine bildet die Anzahl der Besucher in einem Klub (num_vis) an 20 Abenden, den durchschnittlichen Preis f√ºr ein Getr√§nk (avg_drink_price), die Wochentage sowie die durchschnittlichen Ausgaben pro Gast an den entsprechenden Abenden ab.\n\n\n\n\n\n\n\n\nnum_vis\navg_drink_price\navg_expense\nweek_day\n\n\n\n\n289\n4.84\n22.7\nMittwoch\n\n\n295\n5.00\n23.8\nSamstag\n\n\n331\n4.43\n20.5\nSonntag\n\n\n301\n4.81\n25.4\nSamstag\n\n\n303\n4.81\n24.4\nSamstag\n\n\n334\n4.24\n19.8\nSonntag\n\n\n309\n5.12\n24.1\nSonntag\n\n\n275\n5.29\n24.7\nMittwoch\n\n\n286\n4.86\n24.2\nMittwoch\n\n\n291\n5.40\n25.5\nMittwoch\n\n\n324\n4.87\n23.7\nSonntag\n\n\n307\n4.86\n23.4\nSamstag\n\n\n308\n5.14\n24.5\nSamstag\n\n\n302\n5.20\n26.2\nSamstag\n\n\n289\n5.32\n25.1\nMittwoch\n\n\n336\n4.81\n24.8\nSonntag\n\n\n310\n5.04\n22.6\nSonntag\n\n\n261\n5.37\n26.1\nMittwoch\n\n\n314\n4.78\n23.2\nSonntag\n\n\n291\n4.99\n24.2\nMittwoch\n\n\n\n\n\n\n\nDie metrischen Spalten des Datensatzes sind linear unabh√§ngig. Die Korrelationsmatrix auf basis des Pearson Korrelationskoeffizient ergibt allerdings\n\n\n# A tibble: 3 √ó 4\n  term            num_vis avg_drink_price avg_expense\n  &lt;chr&gt;             &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n1 num_vis          NA              -0.717      -0.596\n2 avg_drink_price  -0.717          NA           0.817\n3 avg_expense      -0.596           0.817      NA    \n\n\nD.h., die beiden Spalten avg_expense und avg_drink_price korrelieren stark (\\(\\rho &gt; 0.9\\))!\nVerwendet man nun die numerischen Variablen, um die Anzahl der Clubbesucher anhand eines MLR Modells zu modellieren, so erh√§lt man\n\n\n...\n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     535.4580    55.1249   9.714 2.37e-08 ***\navg_drink_price -45.1042    19.1417  -2.356   0.0307 *  \navg_expense      -0.3753     3.4631  -0.108   0.9150    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n...\n\n\nDie beiden Variablen avg_drink_price und avg_expense sind im obigen Modell beide nicht hochsignifikant (\\(p&gt;0.01\\)).\nFalls aber im Gegensatz dazu jeweils entweder nur die Variable avg_drink_price oder avg_expense verwendet um die Besucherzahl zu modellieren, ergeben sich folgende Statistiken:\n\nModell 1:\n\n\n...\n\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       534.87      53.33  10.029 8.54e-09 ***\navg_drink_price   -46.80      10.74  -4.359 0.000378 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n...\n\n\nModell 2:\n\n\n...\n\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  471.381     53.673   8.782 6.33e-08 ***\navg_expense   -7.040      2.237  -3.148  0.00556 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n...\n\n\n\nDa die beiden Variablen stark korrelieren, sind ihre Effekte im Modell, welches beide Variablen enth√§lt, weder signifikant noch interpretierbar. Allerdings sind die Parameter getrennt betrachtet in Modell 1 und Modell 2 (hoch)signifikant, was ebenso daf√ºr spricht, dass die Feature stark korellieren.\nNoch extremer ist der Effekt, wenn man die Variable weekday in eine Dummy Variable umwandelt, welche jede Auspr√§gungsm√∂glichkeit abbildet:\n\ndata_kantine_dummy &lt;- data_kantine %&gt;%\n  mutate( week_day_wednesday = if_else(week_day == \"Mittwoch\", 1,0),\n          week_day_saturday = if_else(week_day == \"Samstag\", 1,0),\n          week_day_sunday = if_else(week_day == \"Sonntag\", 1,0),\n          week_day = NULL\n          )\n\nIm obigen Datensatz wurde die Variable week_day durch Dummy Variablen ersetzt, welche jeden Wochentag im Datensatz abbilden. Wenn dann die Parameter f√ºr das MLR Modell gesch√§tzt werden, welches den Einfluss der Wochentage auf die Besucherzahl modelliert, dann ergibt sich der Asuzug aus folgender Zusammenfassung:\n\n\n...\n\nCoefficients: (1 not defined because of singularities)\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         322.571      3.742  86.213  &lt; 2e-16 ***\nweek_day_wednesday  -39.429      5.291  -7.452 9.47e-07 ***\nweek_day_saturday   -19.905      5.507  -3.614  0.00214 ** \nweek_day_sunday          NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n...\n\n\nDa die Dummy Variablen perfekt mit dem Intercept korrelieren, kann der Koeffizient f√ºr week_day_sunday nicht berechnet werden.\n\n\n2.3.3.6.1 Variance Inflation Factor\nNeben einer Korrelationsmatrix kann auch der Variance Inflation Factor definiert als\n\\[\n\\text{VIF}_j = \\frac{1}{1-R^2_j}\n\\tag{2.7}\\]\nverwendet werden um zu pr√ºfen, ob Multikolinearit√§t in einem Modell vorhanden ist. Hierbei steht \\(R_j^2\\) f√ºr das Bestimmtheitsma√ü der Regression des Regressors \\(X_j\\) auf alle anderen Regressoren. Als Daumenregel wird h√§ufig genannt, dass falls \\(\\text{VIF}_j\\geq 5\\), der Regressor \\(X_j\\) aus dem Modell entfernt werden sollte.\nUm den VIF f√ºr alle unabh√§ngigen Variablen zu berechnen, kann man die vif()-Funktion der {car} Library verwenden. √Ñhnlich wie beim Erzeugen der Modellzusammenfassung muss man zuerst die extract_fit_engine()-Funktion anwenden, um das Modell an die vif()-Funktion √ºbergeben zu k√∂nnen.\n\nmlr_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  car::vif()\n\n                       GVIF Df GVIF^(1/(2*Df))\nspecies           71.200285  2        2.904828\nisland             3.762055  2        1.392696\nbill_length_mm     6.117069  1        2.473271\nbill_depth_mm      6.274341  1        2.504864\nflipper_length_mm  7.775036  1        2.788375\nsex                2.343434  1        1.530828\nyear               1.172956  1        1.083031\n\n\nIm obigen Output wird neben dem Generalized Variance Inflation Factor (GVIF) auch eine standardisierte Version berechnet. Die generalisierte Version des VIF wird f√ºr alle metrischen Variablen durch die Formel in Gleichung¬†2.7 berechnet. Lediglich die Berechnung des VIF f√ºr die nominalen Feature unterscheidet sich. Die Berechnung f√ºr die nominalen Feature w√ºrde allerdings den Rahmen der Vorlesung sprengen, weshalb diese hier nicht weiter erl√§utert wird.\nWichtig ist allerdings die Interpretation der Werte (unabh√§ngig vom Datentyp): Die standardisierte Version des VIF sollte f√ºr nominale Features betrachtet werden, da dieser die Anzahl der Dummyspalten einbezieht. Falls \\(\\text{GVIF}^{(1/(2*\\text{Df}))}\\geq \\sqrt5 \\approx 2.236\\), dann liegt eventuell bei einem nominalen Feature Multikolinearit√§t vor.\nF√ºr das obige MLR Modell bedeutet das, dass f√ºr die Variablen species, bill_length_mm, bill_depth_mm und flipper_length_mm Multikollinearit√§t vorliegen k√∂nnte. Um den Effekt im obigen Modell zu mitigieren, w√§re eine M√∂glichkeit, neue Modelle zu definieren, welche die einzelnen Variablen ausschlie√üen und f√ºr diese neuen Modelle die VIF zu berechnen.\n\n\n\n2.3.3.7 Weitere wichtige Pr√ºfkriterien\nNeben den bisher erl√§uterten Pr√ºfkriterien sollten wir uns im Zuge unserer Modellanalyse auch mit der Datenwualit√§t auseinandersetzen. Hierbei sind vor allem folgenden Konzpete nicht zu untersch√§tzen:\n\nAusrei√üeranalyse\nEinflussreiche Observationen\n\n\n2.3.3.7.1 Ausrei√üeranalyse\nOb eine Observation ein Ausrei√üer ist, h√§ngt stark von der Definition eines Ausrei√üers ab. In der Literatur werden verschiedene Definitionen verwendet, wobei eine g√§ngige die Definition von John Tukey stammt:\n\n\n\n\n\n\nDefinition\n\n\n\nSei \\(q_{(0.25)}\\) das \\(25\\%\\) Quantil und \\(q_{(0.75)}\\) das \\(75\\%\\) Quantil einer Variable, dann ist eine gegeben Observation ein Ausrei√üer, falls sie au√üerhalb des Intervalls\n\\[\n[q_{(0.25)}-k(q_{(0.75)}-q_{(0.25)}), q_{(0.75)}+k(q_{(0.75)}-q_{(0.25)})], \\qquad k\\geq 0\n\\]\nliegt.\n\\(k\\) ist hierbei frei w√§hlbar, allerdings wird h√§ufig \\(k=1.5\\) verwendet.\n\n\nEine weitere M√∂glichkeit der Ausrei√üerbestimmung verwendet die Cook‚Äôs Distance, die wir im fogenden Abschnitt betrachten.\n\n\n2.3.3.7.2 Einflussreiche Observationen\nObservationen, welche Ergebnisse der Parametersch√§tzung stark beeinflussen, werden einflussreiche Observationen genannt. Um zu bestimmen, ob eine Observation einflussreich ist, verwenden wir die Cook‚Äôs Distance, definiert als\n\\[\nD_i = \\frac{(\\hat{b}-\\hat{b}_{(i)})^\\prime (X^\\prime X)(\\hat{b}-\\hat{b}_{(i)})}{(J+1)\\hat\\sigma^2},\n\\]\nwobei \\(\\hat{b_{(i)}}\\) die gesch√§tzten Parameter ohne die \\(i\\)-te Beobachtung sind und \\(\\hat{\\sigma}\\) der \\(\\text{RMSE}\\) des Modells (siehe Section 2.3.4.3). Falls \\(D_i &gt; F_{J;K-J-1;0.5}\\), dann ist die Observation \\(i\\) ein Ausrei√üer.\nUm den Cook‚Äôs Distance in R zu berechnen, verwenden wir die stats::cooks.distance()-Funktion.\n\ncooks_d &lt;- mlr_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  cooks.distance()\n\ntibble(\n  cooks_d = cooks_d,\n  obs = 1:length(cooks_d)\n) %&gt;%\n  ggplot(aes(x=obs,y=cooks_d))+\n  geom_point(size = 1.5, color = \"darkorange\")+\n  labs(\n    x = \"Observation\",\n    y = \"Cook's Distance\"\n  )+\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.4 G√ºtema√üe\nBei der multiplen linearen Regression k√∂nnen wir analog zur einfachen linearen Regression anhand verschiedener G√ºtema√üe quantifizieren, wie gut das Modell den Zusammenhang zwischen den unabh√§ngigen und der abh√§ngigen Variable beschreibt.\n\n2.3.4.1 Korrigierter \\(R^2\\)\nNeben dem G√ºtema√ü \\(R^2\\) (vgl. Gleichung¬†2.2) wird bei MLR-Modellen auch h√§ufig die korrigierte Version\n\\[\nR^2_{\\text{korr}}  = R^2- \\frac{J\\cdot(1-R^2)}{K-J-1}\n\\tag{2.8}\\]\nverwendet. Hierbei ist allerdings zu beachten, dass \\(R^2_{\\text{korr}}\\) nicht mehr direkt interpretiert werden kann und das G√ºtema√ü auch keine untere Grenze (kann auch negative Werte annehmen) besitzt.\n\n\n2.3.4.2 Akaike Information Criteria\nDas Akaike-Informationskriterium (AIC) wird h√§ufig verwendet, um Modelle untereinander zu vergleichen, da der Bildbereich dieses G√ºtema√ües unbeschr√§nkt ist (kann jede Zahl \\(x\\in\\mathbb{R}\\) annehmen).\n\\[\n\\text{AIC} = \\log\\left(\\frac{\\sum_{k=1}^K(y_k-\\hat{y_k})^2}{K} \\right)+\\frac{2J}{K}\n\\tag{2.9}\\]\nIn R k√∂nnen wir den AIC mithilfe der stats::AIC-Funktion berechnen:\n\nmlr_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  stats::AIC()\n\n[1] 4724.97\n\n\n\n\n2.3.4.3 Loss-Funktionen\nNeben den bisher eingef√ºhrten G√ºtema√üen werden bei Regressionsmodellen auch oft sogenannte Loss-Funktionen berechnet. Diese messen, wie weit die tats√§chlichen Werte der abh√§ngigen Variable von der Sch√§tzung entfernt sind. Eine g√§ngige Loss-Funktion im Regressionskontext ist der Root Mean Squared Error (RMSE). Dieser ist definiert als\n\\[\n\\text{RMSE}(y,\\hat{y}) = \\sqrt{\\frac{1}{K}\\sum_{k=1}^{K} (y_k-\\hat{y}_k)^2}.\n\\tag{2.10}\\]\nDer RMSE wird h√§ufig verwendet, da dieser die euklidische Distanz zwischen zwei Punkten berechnet und deshalb sehr intuitiv interpretierbar ist.\nWir k√∂nnen den RMSE mithilfe der yardstick::mse()-Funktion berechnen. Hierf√ºr m√ºssen wir zuerst die Vorhersagen generieren und diese dann zusammen mit den tats√§chlichen Werten √ºbergeben:\n\nmlr_fit %&gt;%\n  parsnip::augment(data_penguin) %&gt;%\n  yardstick::rmse(truth = .pred, estimate = body_mass_g)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        282.\n\n\nIn der zweiten Zeile berechnen wir mit der parsnip::augment()-Funktion die durch das MLR-Modell gesch√§tzten Werte. Der R√ºckgabewert dieser Funktion ist ein neuer Datensatz, welcher neben dem Datensatz data_penguin noch die Spalten .pred und .resid enth√§lt. In der Spalte .pred sind die durch das Modell gesch√§tzten Vorhersagen enthalten und in der Spalte .resid die Residuen. Diesen neuen Datensatz √ºbergeben wir dann der yardstick::rmse-Funktion mit den Argumenten truth = .pred und estimate = body_mass_g.\nDen Wert .estimate = 282 k√∂nnen wir wie folgt interpretieren:\nIm Schnitt erwarten wir bei der Sch√§tzung des Gewichts eines Pinguins eine Abweichung von \\(282\\) g vom wahren Wert.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "02_Lineare_Regression.html#√ºbungsaufgaben",
    "href": "02_Lineare_Regression.html#√ºbungsaufgaben",
    "title": "2¬† Lineare Regression",
    "section": "2.4 √úbungsaufgaben",
    "text": "2.4 √úbungsaufgaben\n\n2.4.1 Explorative Datenanalyse\n\nAufgabe 2.1 Finde heraus, wie gro√ü die relativen Anteile der jeweiligen Pinguinspezies an der Gesamtanzahl der Pinguine im Datensatz sind.\n\n\nAufgabe 2.2 In Section 2.1.1 haben wir einen Boxplot f√ºr die Schnabell√§nge der verschiedenen Pinguinspezies erstellt. Erstelle jetzt einen Dichteplot, in welchem die empirische Dichte der Schnabell√§ngen der jeweiligen Pinguinspezies dargestellt wird.\n\n\n\n2.4.2 Einfache lineare Regression\n\nAufgabe 2.3 Entferne alle NA-Werte und die Variable year im Datensatz. Erstelle anschlie√üend einen neuen Teildatensatz data_penguin_adelie, welcher nur Pinguine der Adelie Spezies enth√§lt.\n\n\nAufgabe 2.4 ¬†\n\nSch√§tze f√ºr den neu erstellten Datensatz aus Aufgabe¬†2.3 ein einfaches lineares Modell in R, wobei die abh√§ngige Variable durch flipper_length_mm und die unabh√§ngige Variable durch body_mass_g gegeben ist.\nErzeuge f√ºr das in Aufgabe 1. erstellte lineare Modell eine Zusammenfassung in R. Ist die unabh√§ngige Variable zum Signifikanzniveau \\(\\alpha = 0.01\\) statistisch signifikant?\nAngenommen, der Wert der \\(t\\)-Statistik f√ºr die Variable body_mass_g betr√§gt nicht \\(6.30\\), sondern \\(2.88\\). Bestimme das kleinste Signifikanzniveau, zu welchem die Variable signifikant ist. Verwende hierf√ºr folgende Tabelle, welche Quantile der Standardnormalverteilung enth√§lt:\n\n\n\n\n\n\n\n\n\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n\n\n\n\n2.0\n0.9772\n0.9778\n0.9783\n0.9788\n0.9793\n0.9798\n0.9803\n0.9808\n0.9812\n0.9817\n\n\n2.1\n0.9821\n0.9826\n0.9830\n0.9834\n0.9838\n0.9842\n0.9846\n0.9850\n0.9854\n0.9857\n\n\n2.2\n0.9861\n0.9864\n0.9868\n0.9871\n0.9875\n0.9878\n0.9881\n0.9884\n0.9887\n0.9890\n\n\n2.3\n0.9893\n0.9896\n0.9898\n0.9901\n0.9904\n0.9906\n0.9909\n0.9911\n0.9913\n0.9916\n\n\n2.4\n0.9918\n0.9920\n0.9922\n0.9925\n0.9927\n0.9929\n0.9931\n0.9932\n0.9934\n0.9936\n\n\n2.5\n0.9938\n0.9940\n0.9941\n0.9943\n0.9945\n0.9946\n0.9948\n0.9949\n0.9951\n0.9952\n\n\n2.6\n0.9953\n0.9955\n0.9956\n0.9957\n0.9959\n0.9960\n0.9961\n0.9962\n0.9963\n0.9964\n\n\n2.7\n0.9965\n0.9966\n0.9967\n0.9968\n0.9969\n0.9970\n0.9971\n0.9972\n0.9973\n0.9974\n\n\n2.8\n0.9974\n0.9975\n0.9976\n0.9977\n0.9977\n0.9978\n0.9979\n0.9979\n0.9980\n0.9981\n\n\n2.9\n0.9981\n0.9982\n0.9982\n0.9983\n0.9984\n0.9984\n0.9985\n0.9985\n0.9986\n0.9986\n\n\n\n\n\n\n\nAngenommen, ein Pinguin wiegt \\(16000\\) g, wie gro√ü ist die erwartete Flossenl√§nge nach dem in Teilaufgabe 1. gesch√§tzten einfachen linearen Modell?\n\n\n\n\n2.4.3 Kennzahlen\n\nAufgabe 2.5 In Section 2.3.4.2 wurde das AIC eingef√ºhrt. Beurteile anhand Gleichung¬†2.9 , ob ein hoher oder niedriger AIC-Wert f√ºr ein Modell spricht, welches den Zusammenhang zwischen den Variablen gut erkl√§rt.\n\n\nAufgabe 2.6 Beurteile den Wahrheitsgehalt der folgenden Aussage und begr√ºnde deine Entscheidung:\n\n,,Jeder Ausrei√üer ist eine Einflussreiche Observation.``\n\n\n\nAufgabe 2.7 In Section 2.3.3.7.2 haben wir Cook‚Äôs Distance als Methode zur Erkennung von einflussreichen Observationen eingef√ºhrt.\nBerechne f√ºr das Beispiel in Section 2.3.3.7.2 den Wert \\(F_{J;K-J-1;0.5}\\) und entscheide, ob im Datensatz, welcher f√ºr die Berechnungen verwendet wurde, einflussreiche Variablen sind.\nHinweis: Um ein Quantil der \\(F\\)-Verteilung zu berechnen, k√∂nnen wir die qf()-Funktion verwenden.\n\n\n\n2.4.4 Multiple Lineare Regression\nF√ºr die folgenden Aufgaben verwenden wir den hotel_rates-Datensatz der {modeldata}-Library1. Die in {modeldata} enthaltene Version des Datensatzes enth√§lt die durchschnittlichen Zimmerpreise f√ºr Hotels in Lissabon, Portugal, aus den Jahren 2016‚Äì2017. Das Ziel dieser Aufgabe ist es, die Variable avg_price_per_room zu modellieren, die den durchschnittlichen Preis pro Nacht in einem Hotel beschreibt.\n\nAufgabe 2.8 Beschreibe, wie im folgenden Code-Snippet die Daten aufbereitet werden. Gehe dabei auf die einzelnen Funktionen und deren R√ºckgabewert ein.\n\nhotel_rates_filtered &lt;- hotel_rates %&gt;%\n mutate(\n   arrival_month = factor(month(arrival_date)),\n   num_guests = adults+children,\n   is_repeated_guest = factor(is_repeated_guest),\n   ) %&gt;%\n   select(num_guests,total_of_special_requests,\n          required_car_parking_spaces,arrival_month,\n          is_repeated_guest,lead_time,stays_in_week_nights,\n          avg_price_per_room)\n\n\n\nAufgabe 2.9 Erstelle mit R ein MLR-Modell, welches als abh√§ngige Variable avg_price_per_room enth√§lt und als unabh√§ngige Variablen alle anderen Variablen im Datensatz hotel_rates_filtered.\n\n\nAufgabe 2.10 ¬†\n\nErzeugte f√ºr das Modell aus Aufgabe¬†2.9 eine Modellzusammenfassung. Gibt es Variablen im Modell, welche nicht statistisch signifikant sind?\nErweitere die Modellzusammenfassung durch eine Version, welche die standardisierten Koeffizienten enth√§lt. Welche Variable hat den gr√∂√üten Einfluss auf die abh√§ngige Variable avg_price_per_room?\nF√ºhre mithilfe der Modellzusammenfassung einen \\(F\\)-Test durch und evaluiere auf Basis dieses Tests die Modellg√ºte zum Signifikanzniveau \\(\\alpha = 0.05\\).\n\n\n\nAufgabe 2.11 √úberpr√ºfe das Modell aus Aufgabe¬†2.9 im Hinblick auf\n\nHomoskedastizit√§t.\nAutokorrelation.\nDie Annahme der Normalverteilung der Residuen.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "02_Lineare_Regression.html#l√∂sungen",
    "href": "02_Lineare_Regression.html#l√∂sungen",
    "title": "2¬† Lineare Regression",
    "section": "2.5 L√∂sungen",
    "text": "2.5 L√∂sungen\n\nSolution 2.1 (Aufgabe¬†2.1). Um herauszufinden, wie gro√ü der relative Anteil der jeweiligen Pinguinspezies ist, k√∂nnen wir die group_by- und summarise-Funktionen verwenden. Durch das Spezifizieren von rel = n()/nrow(data_penguin) erstellen wir in der Zusammenfassung eine neue Spalte rel welche f√ºr jedes Gruppenmitglied (jede Spezies) zuerst die Anzahl berechnet (durch die n()-Funktion) und diese anschlie√üend durch die Gesamtanzahl der Datenpunkte im Datensatz teilt (die Gesamtanzahl berechnen wir durch nrow(data_penguin)).\n\ndata_penguin %&gt;%\n  group_by(species) %&gt;%\n  summarise(rel = n()/nrow(data_penguin))\n\n# A tibble: 3 √ó 2\n  species     rel\n  &lt;fct&gt;     &lt;dbl&gt;\n1 Adelie    0.442\n2 Chinstrap 0.198\n3 Gentoo    0.360\n\n\n\n\nSolution 2.2 (Aufgabe¬†2.2). \n\ndata_penguin%&gt;%\n  ggplot(aes(x=bill_length_mm, fill =species))+\n  geom_density(alpha = 0.5)+\n  scale_fill_manual(values =c(\"darkorange\",\"purple\",\"cyan4\"))+\n  labs(x =\"Schnabell√§nge in mm\", y=\"Dichte\")+\n  theme_minimal(base_size=14)+\n  theme(legend.position =\"none\")\n\n\n\n\n\n\n\n\nIm Vergleich zur Boxplot-Grafik aus Section 2.1.1 m√ºssen wir bei einem Dichteplot folgende √Ñnderungen vornehmen:\n\nIn Zeile 2 √ºbergeben wir in der aes()-Funktion auf der \\(x\\)-Achse nicht die verschiedenen Spezies, sondern direkt die Schnabell√§nge. Die \\(y\\)-Achse wird automatisch auf die Werte der Dichten gesetzt. Durch das Setzen von fill=species erreichen wir wieder, dass f√ºr jede Spezies sp√§ter ein einzelner Dichteplot erzeugt wird.\nIn Zeile 3 f√ºgen wir die Dichteplots mit der geom_density-Funktion hinzu (analog zu Solution¬†1.11). Durch das Setzen von alpha = 0.5 erreichen wir, dass die F√ºllungen der Dichteplots sich nicht gegenseitig √ºberdecken und die einzelnen Dichten so noch sichtbar sind.\nZum Schluss m√ºssen wir in Zeile 5 noch die Namen der Achsen anpassen.\n\n\n\nSolution 2.3 (Aufgabe¬†2.3). \n\ndata_penguin &lt;- palmerpenguins::penguins\n\ndata_penguin &lt;- data_penguin %&gt;%\n  na.omit() %&gt;%\n  select(-year)\n\ndata_penguin_adelie &lt;- data_penguin %&gt;%\n  filter(species==\"Adelie\")\n\nZuerst erstellen wir einen Datensatz data_penguin, indem wir den penguins-Datensatz der Library palmerpenguins in der ersten Zeile neu einlesen. Danach √ºberschreiben wir den Datensatz mit einer modifizierten Version, wobei wir im ersten Schritt (Zeile 3) die na.omit()-Funktion auf den Datensatz anwenden, um alle NA-Werte zu entfernen, und im zweiten Schritt (Zeile 4) mithilfe der select()-Funktion die Spalte years entfernen. Spalten k√∂nnen wir mithilfe der select-Funktion entfernen, indem wir den --Operator vor den Spaltennamen setzen.\nIn Zeile 7 generieren wir dann einen neuen Datensatz data_penguin_adelie, indem wir auf den Datensatz data_penguin die filter-Funktion anwenden. Durch das Argument species==\"Adelie\" spezifizieren wir in der filter-Funktion, dass nur Eintr√§ge im neuen Datensatz gespeichert werden, welche in der Spalte species die Auspr√§gung \"Adelie\" besitzen.\n\n\nSolution 2.4 (Aufgabe¬†2.4). \n\n\n\nlm_adelie &lt;- linear_reg() %&gt;%\n  fit(data = data_penguin_adelie,\n      formula = flipper_length_mm ~  body_mass_g )\n\nWie in Section 2.2.1 k√∂nnen wir mithilfe der linear_reg-Funktion ein lineares Modell spezifizieren und die Parameter direkt mit der fit-Funktion sch√§tzen. Hierbei verwenden wir die Formel flipper_length_mm ~  body_mass_g, welche beschreibt, dass flipper_length_mm die abh√§ngige Variable ist und body_mass_g die unabh√§ngige Variable. Die Daten f√ºr die Sch√§tzung spezifizieren wir durch das data-Argument. Wie in der Aufgabenstellung beschrieben, verwenden wir den neuen Datensatz data_penguin_adelie, welcher lediglich Pinguine der Spezies Adelie enth√§lt.\n\n\nlm_adelie %&gt;%\n  extract_fit_engine() %&gt;%\n  summary()\n\n\nCall:\nstats::lm(formula = flipper_length_mm ~ body_mass_g, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.426  -3.669   0.239   3.422  17.955 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.656e+02  3.918e+00   42.27  &lt; 2e-16 ***\nbody_mass_g 6.610e-03  1.049e-03    6.30  3.4e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.794 on 144 degrees of freedom\nMultiple R-squared:  0.2161,  Adjusted R-squared:  0.2106 \nF-statistic: 39.69 on 1 and 144 DF,  p-value: 3.402e-09\n\n\nDurch das Verwenden der extract_fit_engine() und summary-Funktion k√∂nnen wir wie in Section 2.2.1 eine Zusammenfassung erzeugen. Da f√ºr die Variable body_mass_g gilt, dass Pr(&gt;|t|)=3.4*10^-9&lt;0.01 ist die Variable zum Signifikanzniveau \\(0.01\\) signifikant.\nEs gilt t value = 2.88. Um den kleinsten Wert \\(\\alpha\\) zu bestimmen, f√ºr den Pr(&gt;|t|)&lt;\\(\\alpha\\) gilt, m√ºssen wir das \\(F^{-1}(2.88)\\)-Quantil der Standardnormalverteilung bestimmen. Anhand der Quantilstabelle, die in der Aufgabenstellung gegeben ist, k√∂nnen wir bei \\(F^{-1}(2.88)\\) den Wert \\(0.9980\\) ablesen. Somit gilt \\(1-\\frac{\\alpha}{2} = 0.9980\\) genau dann, wenn \\(\\alpha = 0.004\\). Das Konfidenzniveau \\(\\alpha = 0.004\\) ist somit das kleinste, zu welchem die Variable body_mass_g signifikant ist unter der Annahme, dass t value = 2.88.\nFalls ein Pinguin \\(16000\\) g wiegt, k√∂nnen wir entweder die Modellgleichung oder R verwenden, um die gesch√§tzte Flossenl√§nge zu berechnen:\n\nAus der Modellzusammenfassung in Teilaufgabe 2. k√∂nnen wir die Sch√§tzgleichung\n\\[\n\\widehat{\\verb|flipper_length_mm|} = 165.6 + 0.006610\\cdot \\verb|body_mass_g|\n\\]\nablesen. Durch das Einsetzen von \\(16,000\\) in diese Sch√§tzgleichung erhalten wir eine erwartete Flossenl√§nge von \\[\n165.6+ 0.006610\\cdot 16,000 = 271.36.\n\\]\nMithilfe der predict() oder augment()-Funktion, k√∂nnen wir das erwartete Gewicht auch mit R sch√§tzen:\n\nlm_adelie %&gt;% \n  predict(tibble(body_mass_g = 16000))\n\n# A tibble: 1 √ó 1\n  .pred\n  &lt;dbl&gt;\n1  271.\n\nlm_adelie %&gt;% \n  augment(tibble(body_mass_g = 16000))\n\n# A tibble: 1 √ó 2\n  .pred body_mass_g\n  &lt;dbl&gt;       &lt;dbl&gt;\n1  271.       16000\n\n\nUm die Flossenl√§nge unter Verwendung der beiden Funktionen zu sch√§tzen, m√ºssen wir die Variable body_mass_g mit dem neuen Wert in einem Tibble oder Data Frame √ºbergeben. Wichtig ist hierbei auch den Variablenname body_mass_g zu √ºbergeben, da sonst nicht klar ist, zu welcher Variable der Wert 16,000 geh√∂rt.\n\n\n\n\nSolution 2.5 (Aufgabe¬†2.5). Nach Gleichung¬†2.9 gilt\n\\[\n\\text{AIC} = \\log\\left(\\frac{\\sum_{k=1}^K(y_k-\\hat{y_k})^2}{K} \\right)+\\frac{2J}{K}\n\\] Die Logarithmusfunktion ist monoton steigend und kann jeden Wert zwischen \\((-\\infty,\\infty)\\) annehmen. je gr√∂√üer also die Abweichung zwischen den Werten \\(y_k-\\hat{y_k}\\) im Summand sind, desto gr√∂√üer ist auch der Wert des Logarithmus. Da die Werte \\(K\\) und \\(J\\) konstant sind, k√∂nnen wir deshalb daraus schlie√üen, dass ein Modell mit niedrigem AIC-Wert die tats√§chlichen Werte \\(y_k\\) besser sch√§tzt.\n\n\nSolution 2.6 (Aufgabe¬†2.6). Nicht jeder Ausrei√üer ist eine Einflussreiche Observation. Folgendes Gegenbeispiel illustriert diesen Effekt:\n\n\n\n\n\n\n\n\n\nObwohl der Punkt \\((20,20)\\) ein Ausrei√üer ist, beeinflusst er kaum die blaue Regressionsgerade.\n\n\nSolution 2.7 (Aufgabe¬†2.7). Um zu √ºberpr√ºfen, ob in den Daten, welche wir f√ºr die Berechnung der Cook‚Äôs Distance in Section 2.3.3.7.2 verwendet haben, einflussreiche Observationen sind, m√ºssen wir das \\(F_{J;K-J-1;0.5}\\)-Quantil berechnen. Da \\(J = 7\\) und \\(K=344\\) ergibt sich\n\nqf(p = 0.5,df1 = 7, df2 = 344-7-1)\n\n[1] 0.9083624\n\n\nWir k√∂nnen diesen Wert nun mit den Werten der Cook‚Äôs Distance vergleichen:\n\nsum(cooks_d &gt;= qf(p = 0.5,df1 = 7, df2 = 344-7-1))\n\n[1] 0\n\n\nIm obigen Code-Snippet Vergleichen wir die Werte des Vektors cooks_d mit dem Quantil der \\(F\\)-Verteilung. Ohne die Anwendung der Summenfunktion ist der R√ºckgabewert eine Liste, welche nur die Werte TRUE und FALSE enth√§lt. Da keiner der Werte im Vektor cooks_d gr√∂√üer ist als das Quantil der \\(F\\)-Verteilung, ist der Vektor mit FALSE Eintr√§gen gef√ºllt. FALSE wird beim aufsummieren in R als \\(0\\) gewertet, weshalb ein Vektor gef√ºllt mit FALSE Werten summiert \\(0\\) ergibt. Wir k√∂nnen daraus schlie√üen, dass keine einflussreichen Observationen im penguins Datensatz enthalten sind.\n\n\nSolution 2.8 (Aufgabe¬†2.8). \n\nhotel_rates_filtered &lt;- hotel_rates %&gt;%\n mutate(\n   arrival_month = factor(month(arrival_date)),\n   num_guests = adults+children,\n   is_repeated_guest = factor(is_repeated_guest),\n   ) %&gt;%\n   select(num_guests,total_of_special_requests,\n          required_car_parking_spaces,arrival_month,\n          is_repeated_guest,lead_time,stays_in_week_nights,\n          avg_price_per_room)\n\n\nIn der ersten Zeile wird ein neuer Datensatz hotel_rates_filtered definiert.\nIn den Zeilen 2-6 wird auf den Datensatz hotel_rates die mutate()-Funktion angewendet, welche bestehende Spalten ver√§ndert oder neue Spalten erstellt.\n\nZuerst wird eine neue Spalte arrival_month erstellt, welche aus der Variable arrival_date mithilfe der month()-Funktion den Monat extrahiert, in welchem die G√§ste in der Unterkunft sind. Der Monat wird anschlie√üend noch mit der factor()-Funktion in einen ungeordneten Faktor umgewandelt.\nEs wird dann eine neue Variable num_guests erstellt, welche die Gesamtanzahl der G√§ste beschreibt. Die Gesamtanzahl wird hierbei durch die Summe der adults und children Spalte berechnet.\nZuletzt wird die Spalte is_repeated_guest in einen ungeordneten Faktor umgewandelt.\n\nIn Zeile 7 werden dann mit der select()-Funktion verschiedene Spalten des angepassten hotel_rates Datensatzes ausgew√§hlt, welche im neuen Datensatz enthalten sein sollen.\n\n\n\nSolution 2.9 (Aufgabe¬†2.9). Erstelle mit R ein MLR Modell, welches als abh√§ngige Variable avg_price_per_room enth√§lt und als unabh√§ngige Variablen alle anderen Variablen im Datensatz hotel_rates_filtered.\n\nmlr_hotel_fit &lt;- linear_reg() %&gt;%\n  fit(data = hotel_rates_filtered,\n      formula = avg_price_per_room ~ .)\n\n\n\nSolution 2.10 (Aufgabe¬†2.10). \n\n\n\nmlr_hotel_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  summary()\n\n\nCall:\nstats::lm(formula = avg_price_per_room ~ ., data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-191.846  -20.582   -2.665   17.254  271.908 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   5.9319     1.3215   4.489 7.21e-06 ***\nnum_guests                   26.9413     0.4475  60.203  &lt; 2e-16 ***\ntotal_of_special_requests     5.2444     0.3214  16.317  &lt; 2e-16 ***\nrequired_car_parking_spaces  12.8375     0.6812  18.845  &lt; 2e-16 ***\narrival_month2                0.7578     1.4398   0.526   0.5987    \narrival_month3               12.7604     1.4524   8.786  &lt; 2e-16 ***\narrival_month4               36.2235     1.4469  25.035  &lt; 2e-16 ***\narrival_month5               36.2536     1.4562  24.897  &lt; 2e-16 ***\narrival_month6               69.7082     1.5216  45.814  &lt; 2e-16 ***\narrival_month7              107.9166     1.3368  80.729  &lt; 2e-16 ***\narrival_month8              134.6784     1.3232 101.779  &lt; 2e-16 ***\narrival_month9               62.2610     1.5422  40.371  &lt; 2e-16 ***\narrival_month10              21.8807     1.4217  15.391  &lt; 2e-16 ***\narrival_month11              -2.5258     1.4863  -1.699   0.0893 .  \narrival_month12              19.7548     1.4966  13.200  &lt; 2e-16 ***\nis_repeated_guest1           -4.9443     1.1371  -4.348 1.38e-05 ***\nlead_time                    -0.1272     0.0033 -38.546  &lt; 2e-16 ***\nstays_in_week_nights         -0.2196     0.1269  -1.730   0.0837 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 33.89 on 15384 degrees of freedom\nMultiple R-squared:  0.7329,  Adjusted R-squared:  0.7326 \nF-statistic:  2483 on 17 and 15384 DF,  p-value: &lt; 2.2e-16\n\n\nDie Aufgabenstellung wurde bei dieser Teilaufgabe absichtlich etwas uneindeutig gestellt. Da die Frage lautet, ob eine Variable nicht statistisch signifikant ist, lautet die Antwort eigentlich Nein. Denn selbst die Variable arrival_month_2 ist zum Signifikanzniveau \\(\\alpha = 0.948\\) signifikant. Normalerweise spricht man aber bei der Frage nach Signifikanz immer von einem Niveau \\(\\alpha \\in (0,0.05)\\). Falls wir allerdings die g√§ngige Daumenregel f√ºr Signifikanz verwenden, dann sie die Variablen arrival_month2,arrival_monht_11 und stays_in_week_nights nicht statistisch signifikant.\n\n\nlibrary(lm.beta)\n\nmlr_hotel_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  lm.beta()%&gt;%\n  summary()\n\n\nCall:\nstats::lm(formula = avg_price_per_room ~ ., data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-191.846  -20.582   -2.665   17.254  271.908 \n\nCoefficients:\n                              Estimate Standardized Std. Error t value Pr(&gt;|t|)\n(Intercept)                   5.931878           NA   1.321469   4.489 7.21e-06\nnum_guests                   26.941284     0.271328   0.447507  60.203  &lt; 2e-16\ntotal_of_special_requests     5.244445     0.069716   0.321405  16.317  &lt; 2e-16\nrequired_car_parking_spaces  12.837463     0.080228   0.681214  18.845  &lt; 2e-16\narrival_month2                0.757801     0.003060   1.439819   0.526   0.5987\narrival_month3               12.760405     0.050979   1.452436   8.786  &lt; 2e-16\narrival_month4               36.223475     0.147026   1.446896  25.035  &lt; 2e-16\narrival_month5               36.253554     0.147491   1.456155  24.897  &lt; 2e-16\narrival_month6               69.708193     0.265733   1.521558  45.814  &lt; 2e-16\narrival_month7              107.916576     0.554979   1.336783  80.729  &lt; 2e-16\narrival_month8              134.678372     0.717227   1.323245 101.779  &lt; 2e-16\narrival_month9               62.261007     0.239576   1.542227  40.371  &lt; 2e-16\narrival_month10              21.880701     0.094708   1.421681  15.391  &lt; 2e-16\narrival_month11              -2.525839    -0.009607   1.486313  -1.699   0.0893\narrival_month12              19.754753     0.074348   1.496565  13.200  &lt; 2e-16\nis_repeated_guest1           -4.944333    -0.018703   1.137071  -4.348 1.38e-05\nlead_time                    -0.127207    -0.197191   0.003300 -38.546  &lt; 2e-16\nstays_in_week_nights         -0.219552    -0.008311   0.126933  -1.730   0.0837\n\n(Intercept)                 ***\nnum_guests                  ***\ntotal_of_special_requests   ***\nrequired_car_parking_spaces ***\narrival_month2                 \narrival_month3              ***\narrival_month4              ***\narrival_month5              ***\narrival_month6              ***\narrival_month7              ***\narrival_month8              ***\narrival_month9              ***\narrival_month10             ***\narrival_month11             .  \narrival_month12             ***\nis_repeated_guest1          ***\nlead_time                   ***\nstays_in_week_nights        .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 33.89 on 15384 degrees of freedom\nMultiple R-squared:  0.7329,  Adjusted R-squared:  0.7326 \nF-statistic:  2483 on 17 and 15384 DF,  p-value: &lt; 2.2e-16\n\n\nDer gesch√§tzte standardisierte Koeffizient der Variable arrival_month_8 ist am gr√∂√üten, weshalb dieser auch den gr√∂√üten Einfluss auf den durchschnittlichen Zimmerpreis im Modell besitzt. Da es sich hierbei um den Monat August handelt, k√∂nnen wir guten Gewissens mutma√üen, dass in diesem Monat die Zimmer aufgrund der Ferienzeit und der damit verbundenen h√∂heren Nachfrage immer ein wenig teurer sind.\nDie Hypothesen des \\(F\\)-Tests lauten \\[\nH_0: b_{\\verb|num_guests|} = ... = b_{\\verb|stays_in_week_nights|} = 0\n\\]\nvs.\n\\[\nH_1: \\text{min. ein Koeffizient ist ungleich } 0\n\\]\nDa \\(p&lt;2.2\\cdot 10^{ -16}&lt;\\alpha = 0.05\\), k√∂nnen wir \\(H_0\\) verwerfen und daraus schlie√üen, dass das Modell als Ganzes signifikant ist.\n\n\n\nSolution 2.11 (Aufgabe¬†2.11). \n\nHomoskedastizit√§t:\n\nresid&lt;-mlr_hotel_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  rstudent()\n\npreds &lt;- mlr_hotel_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  predict(hotel_rates_filtered) \n\ntibble(\n  predictions = preds,\n  residuals = sqrt(abs(resid))\n) %&gt;%\n  ggplot(aes(x=predictions,y=residuals))+\n  geom_point(alpha = 0.2)+\n  geom_smooth(method = \"lm\", se = F, formula = y~x)+\n  labs(\n    y = TeX(\"$\\\\sqrt{|Std. Residuen|}\"),\n    title = \"Quadratwurzel des Betrags \\nder standardisierten Residuen\"\n  )+\n  theme_minimal(base_size=14)\n\n\n\n\n\n\n\n\nIn der erzeugten Grafik ist klar zu erkennen, dass die blaue Trendlinie nicht horizontal ist, sondern eine leichte positive Steigung besitzt. Wir k√∂nnen deshalb nicht ausschlie√üen, dass die Residuen eine konstante Varianz besitzen. Aufgrund der positiven Steigung k√∂nnen wir vermuten, dass die Varianz in den Residuen mit zunehmendem Sch√§tzwert ebenso steigt.\nAutokorrelation:\n\nresid&lt;-mlr_hotel_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  rstudent()\n\npreds &lt;- mlr_hotel_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  predict(hotel_rates_filtered) \n\ntibble(\n  predictions = preds,\n  residuals = sqrt(abs(resid))\n) %&gt;%\n  ggplot(aes(x=predictions,y=residuals))+\n  geom_point(alpha = 0.2)+\n  geom_smooth(method = \"loess\", se = F, formula = y~x)+\n  labs(\n    y = TeX(\"$\\\\sqrt{|Std. Residuen|}\"),\n    title = \"Quadratwurzel des Betrags \\nder standardisierten Residuen\"\n  )+\n  theme_minimal(base_size=14)\n\n\n\n\n\n\n\n\nDurch das Ab√§ndern der method in der geom_smooth-Funktion k√∂nnen wir eine nichtlineare Trendlinie einzeichnen. Da die neue Trendlinie ebenso nicht horizontal ist und zus√§tzlich auch noch leicht nichtlinear, k√∂nnen wir nicht ausschlie√üen, dass die Residuen unabh√§ngig voneinander sind. Da beide dieser Effekte allerdings marginal sind, sollte das f√ºr die Modellg√ºte nicht weiter problematisch sein.\nAnnahme der Normalverteilung der Residuen: Mithilfe des folgenden Skripts k√∂nnen wir sowohl den QQ-Plot als auch ein Histogramm untereinander darstellen. Hierbei verwenden wir die {patchwork}-Library, welche es erlaubt, Grafiken einfach miteinander zu verkn√ºpfen. Indem wir zuerst eine Grafik p1 erstellen, welche den QQ-Plot enth√§lt, und danach ein Histogramm unter p2 speichern, k√∂nnen wir die beiden Grafiken mit dem p1/p2 Befehl vertikal angeordnet darstellen.\n\nlibrary(patchwork)\nresid &lt;- mlr_hotel_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  residuals() %&gt;%\n  tibble(residuals = .)\n\np1 &lt;- resid %&gt;% ggplot(aes(sample=residuals))+\n  geom_qq()+\n  geom_qq_line()+\n  theme_minimal(base_size=14)\n\nmean_res &lt;- mlr_hotel_fit %&gt;%\n  extract_fit_engine()%&gt;%\n  residuals() %&gt;%\n  mean()\n\nsd_res &lt;- mlr_hotel_fit %&gt;%\n  extract_fit_engine()%&gt;%\n  residuals() %&gt;%\n  sd()\n\np2 &lt;- resid %&gt;% \n  ggplot(aes(x=residuals))+\n  geom_histogram(aes(y= after_stat(density)), bins = 50)+\n  stat_function(fun = dnorm,\n                color = \"red\",\n                linewidth = 1.5,\n                alpha = 0.5,\n                args = list(mean = mean_res,\n                            sd = sd_res))+\n  theme_minimal(base_size = 14)\n\np1/p2\n\n\n\n\n\n\n\n\nBeim Betrachten des QQ-Plots f√§llt auf, dass vor allem die oberen Quantile stark von denen einer Normalverteilung abweichen. Dieses Verhalten k√∂nnen wir ebenso im Histogramm erkennen, da im Bereich zwischen \\(250\\) und \\(300\\) ein Balken zu erkennen ist. Bei einer Normalverteilung sollte so ein Balken nicht auftreten! Ansonsten sind im Histogramm die Residuen relativ nah um die theoretische Dichte der Normalverteilung verteilt. Lediglich am Modus kann erkannt werden, dass mehr Residuen um den Ursprung verteilt sind.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "02_Lineare_Regression.html#footnotes",
    "href": "02_Lineare_Regression.html#footnotes",
    "title": "2¬† Lineare Regression",
    "section": "",
    "text": "Ggf. muss mithilfe des install.packages(\"modeldata\")-Befehls die Library erst noch installiert werden.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "03_Wichtige_Konzepte.html",
    "href": "03_Wichtige_Konzepte.html",
    "title": "3¬† Weiterf√ºhrende Konzepte des Data Mining",
    "section": "",
    "text": "3.1 Training und Test Split\nDie Parameter der linearen Regressionsmodelle aus Kapitel¬†2 haben wir unter Verwendung aller Daten gesch√§tzt. Falls wir allerdings erwarten, dass neue Daten zu den Datens√§tzen hinzugef√ºgt werden, ist das nicht die beste Idee, da wir nicht wissen, wie gut unser Modell auf zuvor ungesehenen Daten funktioniert. Eine g√§ngige Praxis ist deshalb, die Daten in ein Trainings- und ein Testset aufzuteilen. Mit den Trainingsdaten werden die Parameter des Modells gesch√§tzt und mit den Testdaten wird die sogenannte Out-of-Sample (OOS) Performance gemessen.\nBeim Aufteilen der Daten sollte beachtet werden, dass das Verh√§ltnis zwischen den beiden Teilen sinnvoll und die Aufteilung zuf√§llig gew√§hlt wird.\nIm obigen Beispiel haben wir gesehen, dass ein zuf√§lliger Split zu besseren Ergebnissen f√ºhren kann. Die Frage ist nun, wie wir in R einen zuf√§lligen Split erzeugen k√∂nnen!",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Weiterf√ºhrende Konzepte des Data Mining</span>"
    ]
  },
  {
    "objectID": "03_Wichtige_Konzepte.html#training-und-test-split",
    "href": "03_Wichtige_Konzepte.html#training-und-test-split",
    "title": "3¬† Weiterf√ºhrende Konzepte des Data Mining",
    "section": "",
    "text": "Beispiel 3.1 In einem gegebenen Datensatz sind 101 Observationen \\((x,y)\\in\\mathbb{R}\\). Nun werden die letzten \\(20\\%\\) dieser Daten als Testdaten festgelegt und die ersten \\(80\\%\\) der Daten als Trainingsdaten. Mit den Trainingsdaten sch√§tzen wir die Parameter eines linearen Regressionsmodells und generieren Vorhersagen auf Basis der Testdaten. Wir k√∂nnen dann die Vorhersageg√ºte des Modells auf den Testdaten visuell evaluieren, indem wir die Abst√§nde der tats√§chlichen Werte der Testdaten zur Regressionsgeraden inspizieren.\n\n\n\n\n\n\n\n\n\nIm obigen Beispiel ist klar zu erkennen, dass der Split (also das Aufteilen der Daten) nicht optimal war. Zwar erzeugen die gesch√§tzten Parameter gute Vorhersagen auf den Trainingsdaten, allerdings sind die Vorhersagefehler in den Testdaten sehr hoch.\nWenn wir stattdessen die Daten in zuf√§llige Trainings- und Testdaten aufteilen, ist f√ºr das gegebene Modell der In-Sample-Fehler (IS-Fehler) zwar h√∂her, allerdings der OOS-Fehler niedriger:\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.1 Training- und Testsplit in R\nDie rsample::initial_split()-, rsample::training()- und rsample::testing()-Funktionen erlauben ein einfaches und effizientes Aufteilen der Daten in Trainings- und Testdaten. Das {rsample}-Paket ist Teil des {tidymodels}-√ñkosystems, weshalb wir auch einfach mit dem Befehl library(tidymodels) dieses hinzuf√ºgen k√∂nnen. In Section 2.3.1 haben wir den penguins-Datensatz verwendet, um die Parameter eines multiplen linearen Regressionsmodells zu sch√§tzen. Des Weiteren haben wir in Section 2.3.4.3 das MLR-Modell auf dem gesamten Datensatz anhand der RMSE-Metrik evaluiert. Hierbei hat sich ein RMSE-Wert von 282 ergeben. Im Folgenden wollen wir das gleiche Modell erstellen, allerdings die Parameter nur mit den Trainingsdaten sch√§tzen und die Performance dann auf den Testdaten evaluieren.\n\ndata_penguin &lt;- palmerpenguins::penguins\n\nset.seed(123)\n\nsplit_penguin &lt;- initial_split(data = data_penguin, prop = 0.8)\ndata_train &lt;- training(split_penguin)\ndata_test &lt;- testing(split_penguin)\n\nIn der dritten Zeile wenden wir den set.seed()-Befehl an. Diese Funktion ist wichtig, um die Reproduzierbarkeit der Resultate zu gew√§hrleisten, wenn zuf√§llige Prozesse im Programmablauf involviert sind. Dadurch, dass wir im Folgenden die Daten zuf√§llig aufteilen, wird unter der Verwendung der set.seed()-Funktion gew√§hrleistet, dass bei jedem Durchf√ºhren der darauffolgenden Zeilen der gleiche zuf√§llige Split durchgef√ºhrt wird.\nIn Zeile 5 verwenden wir die initial_split()-Funktion, um die Daten aufzuteilen. Das Argument data = data_penguin beschreibt, welche Daten aufgeteilt werden sollen, und das Argument prop=0.8 steuert die Proportionalit√§t des Splits. Der Wert prop = 0.8 teilt die Daten somit zuf√§llig in \\(80\\%\\) Trainings- und \\(20\\%\\) Testdaten ein.\nIn Zeile 6 und 7 verwenden wir dann die training()- und testing()-Funktionen, um aus dem split_pinguin-Objekt die Trainings- und Testdaten zu extrahieren. Die Trainings- und Testdaten sind daraufhin in einem Data-Frame bzw. Tibble gespeichert und k√∂nnen wie gewohnt verwendet werden.\nWir k√∂nnen nun die Parameter des MLR-Modells sch√§tzen, wie wir es in #sec-MLRR getan haben:\n\nmlr_spec &lt;- linear_reg()\nmlr_fit &lt;- mlr_spec %&gt;%\n  fit(data = data_train,\n      formula = body_mass_g ~.)\n\nLediglich in Zeile 3 haben wir anstatt des data_penguin-Datensatzes nun den data_train-Datensatz verwendet. Da wir nur an der Modellg√ºte auf Basis der Testdaten interessiert sind, evaluieren wir an dieser Stelle nicht weiter das Modell auf Basis der Trainingsdaten.\nMit der augment()-Funktion k√∂nnen wir nun wieder Vorhersagen und die dazugeh√∂rigen Residuen und mithilfe der rmse()-Funktion den RMSE berechnen:\n\nmlr_fit %&gt;%\n  augment(data_test) %&gt;%\n  rmse(truth = .pred, estimate = body_mass_g)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        289.\n\n\nDer Testfehler mit dem Wert 289 ist also nur knapp gr√∂√üer als der Fehler, welchen wir in Section 2.3.4.3 beobachten konnten (282). Wir k√∂nnen deshalb f√ºr zuvor ungesehene Pinguine erwarten, dass das Gewicht im Schnitt \\(289\\) g h√∂her oder niedriger als der tats√§chliche Wert gesch√§tzt wird.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Weiterf√ºhrende Konzepte des Data Mining</span>"
    ]
  },
  {
    "objectID": "03_Wichtige_Konzepte.html#cross-validation",
    "href": "03_Wichtige_Konzepte.html#cross-validation",
    "title": "3¬† Weiterf√ºhrende Konzepte des Data Mining",
    "section": "3.2 Cross-Validation",
    "text": "3.2 Cross-Validation\nDas Konzept der Cross-Validation (Kreuzvalidierung) kn√ºpft direkt an das des Aufteilens der Trainings- und Testdaten an. Die Idee hierbei ist, dass wir die Daten nicht nur einmal in Trainings- und Testdaten aufteilen, sondern mehrmals auf verschiedene Art und Weise. Die folgende Grafik illustriert die Idee der five-fold Cross-Validation:\n\n\n\n\n\nIn einem ersten Schritt teilen wir die Daten in f√ºnf gleichgro√üe, zuf√§llige Teildatens√§tze (Folds) auf. Vereinigt ergeben diese Teildatens√§tze also wieder den gesamten Datensatz. Diese f√ºnf Folds werden dann f√ºnfmal dupliziert, sodass wir jeden Fold f√ºnfmal auffinden. Die einzelnen Duplikate werden hierbei Splits genannt. In jedem dieser 5 Splits wird dann ein anderer Fold als Testdaten definiert und die verbleibenden vier Folds als Trainingsdaten. Wir k√∂nnen dann vier verschiedene Trainings- und Testdatens√§tze verwenden, um etwa die Parameter eines linearen Modells zu sch√§tzen.\n\n\n\nQuelle\n\n\nIn Example¬†3.1 haben wir gesehen, dass ein unvorteilhafter Training- und Testsplit zu verzerrten Ergebnissen f√ºhren kann. Selbst bei einem zuf√§llig ausgew√§hlten Trainings- und Testsplit besteht das Risiko, einen unvorteilhaften Split zu erstellen. Indem wir die Daten also systematisch aufteilen und den Testfehler mitteln, k√∂nnen wir das Risiko mitigieren. Wir k√∂nnen den Cross-Validation-Testfehler durch den Term\n\\[\n\\text{CV-Loss} = \\frac{1}{5}\\sum_{i=1}^{5} \\text{Loss}_{\\text{OOS}_i}(y,\\hat{y})\n\\] berechnen. \\(\\text{Loss}_{\\text{OOS}_i}\\) steht hierbei f√ºr die OOS-Performance in Split \\(i\\) bez√ºglich einer beliebigen Loss-Funktion wie RMSE.\nWir k√∂nnen die Anzahl der Splits und Folds auch beliebig h√∂her oder niedriger w√§hlen. Falls wir zum Beispiel auf einen Datensatz mit \\(n\\) Datenpunkten \\(n\\)-Fold-Cross-Validation anwenden, d.¬†h., die Testsets aus einem Split entsprechen einem einzelnen Sample, dann spricht man auch von Leave-One-Out-Cross-Validation. Dieses Prinzip garantiert zwar einen sehr robusten \\(\\text{CV-Loss}\\), kann aber unter Umst√§nden auch zu sehr langen Berechnungszeiten f√ºhren, da wir schlie√ülich f√ºr jeden Split ein Modell trainieren.\n\n3.2.1 Cross-Validation in R\nDie rsample::vfold_cv-Funktion erm√∂glicht es uns, ein Cross-Validation-Objekt zu erstellen. Dieses ist nichts anderes als ein verschachteltes Tibble bzw. Data Frame. D.¬†h., jeder Eintrag ist wieder ein Tibble oder Data Frame selbst.\n\nfolds_penguin &lt;- vfold_cv(data = data_penguin, v = 5)\n\nNeben dem Argument data, welches die Daten spezifiziert, m√ºssen wir noch die Anzahl der Folds √ºbergeben. Diese k√∂nnen wir √ºber das Argument v steuern. Falls wir also v=5 setzen, dann bedeutet das, dass wir 5-Fold-Cross-Validation anwenden.\nWir k√∂nnen nun die Parameter eines MLR-Modells f√ºr jeden Split sch√§tzen und den CV-Loss berechnen:\n\nmlr_fit_cv &lt;- mlr_spec %&gt;%\n  fit_resamples(\n    resamples = folds_penguin,\n    preprocessor = body_mass_g ~.\n    )\n\n\nIn der ersten Zeile verwenden wir das bereits spezifizierte MLR-Modell mlr_spec, um f√ºr dieses auf Basis jedes Splits die Parameter zu sch√§tzen.\nIn Zeile 2. verwenden wir statt der fit-Funktion die fit_resamples-Funktion. Wie der Name bereits vermuten l√§sst, k√∂nnen wir mit dieser Funktion die Parameter auf Basis der verschiedenen Splits berechnen.\nWir m√ºssen deshalb in Zeile 3 statt dem Argument data das Argument resamples = folds_penguin √ºbergeben.\nIn der vierten Zeile spezifizieren wir dann mit dem preprocessor-Argument noch die Formel, bez√ºglich welcher die Parameter gesch√§tzt werden sollen. √Ñquivalent dazu wird in der fit-Funktion das Argument formula verwendet. Hintergrund f√ºr die unterschiedlichen Argumentnamen ist, dass wir die fit_resamples-Funktion noch viel umfangreicher nutzen k√∂nnten, das aber den Rahmen dieser Vorlesung sprengt (siehe bei Interesse Machine-Learning-√úbungsskript).\n\nUm nun die Performance der Cross-Validation zu √ºberpr√ºfen, k√∂nnen wir die tune::collect_metrics-Funktion verwenden:\n\nmlr_fit_cv %&gt;% collect_metrics()\n\nDer R√ºckgabewert der collect_metrics-Funktion ist ein Tibble, welches verschiedene Informationen der Evaluation des CV-Objekts enth√§lt:\n\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n287.459\n5.000\n9.545\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.873\n5.000\n0.007\nPreprocessor1_Model1\n\n\n\n\n\n\n\nDa wir keine weiteren Metriken spezifiziert haben, werden automatisch der RMSE sowie \\(R^2_\\text{adj}\\) berechnet. Die Namen der Metriken sind in der Spalte .metric gespeichert. In der Spalte mean k√∂nnen wir den CV-Loss einsehen. Die Spalte n gibt die Anzahl der Folds an und std_error den entsprechenden Standardfehler der Metrik auf Basis der Folds. Die .config-Spalte ist f√ºr diesen Kurs ebenso nicht relevant.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Weiterf√ºhrende Konzepte des Data Mining</span>"
    ]
  },
  {
    "objectID": "03_Wichtige_Konzepte.html#umgang-mit-fehlenden-daten",
    "href": "03_Wichtige_Konzepte.html#umgang-mit-fehlenden-daten",
    "title": "3¬† Weiterf√ºhrende Konzepte des Data Mining",
    "section": "3.3 Umgang mit fehlenden Daten",
    "text": "3.3 Umgang mit fehlenden Daten\nIn Section 1.3.3.2 haben wir bereits die dplyr::na.omit()-Funktion kennengelernt, welche ganze Eintr√§ge entfernt, wenn mindestens eine Variable in des Eintrags nicht vorhanden ist. Diese Art des Umgangs mit fehlenden Daten ist zwar einfach, birgt allerdings Risiken:\n\nFalls wir durch diese Methode eine oder mehrere einflussreiche Variablen entfernen, k√∂nnen die Ergebnisse bei einer anschlie√üenden Regression stark verzerrt sein.\nFalls wir vorher nicht pr√ºfen, wie viele Eintr√§ge durch das Anwenden der Funktion entfernt werden, kann unter Umst√§nden ein Gro√üteil der Daten entfernt werden.\nFalls wir in den Daten ein Muster nicht erkennen, welches durch das Entfernen der Eintr√§ge mit fehlenden Daten entfernt wird, kann das ebenso zu verzerrten Ergebnissen bei einer anschlie√üenden Regression f√ºhren.\n\nAufgrund der obigen Aspekte sollten wir uns also auch mit einer anderen Methode auseinandersetzen, welche sich mit dem Umgang mit fehlenden Werten besch√§ftigt.\n\nAls Beispieldatensatz verwenden wir wieder den Pinguin-Datensatz. Zuerst sollten wir herausfinden, wie viele Eintr√§ge im Datensatz nicht vorhanden sind:\n\ndata_penguin %&gt;%\n  is.na() %&gt;%\n  colSums() %&gt;%\n  bind_rows() %&gt;%\n  t()\n\n                  [,1]\nspecies              0\nisland               0\nbill_length_mm       2\nbill_depth_mm        2\nflipper_length_mm    2\nbody_mass_g          2\nsex                 11\nyear                 0\n\n\nIm Datensatz sind also sowohl numerische als auch nominale Werte teilweise nicht vorhanden.\n\n3.3.1 Imputation\nEine Methode f√ºr den Umgang mit fehlenden Daten ist die Imputation. Die grundlegende Idee hierbei ist simpel: Wir erg√§nzen die fehlenden Werte. Das sinnvolle Erg√§nzen der Werte ist hierbei die gr√∂√üte Schwierigkeit, weshalb wir im Folgenden einige Ans√§tze besprechen.\n\n3.3.1.1 Imputation durch Lagema√üe (cold imputation)\nDas wohl einfachste Imputationsverfahren erg√§nzt fehlende Werte durch Lagema√üe wie den Mittelwert, den Median oder den Modus. Hierbei gilt allerdings zu beachten, dass der Mittelwert und Median ausschlie√ülich f√ºr numerische Variablen verwendet werden k√∂nnen. Das folgende Code-Snippet illustriert, wie wir im Falle des Pinguin-Datensatzes Mean- bzw. Modusimputation durchf√ºhren k√∂nnen:\n\ndata_penguin_imputed &lt;- data_penguin %&gt;%\n  mutate(\n    across(.cols = where(is.numeric),\n           .fns = ~if_else(\n             is.na(.),\n             mean(.,na.rm = TRUE),\n             .)\n    ),\n    across(.cols = where(is.factor),\n           .fns = ~if_else(\n           is.na(.),\n             (\n               data_penguin %&gt;%\n               select(sex) %&gt;%\n               table() %&gt;%\n               as_tibble() %&gt;%\n               filter(n==max(n)) %&gt;%\n               select(sex) %&gt;%\n               pluck(1) %&gt;%\n               factor()\n               ),\n             .)\n    )\n  )\n\n\nIm ersten Schritt erstellen wir einen neuen Datensatz data_penguin_imputed.\nDanach √ºbergeben wir den data_penguin-Datensatz an die mutate-Funktion.\nIn den Zeilen 3‚Äì7 √ºberschreiben wir in den numerischen Spalten die fehlenden Werte mit dem entsprechenden Durchschnitt der Spalte.\n\nHierbei verwenden wir die across- und where-Funktionen. Sinngem√§√ü kann die Kombination dieser beiden Funktionen als ‚Äûw√§hle alle Spalten aus, bei welcher die Bedingung ... gilt‚Äú interpretiert werden. ... ist hierbei ein Platzhalter f√ºr eine Bedingung oder Funktion. In Zeile 3 ist die Bedingung durch die is.numeric()-Funktion gegeben, welche pr√ºft, ob eine Spalte einen numerischen Datentyp hat.\nFalls eine der data_penguin-Spalten einen numerischen Datentyp besitzt, wird die if_else-Funktion auf diese angewendet. Die if_else()-Funktion √ºberpr√ºft ebenso eine Kondition, welche als erstes Argument √ºbergeben wird. Falls die Kondition erf√ºllt ist, so wird das zweite Argument angewendet und falls nicht, entsprechend das dritte.\nWir pr√ºfen also mithilfe der if_else()- und is.na()-Funktionen, ob ein Eintrag in einer Spalte den Wert NA oder NaN besitzt. Falls das der Fall ist, wird dieser Wert mit dem durchschnittlichen Wert dieser Spalte bef√ºllt (berechnet mithilfe der mean-Funktion). Falls der Wert nicht NA oder NaN ist, so wird der urspr√ºngliche Wert beibehalten.\n\nDie Bedeutung der . und ~ Operatoren sind hierbei die folgenden:\n\nDer . Operator wird als Platzhalter f√ºr alle Spalten verwendet, welche aus der vorherigen Operation zur√ºckgegeben wurden. So wird zum Beispiel beim Funktionsaufruf is.na(.) in Zeile 5 mithilfe des . symbolisiert, dass jene Spalten ausgewertet werden, welche zuvor durch den where(is.numeric) selektiert wurden.\nDer ~ Operator wird verwendet um eine anonyme Funktion zu erstellen und anzuwenden. Da das .fn Argument der across()-Funktion eine Funktion selbst erwartet, √ºbergeben wir nicht den ausgewerteten Funktionsaufruf is.na(.) direkt, sondern symbolisieren mithilfe des ~ Operators, dass diese Funktion auf Basis der vorherigen Argumente ausgewertet werden soll.\n\nIn den Zeilen 9‚Äì22 wird das Prozedere f√ºr die Faktorvariablen wiederholt, mit dem entscheidenden Unterschied, dass hier nicht der Durchschnitt, sondern der Modus in den Zeilen 13‚Äì20 berechnet wird. Als zweites Argument der if_else-Funktion wird der Modus der Spalte sex berechnet.\n\nVorteile der Lagema√üimputation umfassen die Einfachheit der Implementierung, schnelle Berechenbarkeit und Stabilit√§t.\n\n\n3.3.1.2 Imputation durch Regression\nNeben der Imputation durch Lagema√üe werden heutzutage auch oft komplexere Algorithmen verwendet, welche die fehlenden Daten genauer approximieren k√∂nnen. Eine relativ einfache, aber dennoch fortgeschrittene Methode ist die Imputation mithilfe eines linearen Modells. Falls in einem Datensatz mit unabh√§ngigen Variablen \\(X = (X_1, X_2, \\ldots, X_J)\\) etwa Werte in der Spalte \\(X_j,\\, 1\\leq j\\leq J\\) vorhanden sind, so kann mithilfe eines MLR-Modells mit der Modellgleichung\n\\[ X_j = \\sum_{i=1,\\\\ i\\neq j}^J \\beta_iX_i+\\beta_0+\\varepsilon\\] gesch√§tzt werden. Zum Sch√§tzen der Parameter k√∂nnen wir hierbei alle Eintr√§ge verwenden, in welchen die Variable \\(X_j\\) vorhanden ist. Dieser Ansatz funktioniert besonders gut, wenn die unterliegenden Daten alle Annahmen des MLR-Modells erf√ºllen.\nMithilfe der {mice}-Library1, k√∂nnen wir nicht nur eine Imputation wie in Section 3.3.1.1 durchf√ºhren, sondern auch eine Regression, wie sie oben beschrieben wurde. Hierf√ºr ben√∂tigen wir die Funktion mice und complete:\n\nlibrary(mice)\n\nmethods &lt;- c(bill_length_mm = \"norm.predict\",\n            bill_depth_mm = \"norm.predict\",\n            flipper_length_mm = \"norm.predict\",\n            body_mass_g = \"norm.predict\",\n            sex = \"logreg\"\n            )\n\ndata_penguin_imputed &lt;- data_penguin %&gt;%\n  mice(method = methods,\n       blocks = names(methods),\n       maxit = 5,\n       m = 1) %&gt;%\n  complete()\n\n\nIn der ersten Zeile f√ºgen wir die {mice}-Library unserer Session hinzu.\nIn den Zeilen 3‚Äì7 spezifizieren wir f√ºr jede Variable mit fehlenden Werten eine Methode, welche f√ºr die Imputation verwendet werden soll. Die Methode \"norm.predict\" spezifiziert hierbei ein MLR-Modell f√ºr die numerischen Variablen und \"logreg\" ein logistisches Modell f√ºr die nominale Variable \"sex\".\nIn Zeile 10 generieren wir einen neuen Datensatz data_penguin_imputed, welcher sp√§ter den vervollst√§ndigten Datensatz enth√§lt.\nAnschlie√üend wenden wir auf den alten Datensatz in den Zeilen 10‚Äì14 die mice-Funktion an. Das Argument method spezifiziert, wie wir die fehlenden Werte ersetzen wollen (als Argument geben wir hierbei den gleichnamigen Vektor aus Zeile 3 an). Das Argument blocks spezifiziert, auf welche Variablen die jeweiligen Imputationsalgorithmen angewendet werden sollen. Durch den Wert names(methods) k√∂nnen wir direkt die Imputationsalgorithmen mit den entsprechenden Variablen matchen.\nDas Argument maxit=1 in Zeile 9 spezifiziert die Anzahl der Iterationen f√ºr das Generieren der synthetischen Daten. Die Details hierf√ºr w√ºrden den Rahmen der Vorlesung sprengen und k√∂nnen unter diesem Link bei Interesse nachgeschlagen werden. Wichtig ist nur das Verst√§ndnis, dass eine h√∂here Anzahl an Iterationen stabilere Werte liefern kann.\nDas Argument m beschreibt, wie oft ein Datensatz mithilfe der Imputationstechniken erzeugt werden soll. Das Argument m=1 spezifiziert somit, dass wir nur einen solchen Datensatz erzeugen wollen.\nDa der R√ºckgabewert der mice-Funktion ein Objekt ist, welches mehr als nur den vervollst√§ndigten Datensatz enth√§lt, verwenden wir zum Schluss die complete-Funktion, welche uns einen vervollst√§ndigten Datensatz zur√ºckgibt.\n\nWir k√∂nnen nun nochmals √ºberpr√ºfen, ob wirklich alle Werte bef√ºllt wurden:\n\ndata_penguin_imputed %&gt;%\n  is.na() %&gt;%\n  colSums() %&gt;%\n  bind_rows() %&gt;%\n  t()\n\n                  [,1]\nspecies              0\nisland               0\nbill_length_mm       0\nbill_depth_mm        0\nflipper_length_mm    0\nbody_mass_g          0\nsex                  0\nyear                 0",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Weiterf√ºhrende Konzepte des Data Mining</span>"
    ]
  },
  {
    "objectID": "03_Wichtige_Konzepte.html#merkmalsselektion-mithilfe-von-lasso-regression",
    "href": "03_Wichtige_Konzepte.html#merkmalsselektion-mithilfe-von-lasso-regression",
    "title": "3¬† Weiterf√ºhrende Konzepte des Data Mining",
    "section": "3.4 Merkmalsselektion mithilfe von Lasso-Regression",
    "text": "3.4 Merkmalsselektion mithilfe von Lasso-Regression\nEin weiteres wichtiges Thema ist die Variablenselektion. Motivierende Faktoren f√ºr die Variablenselektion sind:\n\nIn Umfangreichen Datens√§tzen haben nicht alle unabh√§ngigen Variablen aufgrund von Multikollinearit√§t einen signifikanten Einfluss auf die Zielvariable.\nFalls Interpretierbarkeit eine gro√üe Rolle spielt, ist es manchmal vorteilhaft sich auf einige wenige Variablen festzulegen, welche die gesch√§tzte Variable hinreichend erkl√§ren.\n\nIn der Vorlesung wurden bereits Methoden wie Forward, Backward und Stepwise Selektion behandelt. Wir wollen deshalb in diesem Abschnitt eine weitere, essenzielle Methodik betrachten, welche auf der Idee der linearen Regression aufbaut.\nAngenommen, wir befinden uns in dem bekannten Fall, dass ein Datensatz \\(X = (X_1,\\ldots,X_J)\\) mit \\(X_J\\in\\mathbb{R}^K\\) und \\(K\\) Realisierungen der Zielvariable \\(Y=(y_1,\\ldots,y_k)\\) vorliegt und wir die Parameter \\(\\hat{b_0},\\ldots,\\hat{b_J}\\) sch√§tzen. Die Sch√§tzung der Parameter kann zum Beispiel durch die Minimierung des MSE (siehe vorheriges Kapitel) durchgef√ºhrt werden:\n\\[\n\\min_{b_0,\\cdots,b_J}\\frac{1}{K}\\sum_{k=1}^K (y_k-(b_0+b_1x_{1,k}+\\cdots+b_Jx_{J,k}))^2\n\\] Durch das Hinzuf√ºgen eines sogenannten Penalty-Terms k√∂nnen wir steuern, dass Parameter \\(b_j\\), welche keinen gro√üen Einfluss auf die Zielvariable haben, verkleinert oder sogar ganz eliminiert werden:\n\\[\n\\min_{b_0,\\cdots,b_J}\\frac{1}{K}\\sum_{k=1}^K (y_k-(b_0+b_1x_{1,k}+\\cdots+b_Jx_{J,k}))^2+\\lambda\\sum_{j=0}^J|b_j|\n\\tag{3.1}\\]\n\\(\\lambda\\in[0,\\infty)\\) (\\(\\lambda\\)) steht hier f√ºr den Penalty: Je gr√∂√üer dieser Wert ist, desto st√§rker werden die Parameter bestraft. F√ºr die genaue Funktionsweise und Methodik hinter diesem Konzept siehe Wikipedia Artikel. Wichtig an dieser Stelle ist lediglich zu verstehen, wie der Fehlerterm in Gleichung¬†3.1 definiert ist und welchen Einfluss der Parameter \\(\\lambda\\) hat.\nDie R-Syntax zum Trainieren und Evaluieren eines solchen Modells ist sehr nah an der eines MLR-Modells:\n\nmlr_spec_lasso &lt;-linear_reg(penalty = 20,\n                            mixture = 1,\n                            engine = \"glmnet\") \n\nmlr_lasso_fit &lt;- mlr_spec_lasso %&gt;%\n  fit(data = data_train,\n      formula = body_mass_g~.)\n\nmlr_lasso_fit %&gt;%\n  tidy()\n\n# A tibble: 10 √ó 3\n   term              estimate penalty\n   &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)       63116.        20\n 2 speciesChinstrap   -173.        20\n 3 speciesGentoo       676.        20\n 4 islandDream          -4.23      20\n 5 islandTorgersen       0         20\n 6 bill_length_mm        9.03      20\n 7 bill_depth_mm         0         20\n 8 flipper_length_mm    21.0       20\n 9 sexmale             454.        20\n10 year                -31.8       20\n\n\nBevor wir den obigen Code analysieren, ist es sinnvoll, den R√ºckgabewert zu betrachten. Das Tibble enth√§lt drei Spalten, wobei in der ersten der Variablenname, in der zweiten der gesch√§tzte Lasso-Parameter \\(b^{\\text{Lasso}}_j\\) und in der dritten der Term \\(\\lambda = 20\\) steht. Der Parameter \\(\\lambda = 20\\) hat daf√ºr gesorgt, dass die gesch√§tzten Parameter f√ºr die Variablen bill_depth_mm und islandTorgersen auf \\(0\\) reduziert wurden. Da islandTorgersen Teil eines Dummy-Konstrukts ist, k√∂nnen wir diese Spalte nicht direkt entfernen. Allerdings k√∂nnten wir auf Basis unserer Analyse etwa die Variable bill_depth_mm entfernen.\n\n\n\n\n\n\nAchtung\n\n\n\nDie gesch√§tzten Parameter sind nicht mehr unbiased, weshalb eine statistische Analyse, zum Beispiel durch den \\(t\\)-Test, hier weniger sinnvoll ist als bei einem MLR-Modell.\n\n\nNun zur√ºck zum Code:\n\nmlr_spec_lasso &lt;-linear_reg(penalty = 20,\n                            mixture = 1,\n                            engine = \"glmnet\") \n\nmlr_lasso_fit &lt;- mlr_spec_lasso %&gt;%\n  fit(data = data_train,\n      formula = body_mass_g~.)\n\nmlr_lasso_fit %&gt;%\n  tidy()\n\nIm Vergleich zu einem MLR-Modell wie in Section 2.3.1 haben wir bei der Modellspezifikation durch die linear_reg-Funktion die Argumente penalty und mixture erg√§nzt. Wie bereits besprochen, steht penalty hier f√ºr den Parameter \\(\\lambda\\) aus Gleichung¬†3.1. Der Parameter mixture = 1 spezifiziert hierbei, dass es sich um ein Lasso-Model handelt. Der Parameter mixture kann Werte in dem Intervall \\([0,1]\\) annehmen, wobei wir ausschlie√ülich an dem Fall mixture=1 interessiert sind. Au√üerdem haben wir in Zeile 3 das Argument engine = \"glmnet\" spezifiziert. Dieses steuert, dass die Parameter nicht mehr wie bei einer linearen Regression berechnet werden (aufgrund des Penalty-Terms), sondern durch ein Optimierungsverfahren, welches Gleichung¬†3.1 l√∂st. Im letzten Schritt rufen wir nicht wie gewohnt die extract_fit_engine()- und summary()-Funktionen auf, sondern wenden auf das trainierte Modell die tidy()-Funktion an, welche die gesch√§tzten Parameter als Tibble zur√ºckgibt.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Weiterf√ºhrende Konzepte des Data Mining</span>"
    ]
  },
  {
    "objectID": "03_Wichtige_Konzepte.html#√ºbungsaufgaben",
    "href": "03_Wichtige_Konzepte.html#√ºbungsaufgaben",
    "title": "3¬† Weiterf√ºhrende Konzepte des Data Mining",
    "section": "3.5 √úbungsaufgaben",
    "text": "3.5 √úbungsaufgaben\nIn dieser √úbung werden wir uns noch einmal mit dem hotel_rates-Datensatz aus den √úbungen Aufgabe¬†2.8 ‚Äì Aufgabe¬†2.11 auseinandersetzen.\nVerwende die unten stehende Version des hotel_rates-Datensatzes f√ºr die folgenden Aufgaben.\n\nhotel_rates_filtered &lt;- hotel_rates %&gt;%\n mutate(\n   arrival_month = factor(month(arrival_date)),\n   num_guests = adults+children,\n   is_repeated_guest = factor(is_repeated_guest),\n   ) %&gt;%\n   select(num_guests,total_of_special_requests,\n          required_car_parking_spaces,arrival_month,\n          is_repeated_guest,lead_time,stays_in_week_nights,\n          avg_price_per_room,country)\n\n\nAufgabe 3.1 ¬†\n\nIn der Variable country wurden fehlende Werte mit dem String \"null\" encodiert. Finde heraus, wie viele Eintr√§ge der Variable country den Wert \"null\" besitzen.\nVerwende die dplyr::na_if-Funktion, um die mit \"null\" encodierten Werte in den Wert NA umzuwandeln.\nWir wollen nun die fehlenden Werte erg√§nzen. Da die Variable country nominal ist, k√∂nnen wir keine lineare Regression anwenden. Wende stattdessen in der mice()-Funktion die Methode \"cart\" an, was f√ºr Classification And Regression Trees steht (diese werden im folgenden Kapitel genauer beleuchtet). Setze die Parameter maxit und m beide auf 1 und erzeuge dann mithilfe der complete()-Funktion einen neuen Datensatz hotel_rate_imp, welcher die imputierten Werte enth√§lt.\n\n\n\nAufgabe 3.2 ¬†\n\nDa in dem Datensatz etwa 100 verschiedene Nationalit√§ten vertreten sind, wir aber nur an jenen interessiert sind, welche zur Erkl√§rung der Zielvariable beitragen, sollte die Spalte country gefiltert werden. Der folgende Vektor enth√§lt 39 Country-Codes, von welchen wir vermuten, dass sie einen statistisch signifikanten Einfluss auf die Variable avg_price_per_room haben.\n\ncntry_list&lt;-c(\"and\", \"aut\", \"aze\",\n              \"che\", \"chl\", \"cze\",\n              \"esp\", \"geo\", \"hun\", \"ita\", \n              \"lux\", \"mar\", \"mwi\", \"nga\", \"prt\", \n              \"rus\", \"swe\", \"tur\", \"twn\", \n              \"usa\")\n\nPasse unter Verwendung der filter()-Funktion und des %in%-Operators den Datensatz aus Aufgabe¬†3.1 so an, dass dieser nur noch Eintr√§ge enth√§lt, welche in der Spalte country einen Wert des obigen Vektors enthalten.\nEntferne mit der gleichen Methode alle Eintr√§ge, welche in der Spalte arrival_month die Werte month2 und month11 enthalten.\n\n\n\nAufgabe 3.3 Verwende den in Aufgabe¬†3.2 erstellten Datensatz, um einen Trainings- und Testdatensatz im Verh√§ltnis 4:1 zu erstellen. Um die Reproduzierbarkeit sicherzustellen, verwende den set.seed(123)-Befehl.\n\n\nAufgabe 3.4 ¬†\n\nVerwende die in der vorherigen Aufgabe erstellten Trainingsdaten, um ein MLR-Modell wie in Aufgabe Aufgabe¬†2.9 zu sch√§tzen.\nWelches der noch im Datensatz enthaltenen Herkunftsl√§nder hat den gr√∂√üten Einfluss auf den durchschnittlichen Zimmerpreis?\nBerechne anschlie√üend f√ºr beide Teildatens√§tze den \\(R^2\\) und \\(RMSE\\) und vergleiche die beiden Werte.\n\n\n\nAufgabe 3.5 Verwende den Datensatz aus Aufgabe¬†3.2 um ein 5-Fold-Cross-Validation-(CV-)Objekt zu erstellen. Sch√§tze anschlie√üend auf diesem CV-Objekt ein MLR-Modell wie in Aufgabe¬†3.4 und vergleiche den CV-RMSE mit dem Test-RMSE aus Aufgabe Aufgabe¬†3.4. Achte auch hierbei auf Reproduzierbarkeit und verwende den Befehl set.seed(123).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Weiterf√ºhrende Konzepte des Data Mining</span>"
    ]
  },
  {
    "objectID": "03_Wichtige_Konzepte.html#l√∂sungen",
    "href": "03_Wichtige_Konzepte.html#l√∂sungen",
    "title": "3¬† Weiterf√ºhrende Konzepte des Data Mining",
    "section": "3.6 L√∂sungen",
    "text": "3.6 L√∂sungen\n\nSolution 3.1 (Solution¬†3.1). \n\n\n\nhotel_rates_filtered %&gt;% filter(country==\"null\") %&gt;% nrow()\n\n\n\nhotel_rate_filtered_na &lt;- hotel_rates_filtered %&gt;%\n  mutate(\n    country=na_if(country,\"null\")\n  )\n\n\n\nhotel_rate_imp &lt;- hotel_rate_filtered_na %&gt;%\n  mice(method = \"cart\",\n       blocks = c(\"country\"),\n       maxit = 1 ,\n       m = 1) %&gt;%\n  complete()\n\n\n\n\nSolution 3.2 (Aufgabe¬†3.2). \n\nhotel_rate_imp &lt;- hotel_rate_imp %&gt;%\n  filter(country %in% cntry_list, arrival_month %in% c(1,3:10,12))\n\nDurch das Hinzuf√ºgen von zwei Argumenten in der filter-Funktion werden nur Eintr√§ge zur√ºckgegeben, welche auch beide Bedingungen erf√ºllen. Der %in%-Operator in den beiden Bedingungen √ºberpr√ºft f√ºr jedes Element einer Liste auf der linken Seite des Operators, ob dieses in einer Liste auf der rechten Seite des Operators vorhanden ist. Der R√ºckgabewert des Operators ist eine Liste, welche die Werte TRUE und FALSE enth√§lt. Ein Eintrag besitzt z.¬†B. den Wert TRUE, falls der in country betrachtete Wert in der Liste cntry_list vorhanden ist, und sonst FALSE.\n\n\nSolution 3.3 (Aufgabe¬†3.3). \n\nset.seed(123)\n\nsplit_hotels &lt;- initial_split(hotel_rate_imp, prop = 0.8)\ndata_train &lt;- training(split_hotels)\ndata_test &lt;- testing(split_hotels)\n\n\n\nSolution 3.4 (Aufgabe¬†3.4). \n\n\n\nmlr_res &lt;- linear_reg() %&gt;%\n  fit(formula = avg_price_per_room ~., data = data_train)\n\n\n\nmlr_res %&gt;%\n  extract_fit_engine() %&gt;%\n  lm.beta::lm.beta() %&gt;%\n  tidy() %&gt;%\n  filter(grepl(\"country\",term)) %&gt;%\n  mutate(std_estimate = abs(std_estimate)) %&gt;%\n  arrange(desc(std_estimate)) %&gt;%\n  head(1)\n\n# A tibble: 1 √ó 6\n  term       estimate std_estimate std.error statistic p.value\n  &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 countryprt    -72.3        0.466      26.9     -2.69 0.00712\n\n\nDas Herkunftsland mit dem Code prt, was f√ºr Portugal steht, hat den gr√∂√üten Einfluss auf den Preis.\nZur Erkl√§rung des Codes:\nUm herauszufinden, welches Herkunftsland den gr√∂√üten Einfluss auf den durchschnittlichen Zimmerpreis hat, m√ºssen wir auf das MLR-Modell die lm.beta()-Funktion angeben. Dadurch, dass im Modell relativ viele Dummy Variablen enthalten sind, speichern wir durch das Anwenden der tibble()-Funktion in Zeile 4 die Ergebnisse in einem Tibble. Das erlaubt uns dann in Zeile 5 die filter()-Funktion anzuwenden. In Zeile 5 verwenden wir in der filter()-Funktion die grepl()-Funktion, welche das im ersten Argument gegebene Muster (country) in der Spalte term sucht und nur jene Eintr√§ge zur√ºckgibt, welche den String \"country\" enthalten. Wir sortieren somit alle Variablen aus, welche keine Informationen √ºber das Herkunftsland liefern. In Zeile 6 wenden wir die Betragsfunktion (abs()) auf die Spalte std_estimate an. Durch die arrange(desc(‚Ä¶))-Funktion in Zeile 7 erreichen wir, dass die Eintr√§ge der Gr√∂√üe nach absteigend sortiert sind, sodass wir dann in Zeile 8 mit dem head(1)-Befehl jene Variable ausgeben k√∂nnen, welche den gr√∂√üten Einfluss auf die Zielvariable besitzt.\n\n\nmulti_metric &lt;- metric_set(rmse,rsq)\n\ntest_metrics &lt;- mlr_res %&gt;%\n  augment(data_test) %&gt;%\n  multi_metric(.pred,avg_price_per_room) %&gt;%\n  mutate(dset = \"test\")\n\ntrain_metrics &lt;- mlr_res %&gt;%\n  augment(data_train) %&gt;%\n  multi_metric(.pred,avg_price_per_room) %&gt;%\n  mutate(dset = \"train\")\n\nrbind(train_metrics,test_metrics)\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\ndset\n\n\n\n\nrmse\nstandard\n37.767\ntrain\n\n\nrsq\nstandard\n0.746\ntrain\n\n\nrmse\nstandard\n38.238\ntest\n\n\nrsq\nstandard\n0.732\ntest\n\n\n\n\n\n\n\nEs ist nicht weiter √ºberraschend, dass der IS RMSE niedriger und der IS \\(R^2\\) h√∂her sind als die entsprechenden OOS-Metriken, da wir die Parameter des MLR-Modells auf Basis der Trainingsdaten gesch√§tzt haben.\n\n\n\nSolution 3.5 (Aufgabe¬†3.5). \n\nset.seed(123)\n\nfolds_hotel &lt;- vfold_cv(hotel_rate_imp, v =5)\n\nmlr_fit_cv &lt;- linear_reg() %&gt;%\n  fit_resamples(\n    resamples = folds_hotel,\n    preprocessor = avg_price_per_room ~.\n    )\n\nmetrics_cv &lt;- mlr_fit_cv %&gt;%\n  collect_metrics() %&gt;%\n  mutate(dset = \"cv\") %&gt;%\n  select(-c(n,std_err,.config)) %&gt;%\n  rename(c(\".estimate\" = \"mean\"))\n\nrbind(metrics_cv,test_metrics)\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\ndset\n\n\n\n\nrmse\nstandard\n38.101\ncv\n\n\nrsq\nstandard\n0.740\ncv\n\n\nrmse\nstandard\n38.238\ntest\n\n\nrsq\nstandard\n0.732\ntest\n\n\n\n\n\n\n\nDie Metriken, gemessen am Testset, sind konservativer. Das k√∂nnte daran liegen, dass das einmal auf den Trainingsdaten gesch√§tzte lineare Modell sich zu stark an die Trainingsdaten angepasst hat.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Weiterf√ºhrende Konzepte des Data Mining</span>"
    ]
  },
  {
    "objectID": "03_Wichtige_Konzepte.html#footnotes",
    "href": "03_Wichtige_Konzepte.html#footnotes",
    "title": "3¬† Weiterf√ºhrende Konzepte des Data Mining",
    "section": "",
    "text": "Diese muss eventuell √ºber den Befehl install.packages(\"mice\") installiert werden‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Weiterf√ºhrende Konzepte des Data Mining</span>"
    ]
  },
  {
    "objectID": "04_Regressions_Baeume.html",
    "href": "04_Regressions_Baeume.html",
    "title": "4¬† Regressionsb√§ume",
    "section": "",
    "text": "4.1 Grundidee\nRegressionsb√§ume sind Teil der Classification and Regression Tree (CART)‚ÄëFamilie. Regressions- und Klassifikationsb√§ume werden auch Entscheidungsb√§ume genannt, da die Vorhersagen durch (bin√§r: ‚ÄòJa‚Äô/‚ÄòNein‚Äô) Entscheidungen berechnet werden.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regressionsb√§ume</span>"
    ]
  },
  {
    "objectID": "04_Regressions_Baeume.html#grundidee",
    "href": "04_Regressions_Baeume.html#grundidee",
    "title": "4¬† Regressionsb√§ume",
    "section": "",
    "text": "4.1.1 Darstellung von B√§umen und Vorhersagen\nIn diesem Unterkapitel diskutieren wir zwei M√∂glichkeiten, um Entscheidungsb√§ume darzustellen.\n\n4.1.1.1 Baumdiagramme\nDie grafischen Bestandteile bei der Darstellung eines Entscheidungsbaums als Baumdiagramm sind Knoten (nodes) und Kanten. Bei den Knoten unterscheiden wir vor allem zwischen dem Wurzelknoten (root node) und Blattknoten. Der Wurzelknoten steht hierbei immer ganz oben im Diagramm und die Blattknoten am Ende der jeweiligen Entscheidungspfade. Knoten, welche zwischen dem Wurzel- und den Blattknoten stehen, nennen wir auch Entscheidungsknoten. (Beachte, dass auch der Wurzelknoten ein Entscheidungsknoten ist!) In einem Entscheidungsknoten pr√ºfen wir eine aufgestellte Bedingung und bewegen uns auf Basis der Entscheidung entsprechend entlang der Kanten. Bei numerischen Features sind die Bedingungen in Form eines Gr√∂√üenvergleichs gegeben. Es wird also √ºberpr√ºft, ob das Feature eines Datenpunktes gr√∂√üer (gleich) oder kleiner als ein Schwellenwert ist. √Ñhnlich wie beim Dummy-Encoding bei einer linearen Regression wird bei nominalen Merkmalen lediglich gepr√ºft, ob die Auspr√§gung der Variable mit dem Entscheidungswert √ºbereinstimmt oder nicht. Diesen Vorgang wiederholen wir, bis ein Blattknoten erreicht wird, in welchem die Vorhersagen stehen.\nFolgende Grafik zeigt eine beispielhafte Darstellung eines solchen Entscheidungsbaums.\n\n\n\n\n\n\nFigure¬†4.1\n\n\n\nDas in Figure¬†4.1 dargestellte Baumdiagramm wurde auf Basis eines Datensatzes erzeugt, welcher die beiden Variablen \\(X_1\\) und \\(X_2\\) enth√§lt. In jedem Entscheidungsknoten wird das Entscheidungskriterium durch einen Schwellenwert (Splittingpoint) \\(t_i,i=1,\\ldots,6\\) ausgedr√ºckt. Falls in einem bin√§ren Entscheidungsbaum also \\(6\\) Splittingpoints gegeben sind, dann existieren \\(6 + 1 = 7\\) Blattknoten. Da der oberste Knoten (Wurzelknoten) auch ein Entscheidungsknoten ist, hat dieser teilweise die gleiche Farbe wie die anderen Entscheidungsknoten, ist aufgrund seiner Wurzeleigenschaft aber noch einmal gesondert zu betrachten.\nWir pr√ºfen also im ersten Schritt, ob die Variable \\(X_2\\) kleiner oder gleich \\(t_1\\) ist. Falls das der Fall ist, bewegen wir uns entlang der linken Kante und ansonsten entlang der rechten Kante zum n√§chsten Entscheidungsknoten. Angenommen, \\(X_2\\leq t_1\\), dann bewegen wir uns zum n√§chsten Knoten, in welchem wir die Bedingung \\(X_1\\leq t_2\\) pr√ºfen. Falls diese Bedingung ebenso erf√ºllt ist, landen wir im Blattknoten mit dem Wert \\(R_1\\). Der Wert \\(R_1\\) ist dann die Vorhersage f√ºr den entsprechenden Datenpunkt. Wie die Werte \\(R_k, k=1,\\ldots,6\\) berechnet werden, untersuchen wir in Section 4.2.\n\n\n4.1.1.2 Aufteilung des Featurespace\n√Ñquivalent zur Darstellung eines Diagramms k√∂nnen wir, zumindest bei ein- oder zweidimensionalen Datens√§tzen, einen Entscheidungsbaum auch im Featurespace darstellen. Mit Featurespace meinen wir hier den Raum, welcher durch die Auspr√§gungen aufgespannt wird. Existieren wie in Figure¬†4.1 zum Beispiel zwei Feature \\(X_1\\) und \\(X_2\\), dann k√∂nnen wir die Splittingpoints und Blattknoten in einem Koordinatensystem darstellen:\n\n\n\n\n\n\nFigure¬†4.2\n\n\n\nBeachte, dass die Darstellungen in Figure¬†4.1 und Figure¬†4.2 √§quivalent sind. Wir k√∂nnen also jeweils die eine Darstellung in die andere √ºberf√ºhren. Um etwa den Blattknoten \\(R_6\\) aus Figure¬†4.1 zu erreichen, m√ºssen wir in Figure¬†4.2 die gleichen Bedingungen pr√ºfen:\n\n\\(X_2\\leq t_1\\) ist bei \\(R_6\\) nicht erf√ºllt und deshalb \\(X_2&gt;t_1\\). Wir befinden uns also oberhalb der horizontalen Linie durch den Punkt \\(t_1\\).\n\\(X_1\\leq t_4\\) ist bei \\(R_6\\) ebenso nicht erf√ºllt, weshalb \\(X_1&gt;t_4\\) gilt.\n\\(X_1\\leq t_5\\) ist bei \\(R_6\\) ebenso nicht erf√ºllt, weshalb \\(X_1&gt;t_5\\) gilt.\n\\(X_2\\leq t_6\\) ist bei \\(R_6\\) ebensowenig erf√ºllt, weshalb \\(X_2&gt;t_6\\) gilt.\n\nZusammengefasst muss f√ºr einen Datenpunkt \\((x_1,x_2)\\) also \\(x_2&gt;t_6\\) und \\(x_1&gt;t_5\\) gelten, damit der Datenpunkt in den Blattknoten \\(R_6\\) f√§llt.\n\n\n\n4.1.2 Vorteile gegen√ºber linearen Modellen\nB√§ume geh√∂ren aufgrund ihrer Struktur zu nichtlinearen Modellen. Falls die unterliegenden Daten also nicht der Linearit√§tsannahme unterliegen, k√∂nnen wir bei einem guten Baum erwarten, dass dieser die Zusammenh√§nge in den Daten besser beschreibt.\nBetrachte hierf√ºr folgendes synthetisches Beispiel, welches diesen Effekt illustriert:\n\nBeispiel 4.1 Der Datensatz data_exm besteht aus zwei Spalten x und y, wobei x Werte zwischen \\(0\\) und \\(10\\) annimmt und \\(y\\) durch die Formel \\[y = f(x)+\\varepsilon=\\sin(x)+\\varepsilon \\]\n(mit \\(\\varepsilon \\sim \\mathcal{N}(0,1)\\)) berechnet wird. y soll hierbei die abh√§ngige und x die unabh√§ngige Variable darstellen und das Ziel ist deshalb, y durch x zu approximieren.\nDie folgende Grafik zeigt eine Realisation dieser Simulation:\n\n\n\n\n\n\n\n\n\nFalls wir nun versuchen, den Zusammenhang zwischen y und x mithilfe eines einfachen linearen Modells zu modellieren, so erhalten wir die Regressionsgerade, welche in der folgenden Grafik dargestellt ist.\n\n\n\n\n\n\n\n\n\nOffensichtlich eignet sich die einfache lineare Regression nicht, um den Zusammenhang zwischen y und x zu modellieren. Wir k√∂nnen allerdings den Zusammenhang ebenso durch einen Regressionsbaum modellieren, welcher im Featurespace wie folgt dargestellt werden kann.\n\n\n\n\n\n\n\n\n\nBeachte, dass unser Featurespace in diesem Beispiel nur eindimensional ist, weshalb das resultierende Baummodell keine Fl√§chen (wie in Figure¬†4.2), sondern Geraden enth√§lt. Die rote Linie beschreibt hierbei die durch das Baummodell gesch√§tzten Werte \\(\\hat{y}\\) f√ºr ein gegebenes \\(x\\). Das Baummodell modelliert den Einfluss von x auf y also vergleichsweise gut, obwohl die Daten keinen linearen Zusammenhang besitzen!\n\nZusammengefasst spiegelt sich der erste Vorteil von Baummodellen gegen√ºber linearen Modellen also in der F√§higkeit wider, auch nichtlineare Zusammenh√§nge zu modellieren.\nEinen weiteren Vorteil k√∂nnen wir bei der Betrachtung von Figure¬†4.1 erkennen: Bei Baummodellen ist sehr leicht erkennbar, auf Basis welcher Variablen und Schwellenwerte die Vorhersagen getroffen werden. Obwohl das Modell aus einer Vielzahl von Variablen besteht, kann man durch die Darstellung eines Baumdiagramms effizient den Wert berechnen, welcher durch das Modell vorhergesagt wird.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regressionsb√§ume</span>"
    ]
  },
  {
    "objectID": "04_Regressions_Baeume.html#sec-tdgreedy",
    "href": "04_Regressions_Baeume.html#sec-tdgreedy",
    "title": "4¬† Regressionsb√§ume",
    "section": "4.2 Top-Down-Greedy",
    "text": "4.2 Top-Down-Greedy\nBisher haben wir zwar verschiedene Darstellungen der Regressionsb√§ume betrachtet und herausgefunden, wie wir bei den verschiedenen Darstellungen Vorhersagewerte ablesen k√∂nnen, allerdings nicht, wie die Schwellwerte und Splittingvariablen bestimmt werden.\nDer Algorithmus, welcher hierf√ºr verwendet wird, ist der sogenannte Top-Down-Greedy- oder Recursive-Binary-Splitting-Algorithmus. Um den Algorithmus zu erkl√§ren, orientieren wir uns prim√§r an der Darstellung aus Figure¬†4.1.\nDie Idee des Algorithmus ist, dass wir in jedem Schritt den Feature Space bestm√∂glich zweiteilen, um eine verbesserte Vorhersage im Vergleich zur ungeteilten Version zu erhalten. Formal l√§sst sich der Algorithmus wie folgt darstellen:\n\nInitialisierung:\nBeginne mit dem gesamten Trainingsdatensatz \\[D = \\{(x_k,y_k):x_k\\in\\mathbb{R}^J,\\: y_k\\in\\mathbb{R},\\: k = 1,\\cdots,K\\}\\] in der Wurzel des Baums.\nSpaltensuche :\nF√ºr jedes Feature \\(X_j\\) und jeden potenziellen1 Schwellenwert \\(s\\):\n\nTeile die Daten in zwei Regionen: \\[\\begin{align*}\n  R_1(j, s) &= \\{ x \\in D \\mid x_j \\leq s \\}\\\\\n  R_2(j, s) &= \\{ x \\in D \\mid x_j &gt; s \\}\n\\end{align*}\\]\nBerechne den MSE, welcher durch diese Aufteilung entsteht:\n\\[\\begin{equation*}\n  \\text{MSE}(j, s) = \\frac{1}{|R_1|}\\sum_{i: x_i \\in R_1(j, s)} (y_i - \\bar{y}_{R_1})^2 + \\frac{1}{|R_2|}\\sum_{i: x_i \\in R_2(j, s)} (y_i - \\bar{y}_{R_2})^2\n\\end{equation*}\\]\nWobei \\(\\bar{y}_{R_1}\\) der durchschnittliche Wert aller Datenpunkte \\(y\\) ist, welche in \\(R_1\\) liegen, und \\(\\bar{y}_{R_2}\\) der durchschnittliche Wert aller Datenpunkte \\(y\\), welche in \\(R_2\\) liegen.\n\nWahl des besten Schwellenwerts :\nW√§hle das Paar \\((j^{*},s^{*})\\), welches den MSE minimiert.\nRekursives Aufteilen:\n\nErzeuge zwei neue Knoten f√ºr \\(R_1(j^{*}, s^{*})\\) und \\(R_2(j^{*}, s^{*})\\).\nWiederhole Schritte 2‚Äì4 rekursiv f√ºr jeden dieser Knoten.\n\nStopkriterium pr√ºfen:\nBeende die Aufteilung, wenn eines der folgenden Kriterien erf√ºllt ist:\n\nDie Region enth√§lt weniger als eine vorher festgelegte Mindestanzahl an Beobachtungen.\nEine vorher festgelegte maximale Baumtiefe wurde erreicht.\nDie Reduktion des Fehlers liegt unter einem vorher festgelegten Schwellenwert.\n\nTerminalknoten-Wert setzen:\nWeise jedem Blattknoten den Mittelwert der Zielvariablen \\(\\bar{y}_{R_i}\\:,i=1,\\cdots,I\\) der enthaltenen Datenpunkte zu.\n\n\n4.2.1 Rekursive und Greedy-Eigenschaft des Algorithmus\nDer rekursive Charakter des Algorithmus ist hierbei in Schritt 4. zu erkennen, da nach jedem Aufteilen des Datensatzes in zwei Teildatens√§tze diese Teildatens√§tze nach dem gleichen Prinzip wieder aufgeteilt werden.\nDer greedy Charakter des Algorithmus ist in Schritt 3. wiederzufinden. Da wir in jeder Iteration jenen Split w√§hlen, welcher den MSE in diesem Schritt minimiert, ist gew√§hrleistet, dass wir in jeder Iteration den bestm√∂glichen Split bilden. Dieses Vorgehen kann aber unter Umst√§nden zu einem nicht optimalen Baum f√ºhren: Angenommen, wir wollen eine Region \\(R_i\\) in zwei Teilregionen \\(R_{i,1}\\) und \\(R_{i,2}\\) aufteilen und w√§hlen hierf√ºr das optimale Paar \\((j^*,s^*)\\). Dann existiert keine weitere Kombination \\((\\tilde{j},\\tilde{s})\\), sodass \\(\\text{MSE}(\\tilde{j}, \\tilde{s})&lt; \\text{MSE}(j^*, s^*)\\). Falls wir also \\(R_i\\) bez√ºglich des Paares \\((j^*, s^*)\\) in \\(R_{i,1}\\) und \\(R_{i,2}\\) aufteilen und daraufhin \\(R_{i,1}\\) optimal in \\(R_{i+1,1}\\) und \\(R_{i+1,2}\\) aufteilen, dann kann es sein, dass unter der Verwendung von \\((\\tilde{j},\\tilde{s})\\) aus der letzten Iteration ein Split \\(R_{\\tilde{i}+1,1}\\) und \\(R_{\\tilde{i}+1,2}\\) h√§tte entstehen k√∂nnen, f√ºr den\n\\[\\begin{equation*}\n\\text{MSE}_{\\tilde{i}+1}(j, s) &lt; \\text{MSE}_{i+1}(j, s)\n\\end{equation*}\\] gilt.\n\n\n4.2.2 Stoppkriterien und Pruning\nIm 5. Schritt des Top-Down-Greedy-Algorithmus haben wir verschiedene Kriterien genannt, welche zum Stoppen des Algorithmus f√ºhren k√∂nnen.\nWir wollen in diesem Abschnitt die Stopp-Kriterien ein wenig genauer definieren und untersuchen.\n\n4.2.2.1 Mindestanzahl an Beobachtungen min_n\nDas Kriterium zur Mindestanzahl an Beobachtungen pr√ºft vor jedem weiteren Aufteilen des Featurespace, ob in den durch das Aufteilen resultierenden Teildatens√§tzen eine Mindestanzahl an Beobachtungen liegt. Ziel dieses Kriteriums ist es, zu vermeiden, dass sich das Baummodell zu stark an die Trainingsdaten anpasst. Die Vorhersagen \\(\\hat{y}\\) werden durch das Bilden des Durchschnitts in einem Blattknoten \\(R_m\\) generiert:\n\\[\n\\frac{1}{|R_m|}\\sum_{k:x_k\\in R_m}y_k\n\\] Falls also in \\(R_m\\) nur eine Observation vorhanden ist, kann das zu einer starken Verzerrung bei einer Evaluation mithilfe von Testdaten f√ºhren.\nWir bezeichnen nun mit min_n die Anzahl an Datenpunkten, welche mindestens in einem Teilgebiet liegen m√ºssen, sodass dieses geteilt wird. min_n muss also mindestens den Wert 2 besitzen. Jedoch stellt sich nat√ºrlich die Frage, welchen Wert wir f√ºr min_n beim Sch√§tzen eines Regressionsbaumes festlegen sollen. Nat√ºrlich l√§sst sich diese Frage wie immer nicht eindeutig beantworten. Wir werden sp√§ter sehen, dass R diesen Wert erst mal selbst bestimmt, wir diesen alternativ aber auch √ºbergeben k√∂nnen. Im Rahmen dieser Veranstaltung geben wir uns aber damit zufrieden, welchen Wert R f√ºr optimal h√§lt.\n\n\n4.2.2.2 Baumtiefe tree_depth\nDer Parameter tree_depth kann ebenso zum Stoppen des Algorithmus f√ºhren: Damit der Baum nicht unkontrolliert w√§chst (genauer gesagt an Tiefe gewinnt), stoppen wir den Algorithmus, sobald der Baum eine bestimmte Tiefe besitzt. Jedoch m√ºssen wir noch definieren, was mit Tiefe √ºberhaupt gemeint ist.\n\nDie L√§nge eines Pfades zwischen dem Wurzelknoten und einem beliebigen anderen Knoten \\(R_m\\) ist durch die Anzahl der Kanten definiert, an welchen wir entlanggehen m√ºssen, um den Knoten \\(R_m\\) zu erreichen.\nDie Tiefe eines Baums ist dann definiert durch den l√§ngsten Pfad vom Wurzelknoten zu einem anderen Knoten.\n\nSo hat der Baum in Figure¬†4.1 etwa die Tiefe 4, da wir entlang der rechten Seite des Baums vier Kanten entlanggehen m√ºssen, um entweder beim Knoten mit dem Wert \\(R_6\\) oder \\(R_7\\) zu landen. Beachte: Der l√§ngste Pfad muss also nicht eindeutig sein!\n\n\n4.2.2.3 Reduktion des Fehlers (cost_complexity)\nEin weiteres wichtiges Stoppkriterium ist das Stoppen durch unzureichende Reduzierung des Fehlers. Betrachte hierf√ºr folgenden Ausschnitt aus einem beliebigen Regressionsbaum:\n\n\n\n\n\nWir k√∂nnen f√ºr jeden der drei Knoten den MSE durch die Formel\n\\[\n  \\text{MSE}_{R_m} = \\frac{1}{|R_m|}\\sum_{k:x_k\\in R_m} (\\bar{y}_{R_m}-y_k)^2\n\\] berechnen, wobei \\(R_m\\) f√ºr den entsprechenden Knoten \\(K_{\\text{Eltern}},K_{\\text{Kind}_1},K_{\\text{Kind}_2}\\) steht.\nWir sind nun an der relativen Reduzierung des Fehlers durch den Split des Elternknotens in zwei Kindknoten interessiert. Diese √Ñnderung l√§sst sich mithilfe der \\(\\text{MSE}\\) Terme durch folgende Formel beschreiben:\n\\[\\begin{align}\\label{eq:improvement}\n  & \\frac{|K_{\\text{Eltern}}|\\cdot\\text{MSE}_{K_{\\text{Eltern}}} -(|K_{\\text{Kind}_1}|\\cdot\\text{MSE}_{K_{\\text{Kind}_1}}+|K_{\\text{Kind}_2}|\\cdot\\text{MSE}_{K_{\\text{Kind}_2}})}{|K_{\\text{Eltern}}|\\cdot\\text{MSE}_{K_{\\text{Eltern}}}}\\\\\n  &\\qquad = 1-\\frac{|K_{\\text{Kind}_1}|\\cdot\\text{MSE}_{K_{\\text{Kind}_1}}+|K_{\\text{Kind}_2}|\\cdot\\text{MSE}_{K_{\\text{Kind}_2}}}{|K_{\\text{Eltern}}|\\cdot\\text{MSE}_{K_{\\text{Eltern}}}}\\nonumber\n\\end{align}\\]\nWir nennen \\(\\eqref{eq:improvement}\\) auch den Improvement-Wert. Der Quotient beschreibt hierbei, wie gro√ü der Anteil der Fehlerreduktion im Vergleich zum Eltern-Fehler ist. Da die Summe der Teilfehler (\\(|K_{\\text{Kind}_1}|\\cdot\\text{MSE}_{K_{\\text{Kind}_1}}+|K_{\\text{Kind}_2}|\\cdot\\text{MSE}_{K_{\\text{Kind}_2}}\\)) immer kleiner gleich dem Fehler des Elternknoten ist (\\(|K_{\\text{Eltern}}|\\cdot\\text{MSE}_{K_{\\text{Eltern}}}\\)) ist der Quotient somit immer nicht-negativ und kleiner oder gleich \\(1\\). Falls der Z√§hler in \\(\\eqref{eq:improvement}\\) einen (im Vergleich zum Nenner) kleinen Wert annimmt, dann erreichen wir durch das Aufsplitten des Elternknotens eine gro√üe Verbesserung der Modellperformanz. Der Improvement-Wert kann somit Werte zwischen \\(0\\) und \\(1\\) annehmen, wobei ein Improvement-Wert von \\(1\\) impliziert, dass der Quotient den Wert \\(0\\) besitzt und damit auch die beiden \\(\\text{MSE}\\) der Kindknoten den Wert \\(0\\) annehmen. Falls der Improvement-Wert sehr klein ist, bedeutet das im Umkehrschluss, dass durch das Splitten des Elternknotens keine gro√üe Verringerung des Fehlers erreicht wird.\nWir k√∂nnen nun f√ºr den Improvement-Wert, √§hnlich wie er in \\(\\eqref{eq:improvement}\\) definiert ist, einen Schwellenwert cost_complexity festlegen. Falls ein Improvement-Wert f√ºr einen Knoten kleiner ist als der Schwellenwert cost_complexity wird dieser Knoten nicht weiter geteilt.\nDer Improvement-Wert bezieht sich hierbei aber im Nenner nicht auf den Elternknoten, sondern auf den Wurzelknoten (vgl. Section 4.4.4).\n\n\n4.2.2.4 Pruning\nZwar k√∂nnen alle genannten Kriterien verwendet werden, um das Wachsen des Baums fr√ºhzeitig zu stoppen (sog. Prepruning), allerdings ist das vorzeitige Stoppen durch eine zu kleine Ver√§nderung des Improvement-Wertes zu kurz gegriffen:\nFalls auf einen vermeintlich schlechten Split (Improvement-Wert &lt; cost_complexity) ein besserer folgt (Improvement-Wert &gt; cost_complexity), dann wird der bessere Split durch eine vorzeitige Beendigung nicht ber√ºcksichtigt. Deshalb wird f√ºr gew√∂hnlich das Aufspannen des Baums lediglich durch min_n und tree_depth vorzeitig gestoppt.\nNachdem ein Baum also auf Basis der anderen Kriterien aufgespannt wurde, wird das Cost-Complexity-Pruning (sog. Postpruning) angewendet, um diesen weit aufgespannten Baum zu trimmen:\nWir suchen unter der Verwendung des zuvor festgelegten Parameters cost_complexity= \\(\\alpha\\) denjenigen Teilbaum, welcher den Term\n\\[\\begin{equation}\\label{eq:cct}\n\\sum_{m=1}^{|T|}\\sum_{k:x_k\\in R_m}(y_k-\\bar{y_{R_m}})^2 + \\alpha|T|\n\\end{equation}\\]\nminimiert, wobei \\(|T|\\) f√ºr die Anzahl der Blattknoten steht.\nFalls wir f√ºr \\(\\alpha\\) einen sehr kleinen Wert w√§hlen, sorgt das also daf√ºr, dass der Term \\(\\alpha |T|\\) ebenso relativ wenig Einfluss auf den gesamten Term hat und somit ein Baum mit vielen Blattknoten aufgespannt wird. Falls wir f√ºr \\(\\alpha\\) auf der anderen Seite einen sehr gro√üen Wert w√§hlen, dann dominiert der Term \\(\\alpha |T|\\) die beiden Summanden, was dazu f√ºhren kann, dass ein Baum mit sehr wenigen Blattknoten gefunden wird.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regressionsb√§ume</span>"
    ]
  },
  {
    "objectID": "04_Regressions_Baeume.html#sec-vip",
    "href": "04_Regressions_Baeume.html#sec-vip",
    "title": "4¬† Regressionsb√§ume",
    "section": "4.3 Wichtigkeit der Variablen in Baummodellen",
    "text": "4.3 Wichtigkeit der Variablen in Baummodellen\nIm Vergleich zu linearen Modellen k√∂nnen wir die Koeffizienten der Regressionsb√§ume nicht mehr direkt interpretieren. Es gibt allerdings verschiedene Ans√§tze, welche die Einflussnahme der verschiedenen Variablen im Modell erm√∂glichen.\nEine Idee w√§re, zu z√§hlen, wie oft eine Variable zum Aufteilen des Feature Space verwendet wurde. Allerdings ist diese Idee ohne eine zus√§tzliche Gewichtung zu ungenau, da das wiederholte Verwenden einer Variable den Einfluss auf die Modellg√ºte nur unzureichend beschreibt.\n\nBeispiel 4.2 Angenommen, die Variable \\(X_j\\) wird im ersten Split verwendet, was zu einem sehr hohen Improvement-Wert f√ºhrt. Nun wird die Variable \\(X_j\\) f√ºr keinen weiteren Split verwendet. Falls dann eine Variable \\(X_{\\tilde{j}}\\) mit \\(\\tilde{j}\\neq j\\) existiert, welche in mehr als einem Split verwendet wird, aber der durch den Improvement-Wert gemessene Einfluss insgesamt kleiner ist als jener der Variable \\(X_j\\), dann w√ºrde ohne eine Gewichtung die Variable \\(X_{\\tilde{j}}\\) trotzdem als wichtiger gelten.\n\nWir sollten also neben der absoluten Anzahl der Variablenverwendung auch den Improvement-Wert des entsprechenden Splits ber√ºcksichtigen.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regressionsb√§ume</span>"
    ]
  },
  {
    "objectID": "04_Regressions_Baeume.html#regressionsb√§ume-in-r",
    "href": "04_Regressions_Baeume.html#regressionsb√§ume-in-r",
    "title": "4¬† Regressionsb√§ume",
    "section": "4.4 Regressionsb√§ume in R",
    "text": "4.4 Regressionsb√§ume in R\nNachdem wir uns in den vorherigen Abschnitten mit der Theorie und Darstellung von Entscheidungsb√§umen auseinandergesetzt haben, wollen wir in diesem Abschnitt lernen, wie man mit Entscheidungsb√§umen in R umgeht.\nHierf√ºr betrachten wir wieder den penguins-Datensatz aus den vorherigen √úbungen:\n\ndata_penguin &lt;- palmerpenguins::penguins \n\n\n4.4.1 Sch√§tzung der Parameter\nZiel ist es, die Variable body_mass_g mit einem Regressionsbaum zu modellieren. Wir k√∂nnen hierf√ºr die decision_tree()-Funktion der {tidymodels}-Bibliothek verwenden:\n\ntree_spec_penguins &lt;- decision_tree(mode = \"regression\")\n\n√Ñhnlich wie bei der linearen Regression k√∂nnen wir durch die decision_tree()-Funktion einen Entscheidungsbaum spezifizieren. Da Entscheidungsb√§ume sowohl f√ºr Klassifikations- als auch f√ºr Regressionsaufgaben verwendet werden k√∂nnen, m√ºssen wir mithilfe des mode-Arguments spezifizieren, f√ºr welche Art von Problem wir den Entscheidungsbaum verwenden wollen. Das Aufrufen des tree_spec_penguins Objekts zeigt dann die Spezifikation des Entscheidungsbaums:\n\ntree_spec_penguins\n\nDecision Tree Model Specification (regression)\n\nComputational engine: rpart \n\n\nUm die Parameter des Baumes zu sch√§tzen, verwenden wir wie auch bei der linearen Regression die fit()-Funktion:\n\ntree_fit_penguins &lt;- tree_spec_penguins %&gt;%\n  fit(data = data_penguin,\n      formula = body_mass_g ~.\n  )\n\nDer R√ºckgabewert der fit()-Funktion ist ein Decision-Tree-Objekt, welches neben der urspr√ºnglichen Spezifikation nun auch ein fit-Attribut enth√§lt. Dieses fit Attribut enth√§lt Informationen √ºber die Variablen und Schwellenwerte, welche in den jeweiligen Splits verwendet wurden. Durch das Aufrufen der extract_fit_engine()-Funktion k√∂nnen wir diese Informationen extrahieren:\n\ntree_fit_penguins %&gt;% extract_fit_engine()\n\nn=342 (2 Beobachtungen als fehlend gel√∂scht)\n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 342 219307700 4201.754  \n  2) species=Adelie,Chinstrap 219  41488530 3710.731  \n    4) sex=female 109   8586055 3420.642 *\n    5) sex=male 110  14640890 3998.182 *\n  3) species=Gentoo 123  31004250 5076.016  \n    6) sex=female 61   4853135 4670.492 *\n    7) sex=male 62   6250000 5475.000 *\n\n\nIn der ersten Zeile des Outputs wird angezeigt, wie viele Datenpunkte zum Sch√§tzen der Baumparameter verwendet wurden:\n\n\nn=342 (2 Beobachtungen als fehlend gel√∂scht)\n\n...\n\n\nDa in zwei Beobachtungen fehlende Werte vorhanden sind, hat die fit-Funktion diese entfernt und die Parameter auf Basis der verbleibenden 342 Beobachtungen gesch√§tzt.\nIn der n√§chsten Zeile ist eine Legende gegeben, welche Informationen √ºber den darauffolgenden Output enth√§lt:\n\n\n...\n\nnode), split, n, deviance, yval\n      * denotes terminal node\n...\n\n\n\nIn der ersten Spalte (node)) ist die Knotennummer gegeben. Der Knoten mit der Nummer 1) steht hierbei f√ºr den Wurzelknoten.\nIn der zweiten Spalte (split) ist die Splittingvariable \\(X_j\\) gegeben.\nIn der dritten Spalte (n) ist die Anzahl der Observationen in dem entsprechenden Knoten aufgef√ºhrt.\nIn der vierten Spalte (deviance) ist die quadrierte Abweichung der Observationen im Knoten von den tats√§chlichen Werten im Knoten aufgef√ºhrt. Diese berechnet sich durch \\(\\text{deviance} = \\text{MSE}\\cdot n\\).\nIn der f√ºnften Spalte (yval) ist der Sch√§tzwert \\(\\hat{y}\\) des entsprechenden Knotens gegeben.\nIn der letzten Spalte (* denotes terminal node) einer Zeile steht ein *, falls es sich in der entsprechenden Zeile um einen Blattknoten handelt.\n\n\n\n...\n\n1) root 342 219307700 4201.754  \n  2) species=Adelie,Chinstrap 219  41488530 3710.731  \n    4) sex=female 109   8586055 3420.642 *\n    5) sex=male 110  14640890 3998.182 *\n  3) species=Gentoo 123  31004250 5076.016  \n    6) sex=female 61   4853135 4670.492 *\n    7) sex=male 62   6250000 5475.000 *\n...\n\n\nDie verbleibenden Zeilen enthalten die zuvor beschriebenen Informationen der Legende. In jeder Zeile steht entsprechend ein Knoten des Baums, wobei die Blattknoten mit einem * in der letzten Zeile versehen sind.\nDie erste Splitting-Variable ist durch species gegeben. Falls eine Beobachtung die Auspr√§gungen species=Adelie oder species=Chinstrap enth√§lt (Knoten 2)), so werden diese zur Splitting-Bedingung sex=female (Knoten 4)) bzw. sex = male (Knoten 5)) geleitet. Hat eine Beobachtung die Auspr√§gung species=Gentoo (Knoten 3)), dann wird ebenso gepr√ºft, ob der entsprechende Pinguin das Geschlecht male (Knoten 7)) oder female (Knoten 6)) besitzt.\nEs wurden in dem gegebenen Baum also lediglich die Variablen species und sex verwendet, um den Baum aufzuspannen.\n\n\n4.4.2 Darstellung des Baums\nWir k√∂nnen den gesch√§tzten Baum aus Section 4.4.1 als Bin√§rbaum mithilfe der rpart.plot-Library darstellen2:\n\nlibrary(rpart.plot)\n\ntree_fit_penguins %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot()\n\n\n\n\n\n\n\n\nDas Vorgehen ist hierbei (dank der {tidymodels}) Syntax, √§hnlich wie beim Extrahieren der Modellparameter eines linearen Modells:\n\nIm ersten Schritt √ºbergeben wir das tree_fit_penguins-Objekt in die extract_fit_engine()-Funktion.\nDie extract_fit_engine()-Funktion extrahiert die Modellparameter, welche dann an die rpart.plot() √ºbergeben werden.\nDie rpart.plot()-Funktion visualisiert die Splittingvariablen und Schwellenwerte in einer bin√§ren Baumdarstellung.\n\nDer obere Wert in einem Knoten gibt dabei an, wie gro√ü der relative Anteil der Datenpunkte im Vergleich zu allen Datenpunkten auf dieser Ebene des Baums ist. Da auf der Ebene des Wurzelknotens nur ein Knoten ist, ist der relative Anteil in diesem entsprechend 100%. In der zweiten Ebene sind zwei Knoten, wobei der linke Knoten 64% und der rechte Knoten 34% der Datenpunkte enth√§lt. Der untere Wert in einem Knoten gibt den Sch√§tzwert der Zielvariable an. So ist etwa der Sch√§tzwert \\(\\hat{y}\\) im linken unteren Blattknoten durch \\(3421\\) g gegeben. Unter einem Knoten, welcher kein Blattknoten ist, steht eine Entscheidungsregel. Im Split nach dem Wurzelknoten wird zum Beispiel gepr√ºft, ob ein Pinguin zu den Spezies Adelie oder Chinstrap geh√∂rt.\n\n\n4.4.3 Wichtigkeit der Variablen\nDamit wir die Wichtigkeit der Variablen wie in Section 4.3 beschrieben berechnen k√∂nnen, verwenden wir das {vip}-Paket. vip steht in diesem Kontext f√ºr Variable Importance Plots.\n\nlibrary(vip)\n\nDie Funktion vi() berechnet f√ºr ein gegebenes Baummodell die Wichtigkeit der Variablen. Bei Entscheidungsb√§umen ergibt sich die Wichtigkeit einer Variablen aus der Summe der Improvement Werte aller Splits, in denen die Variable verwendet wurde.3\n\ntree_fit_penguins %&gt;%\n  extract_fit_engine() %&gt;%\n  vi()\n\n\n\n\n\n\n\n\n\nVariable\nImportance\n\n\n\n\nbill_depth_mm\n156249723\n\n\nflipper_length_mm\n156041084\n\n\nspecies\n146814916\n\n\nisland\n94470472\n\n\nbill_length_mm\n75382085\n\n\nsex\n37965241\n\n\n\n\n\n\n\nAuff√§llig ist hierbei, dass, obwohl der finale Baum nur die Variablen sex und island enth√§lt, die wichtigsten Variablen bill_depth_mm und flipper_length_mm sind. Das liegt daran, dass die Wichtigkeit der Variablen auf Basis des ungetrimmten Baumes berechnet wird. Dieses Ph√§nomen ist ein Indikator daf√ºr, dass die Standardwerte min_n, complexity_parameter und tree_depth in diesem Beispiel nicht optimal sein k√∂nnten, da die wichtigsten Variablen aus dem finalen Baum entfernt wurden.\nWir k√∂nnen die Improvement-Werte auch mithilfe der vip()-Funktion grafisch darstellen:\n\ntree_fit_penguins %&gt;%\n  extract_fit_engine() %&gt;%\n  vip()\n\n\n\n\n\n\n\n\n\n\n4.4.4 Berechnung der Improvement-Werte\nUm die Improvement-Werte bzw. die Reduktion der Fehler durch einen Split wie in Section 4.2.2.3 zu berechnen, k√∂nnen wir auf das cptable-Attribut des Baumes zugreifen. Diese k√∂nnen wir durch die Anwendung der pluck-Funktion extrahieren:\n\ntree_fit_penguins %&gt;%\n  extract_fit_engine() %&gt;%\n  pluck(\"cptable\")\n\n          CP nsplit rel error    xerror       xstd\n1 0.66944717      0 1.0000000 1.0037351 0.06129720\n2 0.09074516      1 0.3305528 0.3351579 0.02223837\n3 0.08326927      2 0.2398077 0.2533673 0.01869944\n4 0.01000000      3 0.1565384 0.1594846 0.01174359\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nAnstatt der extract_fit_engine- und pluck-Funktion kann man die Complexity-Parameter-Tabelle auch durch folgendes Code-Snippet extrahieren:\n\ntree_fit_penguins$fit$cptable\n\nAus Konsistenzgr√ºnden ist es aber empfehlenswert, die zuvor aufgef√ºhrte Tidy-Syntax zu verwenden.\n\n\nDie cptable gibt neben der Fehlerreduktion durch einen Split noch weitere Informationen, welche wir im Folgenden analysieren wollen.\n\nDie Spalte CP (Complexity Parameter) gibt einen Improvement-Wert des jeweiligen Splits an. In der ersten Zeile wird somit der Improvement-Wert durch das Teilen des Wurzelknotens anhand der Formel in \\(\\eqref{eq:improvement}\\) berechnet. Die darauffolgenden Improvement-Werte beziehen sich ebenfalls auf die Verbesserung bez√ºglich des Fehlers des Wurzelknotens, weshalb hier die Formel\n\\[\\begin{align}\\label{eq:lorem}\n\\frac{|K_{\\text{Eltern}}|\\cdot\\text{MSE}_{K_{\\text{Eltern}}} -(|K_{\\text{Kind}_1}|\\cdot\\text{MSE}_{K_{\\text{Kind}_1}}+|K_{\\text{Kind}_2}|\\cdot\\text{MSE}_{K_{\\text{Kind}_2}})}{|K_{\\text{Wurzel}}|\\cdot\\text{MSE}_{K_{\\text{Wurzel}}}}\n\\end{align}\\] verwendet wird.\nIn Gleichung \\(\\eqref{eq:lorem}\\) steht im Nenner nicht mehr \\(|K_{\\text{Eltern}}|\\cdot\\text{MSE}_{K_{\\text{Eltern}}}\\), sondern \\(|K_{\\text{Wurzel}}|\\cdot\\text{MSE}_{K_{\\text{Wurzel}}}\\). Der Wert CP beschreibt also den Anteil der Fehlerverbesserung, den ein Split erzeugt, relativ zum Gesamtfehler des Modells ohne jegliche Splits (also nur der Wurzelknoten). Um also etwa den CP-Wert in der zweiten Zeile zu berechnen, k√∂nnen wir die tabellarisch-hierarchische Darstellung\n\n\nn=342 (2 Beobachtungen als fehlend gel√∂scht)\n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 342 219307700 4201.754  \n  2) species=Adelie,Chinstrap 219  41488530 3710.731  \n    4) sex=female 109   8586055 3420.642 *\n    5) sex=male 110  14640890 3998.182 *\n  3) species=Gentoo 123  31004250 5076.016  \n    6) sex=female 61   4853135 4670.492 *\n    7) sex=male 62   6250000 5475.000 *\n\n\nverwenden. Der CP Wert berechnet sich dann durch die deviance Terme, welche durch das Teilen von Knoten 3) in die Knoten 6) und 7) entstehen:\n\\[\\begin{equation*}\n\\frac{31004250-(4853135+6250000)}{219307700} = 0.09074517\n\\end{equation*}\\]\nDer letzte Eintrag in der Spalte CP (0.01) ist der Default-Parameter der decision_tree-Funktion f√ºr das Argument cp (vgl. Section 4.2.2.3).\nDie Spalte nsplit beschreibt die Anzahl der Splits. Da der letzte Eintrag 3 ist, hei√üt das, dass der Feauturespace dreimal geteilt wurde.\nDie Spalte rel error beschreibt den relativen Fehler des Entscheidungsbaumes beim entsprechenden Split bez√ºglich des Wurzelknotens. Dieser kann mithilfe der Formel \\[\\begin{equation*}\n  \\text{rel error}_i = \\begin{cases}\n    1,\\qquad &\\text{falls } i=1\\\\\n    \\text{rel error}_{i-1}-\\text{CP}_{i-1},\\qquad &\\text{falls } i&gt;1\n  \\end{cases}\n\\end{equation*}\\] berechnet werden. Um also etwa den rel err f√ºr den letzten Eintrag \\((i=4)\\) berechnen wollen, dann lautet die Formel mit eingesetzten Zahlen hierf√ºr \\[\\begin{equation*}\n0.2398077-0.08326927 = 0.1565384\n\\end{equation*}\\]\nDie Spalten xerror und xstd beschreiben den rel error und dessen Standardabweichung, welche sich durch eine Cross-Validation ergeben haben.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regressionsb√§ume</span>"
    ]
  },
  {
    "objectID": "04_Regressions_Baeume.html#sec-erweiterungen",
    "href": "04_Regressions_Baeume.html#sec-erweiterungen",
    "title": "4¬† Regressionsb√§ume",
    "section": "4.5 Erweiterungen",
    "text": "4.5 Erweiterungen\nBaummodelle bilden neben linearen Modellen eine wichtige Grundlage f√ºr weiterf√ºhrende Modelle der Regression, welche zum Standardrepertoire eines Data Scientists geh√∂ren. Random Forests sind eine dieser Erweiterungen, welche wir in diesem Abschnitt diskutieren wollen.\nDie grundlegende Idee kann in zwei Teilen erkl√§rt werden.\n\n4.5.1 Erstellung eines Bootstrap-Datasets\nAngenommen, ein Feature-Datensatz besteht aus \\(K\\) Datenpunkten \\((x_1,\\ldots,x_K)\\) und \\(J\\) Variablen, also \\(x_k\\in\\mathbb{R}^J\\) f√ºr \\(k=1,\\ldots,K\\). Wir nehmen au√üerdem an, dass zu jedem Datenpunkt \\(x_k\\) mit \\(k=1,\\ldots,K\\) ein Zielwert \\(y_k\\) existiert. Der gesamte Datensatz ist somit durch \\(D=\\{(x_k,y_k)|x_k\\in\\mathbb{R}^J, y\\in\\mathbb{R}, k=1,\\ldots,K\\}\\) gegeben. Ein Bootstrap-Sample ist dann ein Datensatz \\(D^{(b)}\\) der Gr√∂√üe \\(K\\), welcher durch zuf√§lliges Ziehen mit Zur√ºcklegen von Samples aus dem Datensatz \\(D\\) generiert wird. Die Datenpunkte in \\(D^{(b)}\\) k√∂nnen also mehrfach im Bootstrap-Datensatz auftauchen!\n\n\n4.5.2 Trainieren der Baummodelle und Aggregation\nNachdem ein Bootstrap-Sample \\(D^{(b)}\\) erstellt wurde, sch√§tzen wir dann mithilfe dieses Samples einen Regressionsbaum. Beim Sch√§tzen des Baumes verwenden wir allerdings in jedem potenziellen Split nur mtry\\(&lt;J\\) zuf√§llig ausgew√§hlte Variablen, um stark korrelierte B√§ume zu vermeiden.\nDieses Vorgehen wiederholen wir dann \\(B&gt;1\\) mal, sodass wir schlussendlich \\(B\\) Regressionsb√§ume mithilfe von \\(B\\)¬†Bootstrap-Samples gesch√§tzt haben. Ein Random-Forest-Sch√§tzwert \\(\\hat{y}\\) f√ºr einen Datenpunkt \\(x\\) berechnet sich dann durch den durchschnittlichen Vorhersagewert aller \\(B\\) B√§ume:\n\\[\\begin{equation*}\n  \\hat{y} = \\frac{1}{B}\\sum_{b=1}^B \\hat{y}^{(b)},\n\\end{equation*}\\]\nwobei \\(\\hat{y}^{(b)}\\) der Vorhersagewert ist, generiert durch Baum \\(b\\).\n\n\n4.5.3 Random Forests in R\n√Ñhnlich wie Regressionsb√§ume k√∂nnen wir auch Random Forests mithilfe des {tidymodels}-Pakets sch√§tzen.\n\nset.seed(123)\n\nrf_spec_penguins &lt;- rand_forest(\n  mode = \"regression\",\n  trees = 100\n)\n\nrf_fit_penguins &lt;- rf_spec_penguins %&gt;%\n  fit(\n    data = data_penguin,\n    formula = body_mass_g ~.\n  )\n\nZuerst erstellen wir mithilfe der rand_forest()-Funktion eine Modellspezifikation. Als Argumente √ºbergeben wir wieder den Modus \"regression\" und zus√§tzlich das Argument trees = 100, welches spezifiziert, dass \\(B=100\\). Das Erstellen der Bootstrap-Sample, das Sch√§tzen der B√§ume und die Aggregation werden dann im zweiten Schritt durch die fit()-Funktion vorgenommen. Als Argumente f√ºr die fit-Funktion m√ºssen wir wie gew√∂hnlich die Daten und die Formel √ºbergeben. Dadurch, dass sowohl das Erstellen der Bootstrap-Samples als auch das Sch√§tzen der B√§ume eine Zufallskomponente enth√§lt, sollten wir durch die set.seed()-Funktion reproduzierbare Ergebnisse erm√∂glichen.\nNach dem Sch√§tzen des Random Forests k√∂nnen wir durch das Aufrufen des Objekts rf_fit_penguins einige Informationen √ºber den Random Forest extrahieren:\n\nrf_fit_penguins\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~100,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n\nType:                             Regression \nNumber of trees:                  100 \nSample size:                      333 \nNumber of independent variables:  7 \nMtry:                             2 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       87833.61 \nR squared (OOB):                  0.8645322 \n\n\n\nDer Parameter Mtry gibt an, wie viele Variablen zuf√§llig f√ºr einen Split verwendet werden.\nDer Parameter Target node size gibt an, wie viele Datenpunkte mindestens in einem Knoten sein m√ºssen, bevor dieser wieder geteilt wird.\nOOB prediction error (MSE) gibt an, wie gro√ü der MSE eines OOS-Datensatzes ist, welcher im Zuge der Sch√§tzung des Random Forests automatisch generiert wird.\nR squared (OOB) gibt f√ºr das gleiche OOS-Sample den berechneten \\(R^2\\) an.\n\nVorhersagen f√ºr die gegebenen Sample k√∂nnen wieder mithilfe der predict()-Funktion erstellt werden:4\n\ndata_penguin_cleaned &lt;- data_penguin %&gt;% na.omit()\n\nrf_fit_penguins %&gt;%\n  predict(data_penguin_cleaned)\n\n\n\n# A tibble: 5 √ó 1\n  .pred\n  &lt;dbl&gt;\n1 3761.\n2 3602.\n3 3435.\n4 3540.\n5 3891.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regressionsb√§ume</span>"
    ]
  },
  {
    "objectID": "04_Regressions_Baeume.html#√ºbungsaufgaben",
    "href": "04_Regressions_Baeume.html#√ºbungsaufgaben",
    "title": "4¬† Regressionsb√§ume",
    "section": "4.6 √úbungsaufgaben",
    "text": "4.6 √úbungsaufgaben\n\n4.6.1 Theorieaufgaben\n\nAufgabe 4.1 ¬†\n\nAngenommen, ein Regressionsbaum besteht lediglich aus dem Wurzelknoten. Seine Performance wird mit einem linearen Modell verglichen, welches lediglich den Parameter \\(b_0\\) enth√§lt. Ist zu erwarten, dass das Baummodell den Zusammenhang in den Daten besser erkl√§rt, oder das lineare Modell? Begr√ºnde die Antwort.\nAngenommen, es liegt ein Datensatz vor, welcher lediglich aus einer \\(X\\)- und \\(Y\\)-Variable besteht. Beim Betrachten einer Punktewolke f√§llt auf, dass der Zusammenhang zwischen den beiden Variablen schon sehr einer Geraden √§hnelt. Ist zu erwarten, dass im Vergleich zu einem einfachen linearen Regressionsmodell ein Regressionsbaum den Zusammenhang besser erkl√§rt?\n\n\n\nAufgabe 4.2 ¬†\n\nErkl√§re, warum Cost-Complexity-Tuning sowohl als Pre- als auch als Postpruning-Methode gesehen werden kann.\nWarum ist es keine gute Idee, Cost-Complexity-Pruning als Prepruning-Methode zu verwenden?\n\n\n\nAufgabe 4.3 Gegeben sei folgende Grafik, welche die Prognosewerte eines Regressionsbaums zeigt.\n\n\n\n\n\n\n\n\n\n\nBestimme, wie viele Blattknoten der abgebildete Baum enth√§lt.\nFormuliere die Funktionsvorschrift des Regressionsbaums als st√ºckweise definierte Funktion. Runde die visuell abgelesenen Werte auf eine Nachkommastelle.\nIst zu erwarten, dass bei einer einfachen linearen Regression eine √§hnlich gute Anpassung an die Daten stattfinden w√ºrde?\n\n\n\nAufgabe 4.4 ¬†\n\nGegeben seien folgende beiden Feature Spaces gesch√§tzter Baummodelle:\n\n\n\n\n\n\n\n\n\nSkizziere f√ºr beide Abbildungen den entsprechenden Bin√§rbaum unter der Angabe der Splitting-Variable, des Schwellenwertes und des Sch√§tzwertes in den Blattknoten. Hierbei m√ºssen nur die Sch√§tzwerte in den Blattknoten angegeben werden. Hinweis: In der unteren Abbildung sind die Sch√§tzwerte in den entsprechenden Ausschnitten in Rot abgebildet.\nGegeben seien folgende beiden B√§ume in Bin√§rbaumdarstellung:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkizziere f√ºr beide Abbildungen die entsprechenden Featurespaces unter der Angabe der Splitting-Variablen, des Schwellenwertes und des Sch√§tzwertes auf den Geraden bzw. in den Regionen.\n\n\n\nAufgabe 4.5 Gegeben seien folgende tabellarische/hierarchische Darstellungen eines Baums:\n\n\nn= 300 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 300 3516.58000 2.4522780  \n  2) x&gt;=3.461538 196   90.79800 0.2780154 *\n  3) x&lt; 3.461538 104  752.97340 6.5499260  \n    6) x&lt; 0.8528428 26   28.73169 2.9676420 *\n    7) x&gt;=0.8528428 78  279.37280 7.7440200 *\n\n\nAu√üerdem sei folgende CP Tabelle gegeben, welche durch den gleichen Baum erzeugt wurde:\n\n         CP nsplit rel error    xerror       xstd\n1         a      0         b 1.0080905 0.07565504\n2 0.1265061      1 0.2399409 0.2481654 0.02622409\n3 0.0500000      2         c 0.1164630 0.01292697\n\nBerechne die fehlenden Werte a, b und c.\n\nDer Wert \\(b\\) ist immer \\(1\\), da es sich um den relativen Fehler bez√ºglich des Wurzelknotens im Wurzelknoten selbst handelt.\nDa der Wert \\(a\\) der erste in der cp Tablle ist, berechnet sich dieser durch die Formel\n\\[\\begin{equation*}\n  \\text{CP}_1 = 1- \\frac{90.79800 + 752.97340}{3516.58000} = 0.7600591\n\\end{equation*}\\]\nDer Wert \\(c\\) berechnet sich durch die Formel \\[\\begin{equation*}\n  \\text{rel err}_3 = \\text{rel err}_2-\\text{CP}_2 = 0.2399409 ‚àí 0.1265061 = 0.1134348\n\\end{equation*}\\]\n\n\n\n\n4.6.2 R Aufgaben\nIn dieser √úbung verwenden wir den bereits bekannten hotel_rates-Datensatz aus den letzten √úbungen.\n\n4.6.2.1 Modellvergleiche\nVerwende die unten stehende Version des hotel_rates-Datensatzes f√ºr die folgenden Aufgaben.\n\nhotel_rates_filtered &lt;- hotel_rates %&gt;%\n mutate(\n   arrival_month = factor(month(arrival_date)),\n   num_guests = adults+children,\n   is_repeated_guest = factor(is_repeated_guest),\n   ) %&gt;%\n   select(num_guests,total_of_special_requests,\n          required_car_parking_spaces,arrival_month,\n          is_repeated_guest,lead_time,stays_in_week_nights,\n          avg_price_per_room,country) %&gt;%\n  na.omit()\n\n\nAufgabe 4.6 Generiere mithilfe des hotel_rates_filtered-Datensatzes einen \\(5\\)-fold Cross-Validation (CV)-Split. Verwende dabei das Seed 123.\n\n\nAufgabe 4.7 Verwende das CV-Objekt aus Aufgabe¬†4.6 um jeweils eine multiple lineare Regression, einen Regressionsbaum und einen Randomforest (bestehend aus 500 B√§umen) zu sch√§tzen, und vergleiche die OOS-Performance anhand der Metriken RMSE und \\(R^2\\). Die Zielvariable soll hierbei avg_price_per_room sein und die erkl√§renden Variablen alle verbleibenden. Setze beim Bearbeiten dieser Aufgabe ebenfalls einen Random Seed auf 123, um auch beim Random-Forest-Modell reproduzierbare Ergebnisse zu erzielen.\n\n\n\n4.6.2.2 Einfluss der Hyperparameter\nIn Section 4.2.2 haben wir gesehen, dass die Parameter cost_complexity (CP), tree_depth (Baumtiefe) und min_n (Mindestanzahl der Datenpunkte f√ºr einen weiteren Split) die Form des Baumes stark beeinflussen k√∂nnen.\nWir wollen deshalb in den folgenden √úbungsaufgaben diesen Einfluss untersuchen.\n\nhotel_rates_adj &lt;- hotel_rates %&gt;%\n mutate(\n   arrival_month = factor(month(arrival_date)),\n   num_guests = adults+children,\n   is_repeated_guest = factor(is_repeated_guest),\n   ) %&gt;%\n  select(-c(adults,children,agent))\n\n\n\nAufgabe 4.8 Erstelle mithilfe des oben eingef√ºhrten Datensatzes hotel_rates_adj einen Trainings- und Test-Split im Verh√§ltnis 5:1 unter Verwendung des Seeds 123.\n\n\nAufgabe 4.9 In dieser Aufgabe wollen wir verschiedene Versionen eines Regressionsbaums trainieren. Der Fokus soll hierbei auf der Wahl der verschiedenen Hyperparameter und deren Einfluss auf die Modellperformanz auf den Testdaten liegen.\nVerwende f√ºr jeden der folgenden beschriebenen Regressionsb√§ume die Formel\n\nformula_hr &lt;- avg_price_per_room~.\n\nHinweis: Man kann in der fit-Funktion dem Argument formula damit den Wert formula_hr zuweisen und muss nicht immer die gleiche Spezifikation ausschreiben.\n\n\nVerwende den Trainings-Split aus Aufgabe¬†4.8 um einen Regressionsbaum mithilfe der Formel formula_hr zu trainieren. Bei der Modellspezifikation sollen alle Hyperparameter des Baumes auf deren Standardeinstellung gelassen werden.\nStelle den Baum grafisch dar (rpart.plot()) und beantworte die folgenden Fragen:\n\nWelcher Blattknoten enth√§lt die kleinste bzw. gr√∂√üte Anzahl an Datenpunkten?\nWelcher ist der gr√∂√üte bzw. kleinste Sch√§tzwert im dargestellten Baum (bezogen auf alle Knoten)?\nWelche Tiefe hat der resultierende Baum?\n\nBestimme mithilfe der vip()-Funktion welche Variable im Modell den gr√∂√üten und kleinsten Importance-Wert besitzt.\nBerechne auf Basis der Testdaten den RMSE und \\(R^2\\).\n\nVerwende das Trainingsset aus Aufgabe¬†4.8 um einen weiteren Regressionsbaum zu trainieren.\n\nVerwende hierbei die Hyperparameter:\n\ntree_params &lt;- tibble(\n cost_complexity = 0.00000000915,\n tree_depth = 15,\n min_n = 32\n)\n\nWelche Tiefe besitzt dieser neue Baum?\nBerechne f√ºr diesen Baum ebenso auf den Testdaten den RMSE und \\(R^2\\) und vergleiche diese mit der Testperformanz des Baumes aus Teilaufgabe 1. iv.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regressionsb√§ume</span>"
    ]
  },
  {
    "objectID": "04_Regressions_Baeume.html#l√∂sungen",
    "href": "04_Regressions_Baeume.html#l√∂sungen",
    "title": "4¬† Regressionsb√§ume",
    "section": "4.7 L√∂sungen",
    "text": "4.7 L√∂sungen\n\nSolution 4.1 (Aufgabe¬†4.1). \n\nEin lineares Modell, welches lediglich den Parameter \\(b_0\\) enth√§lt, besitzt den Sch√§tzwert \\(\\hat{y_k} = \\hat{b_0} = \\bar{y} = \\frac{1}{K}\\sum_{k=1}^{K}y_k\\) f√ºr alle \\(k\\). Ein Regressionsbaum, welcher lediglich aus dem Wurzelknoten besteht, besitzt ebenso den Sch√§tzwert \\(\\hat{y_k} = \\bar{y}_{R_0} = \\frac{1}{|R_0|}\\sum_{k:x_k \\in R_0}^{K}y_k\\). Die beiden Modelle werden in diesem Fall also gleich gut (bzw. gleich schlecht!) die Zusammenh√§nge in den Daten erkl√§ren.\nFalls der Datensatz bereits sehr einer Variablen √§hnelt, dann ist zu erwarten, dass das lineare Modell den Zusammenhang besser erkl√§rt. Das liegt daran, dass ein Baummodell eine st√ºckweise konstante Funktion ist und deshalb die Steigung (falls \\(b_1\\neq 0\\)) so gut abbilden kann wie eine Gerade selbst.\n\n\n\nSolution 4.2 (Aufgabe¬†4.2). \n\nCost-Complexity-Pruning kann als Postpruning-Methode gesehen werden, da das Trimmen des Baumes anhand des CP-Wertes nach dem vollst√§ndigen Aufspannen stattfindet. Es kann auf der anderen Seite allerdings auch als Prepruning-Kriterium aufgefasst werden, welches einen Baum daran hindert, einen Knoten weiter zu splitten, falls der Improvement-Wert zu klein ist.\nWie in Section 4.2.2.4 erl√§utert, kann das fr√ºhzeitige Beenden des Aufspannens eines Baumes dazu f√ºhren, dass ein potenziell besserer Split nachfolgend nicht mehr durchgef√ºhrt wird.\n\n\n\nSolution 4.3 (Aufgabe¬†4.3). \n\nDer Baum enth√§lt sieben Blattknoten, was an den sieben horizontalen Geraden zu erkennen ist.\nDie Funktionsvorschrift ist gegeben durch \\[\\begin{equation*}\n  f(x) = \\begin{cases}\n    0.25,\\quad &\\text{ falls } x&lt;3.1\\\\\n    3.6,\\quad  &\\text{ falls } 3.1\\leq x &lt; 4\\\\\n    7,\\quad    &\\text{ falls } 4\\leq x &lt; 4.3\\\\\n    9.2,\\quad  &\\text{ falls } 4.3\\leq x &lt; 5.75\\\\\n    5.5,\\quad  &\\text{ falls } 5.75\\leq x &lt; 6.4\\\\\n    2.3,\\quad  &\\text{ falls } 6.4\\leq x &lt; 7.2\\\\\n    0.2,\\quad  &\\text{ falls } 7.2\\leq x \\\\\n  \\end{cases}\n\\end{equation*}\\]\nEs ist zu erwarten, dass eine einfache lineare Regression sich nur schlechter an die Daten anpassen kann, da der Zusammenhang der Daten stark nichtlinear ist.\n\n\n\nSolution 4.4 (Aufgabe¬†4.4). \n\nDie entsprechenden Baumdiagramme sind gegeben durch:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie beiden Feature Spaces sind gegeben durch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution 4.5 (Aufgabe¬†4.5). Die vollst√§ndige CP-Tabelle ist durch\n\n\n         CP nsplit rel error    xerror       xstd\n1 0.7600591      0 1.0000000 1.0049032 0.07536700\n2 0.1265061      1 0.2399409 0.2491316 0.02623625\n3 0.0500000      2 0.1134348 0.1178125 0.01326705\n\n\ngegeben.\nAlso \\(a = 0.7600591\\), \\(b = 1\\) und \\(c = 0.1134348\\).\n\n\nSolution 4.6 (Aufgabe¬†4.6). \n\nset.seed(123)\nfolds_hotel_rate &lt;- vfold_cv(hotel_rates_filtered,5)\n\n\n\nSolution 4.7 (Aufgabe¬†4.7). \n\nset.seed(123)\n\nmulti_metric &lt;- metric_set(rmse,rsq)\n\nmlr_fit_hr &lt;- linear_reg() %&gt;%\n  fit_resamples(\n    resamples = folds_hotel_rate,\n    preprocessor = avg_price_per_room ~.,\n    metrics = multi_metric\n  )\n\ntree_fit_hr &lt;- decision_tree(\n  mode = \"regression\"\n) %&gt;%\n  fit_resamples(\n    resamples = folds_hotel_rate,\n    preprocessor = avg_price_per_room ~.,\n    metrics = multi_metric\n  )\n\nrf_fit_hr &lt;- rand_forest(\n  mode = \"regression\",\n  trees = 500\n) %&gt;%\n  fit_resamples(\n    resamples = folds_hotel_rate,\n    preprocessor = avg_price_per_room ~.,\n    metrics = multi_metric\n  )\n\n\ncv_comparison &lt;- rbind(\n  mlr_fit_hr %&gt;%\n    collect_metrics() %&gt;%\n    mutate(model = \"mlr\") %&gt;%\n    select(-.config),\n  tree_fit_hr %&gt;%\n    collect_metrics() %&gt;%\n    mutate(model = \"tree\") %&gt;%\n    select(-.config),\n  rf_fit_hr %&gt;%\n    collect_metrics() %&gt;%\n    mutate(model = \"random forest\") %&gt;%\n    select(-.config) \n)\n\ncv_comparison\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\nmodel\n\n\n\n\nrmse\nstandard\n33.33\n5.00\n0.33\nmlr\n\n\nrsq\nstandard\n0.74\n5.00\n0.01\nmlr\n\n\nrmse\nstandard\n37.06\n5.00\n0.36\ntree\n\n\nrsq\nstandard\n0.68\n5.00\n0.01\ntree\n\n\nrmse\nstandard\n29.61\n5.00\n0.16\nrandom forest\n\n\nrsq\nstandard\n0.81\n5.00\n0.00\nrandom forest\n\n\n\n\n\n\n\nDer Tabelle ist zu entnehmen, dass das Random-Forest-Modell bez√ºglich beider Metriken die beste Performance hat. Auf den Test Folds betr√§gt bei diesem Modell der durchschnittliche \\(R^2\\) √ºber \\(80\\%\\). Wir k√∂nnen also mithilfe dieses Modells mehr als \\(80\\%\\) der Varianz in unseren Testdaten erkl√§ren.\nDas einfache Baummodell schneidet in diesem Vergleich am schlechtesten ab, was vor allem daran liegen k√∂nnte, dass wir die Parameter complexity_parameter, min_n und tree_depth nicht weiter angepasst haben.\nDurch die Beschr√§nkung der Linearit√§tsannahme auf die Koeffizienten und die Variablen ist das MLR-Modell f√ºr diesen Datensatz vermutlich zu einfach. Eine Idee w√§re an dieser Stelle, die einzelnen Variablen genauer zu untersuchen und durch die Einf√ºhrung von nichtlinearen Variablentransformationen zu verbessern.\n\n\nSolution 4.8 (Aufgabe¬†4.8). \n\nset.seed(123)\n\nhotel_rates_split &lt;- initial_split(hotel_rates_adj, prop = 5/6)\ndata_train &lt;- training(hotel_rates_split)\ndata_test &lt;- testing(hotel_rates_split)\n\n\n\nSolution 4.9 (Aufgabe¬†4.9). In dieser Aufgabe wollen wir verschiedene Versionen eines Regressionsbaums trainieren. Der Fokus soll hierbei auf der Wahl der verschiedenen Hyperparameter und deren Einfluss auf die Modellperformanz auf den Testdaten liegen.\nVerwende f√ºr jeden der folgenden beschriebenen Regressionsb√§ume die Formel\n\nformula_hr &lt;- avg_price_per_room~.\n\nHinweis: Man kann in der fit-Funktion dem Argument formula damit den Wert formula_hr zuweisen und muss nicht immer die gleiche Spezifikation ausschreiben.\n\n\n\n\ntree_fit_hr_vanilla &lt;- decision_tree(mode = \"regression\") %&gt;%\n  fit(formula = formula_hr,\n      data = data_train)\n\ntree_fit_hr_vanilla %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot()\n\n\n\n\n\n\n\n\n\nDie meisten Datenpunkte enth√§lt der ganz linke Wurzelknoten (27%) und der ganz rechte Knoten am wenigsten (1%).\nDer gr√∂√üte Sch√§tzwert ist durch \\(280\\) im ganz rechten Knoten gegeben und der kleinste Sch√§tzwert durch \\(50\\) im ganz linken Knoten.\nDer Baum hat die Tiefe 5.\n\n\n\ntree_fit_hr_vanilla %&gt;% augment(data_test) %&gt;%\n  yardstick::rmse(.pred,avg_price_per_room)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        32.0\n\ntree_fit_hr_vanilla %&gt;% augment(data_test) %&gt;%\n  yardstick::rsq(.pred,avg_price_per_room)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.767\n\n\nDurch das √úbergeben des tree_fit_hr-Objektes an die vip()-Funktion k√∂nnen wir die Wichtigkeitswerte der Variablen ablesen:\n\ntree_fit_hr_vanilla %&gt;% vip(num_features = 17)\n\n\n\n\n\n\n\n\nFalls wir allerdings nicht den Parameter num_features spezifizieren, wird nur der Wichtigkeitswert der wichtigsten \\(10\\) Feature ausgegeben. Wir m√ºssen diesen Wert deshalb auf die Anzahl der im Baum verwendeten Feature (17) setzen, um auch Informationen √ºber alle 17 Feature zu erhalten. Die Variable historical_adr besitzt demnach den gr√∂√üten und die Variable new_years den kleinsten Improvement-Wert.\n\n\n\n\ntree_fit_hr_tuned &lt;- decision_tree(mode = \"regression\",\n                         cost_complexity = 0.00000000915,\n                         tree_depth = 15,\n                         min_n = 32\n                         ) %&gt;%\n                         fit(formula = formula_hr,\n                             data = data_train\n                             )\n\nUm die Tiefe des Baumes zu bestimmen, kann man entweder wie zuvor den Baum grafisch darstellen und den l√§ngsten Pfad zwischen Wurzelknoten und Blattknoten bestimmen, oder man berechnet die Tiefe.\nDer Baum tree_fit_hr_tuned l√§sst sich mithilfe der rpart.plot() Funktion darstellen:\n\ntree_fit_hr_tuned %&gt;%\n    extract_fit_engine() %&gt;%\n    rpart.plot(clip.facs = TRUE,\n               faclen = 3,\n               cex = 1e-5)\n\n\n\n\n\n\n\n\nDurch das Setzen der Parameter clip.facs = TRUE, faclen = 3 und cex = 1e-5 erreichen wir, dass sich der sehr tiefe Baum halbwegs gut darstellen l√§sst. Wir k√∂nnen dann die Tiefe des Baumes Grafisch bestimmen. Diese ist durch 15 gegeben.\nDamit wir auch die Teife von B√§umen bestimmen k√∂nnen, welche sich nur sehr schlecht grafisch darstellen lassen, leiten wir im folgenden eine geschlossene Formel f√ºr die Tiefe des Baumes her.\nFolgender Abschnitt ist nicht relevant f√ºr die Klausur, sondern f√ºr interessierte Leserinnen und Leser gedacht.\nJeder Knoten eines Baumes in R besitzt eine Identifikationsnummer, welche sich durch die Folge\n\\[\\begin{equation}\na_{n+1} = \\begin{cases}\n1\\qquad &\\text{ falls } n=0 \\text{ Wurzelknoten }\\\\\na_n\\cdot 2 &\\text{ falls } n\\geq 1 \\text{ und } a_{n+1} \\text{ linker Kindknoten von } a_n\\\\\na_n\\cdot 2+1 &\\text{ falls } n\\geq 1 \\text{ und } a_{n+1} \\text{ rechter Kindknoten von } a_n\n\\end{cases}\n\\end{equation}\\] bestimmen l√§sst. Diese rekursive Folge l√§sst sich dann zum Beispiel f√ºr alle rechten Knoten darstellen als \\(a_n = 2^{n+1}-1\\). \\(n\\) steht hierbei f√ºr die \\(n\\)-te Ebene des Baumes. Somit sind die IDs f√ºr die Blattknoten durch die tiefste Ebene \\(N\\) gegeben und besitzen Werte zwischen \\(2^n\\) und \\(2^{n+1}-1\\). Um die Tiefe des Baumes also rechnerisch zu bestimmen, m√ºssen wir zuerst den Knoten mit der h√∂chsten ID finden:\n\ntree_fit_hr_tuned %&gt;%\n  extract_fit_engine() %&gt;%\n  pluck(\"frame\") %&gt;%\n  row.names() %&gt;%\n  as.integer() %&gt;%\n  max()         \n\n[1] 62055\n\n\nDas Attribut \"frame\" enth√§lt Informationen √ºber alle Knoten, welche im finalen Baum verwendet wurden. Die Zeilennamen enthalten etwa die Knoten-ID, welche wir durch die rekursive Folge bestimmen k√∂nnen. Durch das Anwenden der row.names()-Funktion extrahieren wir also aus dem Objekt \"frame\" alle Knoten-IDs. Damit wir den maximalen Wert finden k√∂nnen, m√ºssen wir die Knoten-IDs in den Datentyp &lt;int&gt; umwandeln. Mithilfe der max()-Funktion k√∂nnen wir dann den gr√∂√üten Wert extrahieren. Um nun die Ebene des tiefsten Knotens (der Knoten mit der h√∂chsten ID) zu bestimmen, k√∂nnen wir folgende Umformung anwenden:\n\\[\\begin{align*}\na_n  &= 2^{n+1}-1 \\iff n = \\lceil\\log_2(a_{n+1})-1\\rceil\n\\end{align*}\\]\nSomit gilt f√ºr die Tiefe des Baumes\n\\[\\begin{equation*}\n  n = \\lceil\\log_2(62055)-1\\rceil = 15,\n\\end{equation*}\\]\nwobei \\(\\lceil \\cdot \\rceil\\) obere Gau√üklammern darstellen.5\n\n\ntree_fit_hr_tuned %&gt;%\n  augment(data_test) %&gt;%\n  yardstick::rmse(.pred,avg_price_per_room)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        20.5\n\n\nIm Vergleich zum Modell mit unangepassten Hyperparametern ist das Modell aus dieser Teilaufgabe besser, da der Test-RMSE nur 20.5 betr√§gt (32 f√ºr das Modell aus Teilaufgabe 4.9.1. ii.)\n\ntree_fit_hr_tuned %&gt;%\n  augment(data_test) %&gt;%\n  yardstick::rsq(.pred,avg_price_per_room)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.904\n\n\nEbenso ist der \\(R^2\\) im Vergleich zum vorherigen Modell (0.767) besser. Wir k√∂nnen also schlussfolgern, dass das Finden von guten Hyperparametern die Modellperformanz stark verbessern kann.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regressionsb√§ume</span>"
    ]
  },
  {
    "objectID": "04_Regressions_Baeume.html#footnotes",
    "href": "04_Regressions_Baeume.html#footnotes",
    "title": "4¬† Regressionsb√§ume",
    "section": "",
    "text": "Diese sind durch die Mittelwerte aller benachbarten Punkte gegeben.‚Ü©Ô∏é\nDiese muss eventuell zuerst mit dem Befehl install.packages(\"rpart.plot\") installiert werden‚Ü©Ô∏é\nStreng genommen wird auch noch der Improvement Wert der sog. Surrogate-Splits verwendet. F√ºr Details siehe rpart Dokumentation‚Ü©Ô∏é\nExemplarisch wurden hier nur die ersten 5 Werte ausgegeben.‚Ü©Ô∏é\nF√ºr \\(x\\in (n,n+1),\\: n\\in \\mathbb{N}\\) gilt \\(\\lceil x \\rceil = n+1\\).‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regressionsb√§ume</span>"
    ]
  },
  {
    "objectID": "05_Neuronale_Netze.html",
    "href": "05_Neuronale_Netze.html",
    "title": "5¬† Neuronale Netze",
    "section": "",
    "text": "5.1 Einf√ºhrung\nNeuronale Netze werden h√§ufig mit der Funktionsweise des menschlichen Gehirns verglichen, auch wenn dieser Zusammenhang selbst bei den einfachsten Konzepten an seine Grenzen st√∂√üt (vgl. Quanta Magazin: AI is nothing like a brain and that‚Äôs okay). Auch wenn neuronale Netze bereits seit den 1940er Jahren existieren, wurde deren Anwendung wie in Large Language Models (ChatGPT, LeChat¬†‚Ä¶) erst in den vergangenen Jahren f√ºr die breite Gesellschaft zug√§nglich. Damit wir moderne Architekturen wie Transformer verstehen k√∂nnen, auf denen zum Beispiel auch Large Language Models basieren, d√ºrfen wir die Grundlagen, gelegt durch einfache neuronale Netze, nicht vernachl√§ssigen.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Neuronale Netze</span>"
    ]
  },
  {
    "objectID": "05_Neuronale_Netze.html#einf√ºhrung",
    "href": "05_Neuronale_Netze.html#einf√ºhrung",
    "title": "5¬† Neuronale Netze",
    "section": "",
    "text": "5.1.1 Anwendung von Neuronalen Netzen in der Regression\nNeuronale Netze werden in ihrer einfachen Form, wie wir sie in dieser Veranstaltung behandeln, vorwiegend f√ºr Regressions- und Klassifikationsaufgaben verwendet. Wir befinden uns also bei diesen Anwendungsf√§llen im Bereich des Supervised Learnings, da wir die Zielvariable kennen.\nBei einer Regressionsaufgabe ist wie immer das Ziel, eine Funktion \\(f:\\mathbb{R}^J\\to\\mathbb{R}\\) zu finden, welche den bestm√∂glichen Zusammenhang \\(y = f(x)+\\varepsilon\\) erkl√§rt. Im Falle eines Neuronalen Netzes beschreiben wir die Funktion \\(f\\) allgemein durch Weights (Gewichte) und Biases (Konstanten), welche den R√ºckgabewert beeinflussen.\n\n\n5.1.2 Aufbau eines Neuronalen Netzes\nIm folgenden Abschnitt werden wir die grundlegende Idee eines einfachen neuronalen Netzes sequenziell erweitern, bis wir bei sogenannten Feed-Forward Deep Neural Networks (FFDNN) angekommen sind.\nFeed-Forward-neuronale Netze (FFNN) bestehen aus Neuronen, welche in einer festen Reihenfolge angeordnet sind und Daten vorw√§rts verarbeiten.\nSo k√∂nnen wir ein FFNN, bestehend aus einem einzelnen Neuron, grafisch wie folgt darstellen:\n\n\n\n\n\n\nFigure¬†5.1\n\n\n\nHierbei steht\n\n\\(I_1\\) f√ºr den Input\n\\(B_1\\) f√ºr den Bias (Konstante)\n\\(O_1\\) f√ºr den Output\n\\(\\omega_{I_1,O_1},\\: \\omega_{B_1,O_1}\\) f√ºr den Wert des Gewichts und des Bias\n\\(\\sigma\\) f√ºr eine Aktivierungsfunktion.\n\nDie Pfeile in der obigen Grafik deuten die Richtung des Informationsflusses an. Der Input \\(I_1\\) wird durch den Parameter \\(\\omega_{I_1,O_1}\\) gewichtet. Neben dem Input wird ein konstanter Term (Bias) \\(\\omega_{B_1,O_1}\\) dem gewichteten Input hinzugef√ºgt. Die beiden Terme werden durch die Summation aggregiert und anschlie√üend durch eine (nicht-)lineare Aktivierungsfunktion transformiert. Diese Transformation der aggregierten Summation ist dann der Output des Modells.\nMathematisch l√§sst sich dieses einfache FFNN also durch\n\\[\\begin{equation}\\label{eq:nn_anal}\nf(x) = \\sigma(\\omega_{I_1,O_1}\\cdot x + \\omega_{B_1,O_1})\n\\end{equation}\\]\nbeschreiben.\nBeispiele f√ºr Aktivierungsfunktionen sind:\n\n\n\nTable¬†5.1: √úbersicht √ºber verschiedene Aktivierungsfunktionen\n\n\n\n\n\n\n\n\n\n\nAktivierungsfunktion\nMathematische Definition\nBildbereich\n\n\n\n\nSigmoid\n\\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)\n\\((0, 1)\\)\n\n\nTanh\n\\(\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)\n\\((-1, 1)\\)\n\n\nReLU\n\\(\\text{ReLU}(x) = \\max(0, x)\\)\n\\([0, \\infty)\\)\n\n\nLeaky ReLU\n\\(\\text{LeakyReLU}(x) = \\max(\\alpha x, x)\\)\n\\((-\\infty, \\infty),\\: (\\alpha &gt; 0)\\)\n\n\n\n\n\n\n\nBeispiel 5.1 Wir k√∂nnen in R ein einfaches FFNN wie folgt darstellen. Die Notation ist hierbei konsistent mit der aus Figure¬†5.1.\n\nReLU &lt;- function(x){\n  max(x,0)\n}\n\ni1 &lt;- 2\nb1 &lt;- 1\n\nw_i1o1 &lt;- 1.5\nw_b1o1 &lt;- 2.5\n\n(o1 &lt;- ReLU(w_i1o1*i1+w_b1o1*b1))\n\n[1] 5.5\n\nw_i1o1 &lt;- -1.5\nw_b1o1 &lt;- -2.5\n\n(o1 &lt;- ReLU(w_i1o1*i1+w_b1o1*b1))\n\n[1] 0\n\n\n\nWir k√∂nnen die Idee aus Figure¬†5.1 leicht erweitern, indem wir einzelne Neuronen aneinanderketten. So wird der Output \\(O\\) des ersten Neurons als Input \\(I\\) des n√§chsten Neurons verwendet. Diese Verkettungen k√∂nnen beliebig oft wiederholt werden, was dann einem tiefen Neuronalen Netz entspricht. Wir nennen dann die Glieder dieser Verkettung einzelne Layer.\n\n\n\n\n\n\nFigure¬†5.2\n\n\n\nFigure¬†5.2 zeigt exemplarisch ein FFNN mit \\(K\\) hidden Layern. Hidden Layer sind jene Neuronen in der Verkettung, welche zwischen der Input- und der Outputschicht liegen.\n\nBeispiel 5.2 Modellierung eines FFNN mit einem Hidden Layer.\n\n#Input\ni1 &lt;- 2\n\n#Bias\nb1 &lt;- 1\nb2 &lt;- 1\nb3 &lt;- 1\n\n# Weight und Bias fuer die erste Layer (Input)\nw_i1h11 &lt;- 1.1\nw_b1h11 &lt;- 2.1\n\n# Weight und Bias fuer die zweite Layer (Hidden)\n\nw_h11h21 &lt;- 1.2\nw_b1h21 &lt;- 2.2\n\n# Weight und Bias fuer die dritte Layer (Output)\n\nw_h21o1 &lt;- 1.3\nw_b2o1 &lt;- 2.3\n\n\n# Berechnung des Ouputs der ersten Layer\n\nh11 &lt;- ReLU(w_i1h11*i1+\n           w_b1h11*b1)\n\n# Berechnung des Ouputs der zweiten Layer\n\nh21 &lt;- ReLU(w_h11h21*h11+\n           w_b1h21*b2)\n\n# Berechnung des Ouputs der finalen Layer\n\n(o1 &lt;- ReLU(w_h21o1*h21+\n           w_b2o1*b3)\n)\n\n[1] 11.868\n\n\n\nDie bisher betrachteten Architekturen bestanden lediglich aus einem Input und die Verkettungen der Neuronen waren ebenso eindimensional. Mehrdimensionale Inputs k√∂nnen ganz einfach durch das Hinzuf√ºgen von weiteren Inputneuronen modelliert werden, wobei jeder Input \\(I_1,\\ldots,I_J\\) entsprechend ein eigenes Gewicht erh√§lt und diese in der ersten Schicht aggregiert werden:\n\\[\\begin{equation*}\nH_{1,1} = \\sigma\\left(B_1\\cdot \\omega_{B_1,H_{1,1}}+\\sum_{j=1}^{J}\\omega_{I_j,H_{1,1}}\\cdot I_j\\right)\n\\end{equation*}\\]\nAllerdings gehen in den darauffolgenden Schichten wom√∂glich viele Informationen durch diese einfache Aggregation verloren. Wir k√∂nnen in den verschiedenen Schichten eines FFNN mehrere Neuronen auch √ºbereinander darstellen und durch zus√§tzliche Gewichte mit den Neuronen aus der vorherigen Schicht verkn√ºpfen. Den allgemeinen Fall k√∂nnen wir dann wie in der folgenden Grafik aufgezeigt darstellen. Hinweis: In diesem Modell ist der Output \\(N_{K+1}\\)-dimensional. Bei den Regressionsproblemen, welche wir betrachten, ist der Output allerdings immer eindimensional.\n\n\n\n\n\n\nFigure¬†5.3\n\n\n\nDas oben dargestellte FFNN besteht also aus \\(J\\) Inputneuronen, \\(K\\) Hidden-Layern, welche die Dimensionen \\(N_1,\\ldots,N_K\\) besitzen, und einem Output-Layer der Dimension \\(N_{K+1}\\). Beginnend bei der Input-Layer kommuniziert jedes Neuron √ºber ein Gewicht mit jedem Neuron des ersten Hidden-Layers. Durch die Vielzahl an Verbindungen zwischen den einzelnen Schichten k√∂nnen wir somit auch sehr komplizierte Zusammenh√§nge gut modellieren. Neben der Verwendung von mehreren Schichten und vielen Neuronen wirkt sich allerdings vor allem die Aktivierungsfunktion auf das Ergebnis aus. Falls eine lineare Aktivierungsfunktion zwischen den einzelnen Schichten vorliegt, dann zerf√§llt das FFNN in eine multiple lineare Regression.\nEine Version des Modells, wie das in Figure¬†5.3 dargestellt ist, k√∂nnen wir in R wie folgt modellieren:\n\nBeispiel 5.3 Gegeben sei folgende Netzarchitektur:\n\n\n\n\n\n\nFigure¬†5.4\n\n\n\nDann k√∂nnen wir diese in R durch folgende Funktion beschreiben:\n\nnn &lt;- function(x,w1,w2,w3){\n  h1 &lt;- pmax(w1*x+b1,0)\n  h2 &lt;- pmax(w2%*%h1+b2,0)\n  (o &lt;- pmax(w3%*%h2+b3,0))\n}\n\nDer %*%-Operator steht hierbei f√ºr Matrixmultiplikation.\n\nIn der ersten Schicht wird der eindimensionale Input mit einem Vektor w1 multipliziert. Da der erste Hidden-Layer aus drei Neuronen besteht, ist dieser Gewichtsvektor deshalb ein Element in \\(\\mathbb{R}^3\\). Auf dieses Produkt wird ein Bias-Vektor der gleichen Dimension addiert. Anschlie√üend wird die pmax()-Funktion auf den resultierenden Vektor angewendet. Diese gibt einen Vektor der gleichen Dimension aus, wobei jedes Element das Maximum aus dem linken und rechten Argument enth√§lt (dies entspricht der ReLu-Aktivierungsfunktion). Der R√ºckgabewert der ersten Hidden-Layer ist demnach ein Vektor, der in \\(\\mathbb{R}^3\\) ist.\nDamit im zweiten Schritt jedes Neuron der ersten Hidden-Layer mit den Neuronen der zweiten Hidden-Layer verbunden werden kann, m√ºssen wir die Dimension der Gewichte entsprechend anpassen. Somit verwenden wir f√ºr die Verkn√ºpfung des ersten Neurons mit allen Neuronen des zweiten Hidden Layers einen Vektor in \\(\\mathbb{R}^4\\). Ebenso verwenden wir f√ºr die Verbindungen der verbleibenden zwei Neuronen einen Vektor derselben Dimension. Diese drei Vektoren k√∂nnen wir dann in einer Matrix w2\\(\\in\\mathbb{R}^{4\\times 3}\\) speichern. Die Matrix w2 wird dann also an die ersten Hidden-Layer multipliziert und √§hnlich wie im ersten Schritt mit einem Bias-Vektor addiert, bevor das komponentenweise Maximum zwischen dem resultierenden Vektor und \\(0\\) zur√ºckgegeben wird.\nIm letzten Schritt wird das Vorgehen final wiederholt, jedoch ist der Output eindimensional, weshalb die Gewichte nun wieder in einem Vektor aus \\(\\mathbb{R}^{1\\times 4}\\) (Zeilenvektor statt Spaltenvektor) gespeichert werden k√∂nnen. Der Bias \\(B_3\\) ist dann ebenso nur noch eindimensional.\n\nKonkret k√∂nnten die Gewichtsmatrizen zum Beispiel wie folgt aussehen:\n\nset.seed(234)\n\nw1 &lt;- matrix(rnorm(3),\n             ncol = 1)\n\nw2 &lt;- matrix(rnorm(12),\n             ncol = 3)\n\nw3 &lt;- matrix(rnorm(4),\n             ncol = 4)\n\nBeachte: Die Gewichtsmatrizen enthalten Realisationen einer Standardnormalverteilung, weshalb aus Gr√ºnden der Reproduzierbarkeit ein seed gesetzt wurde.\nUm nun Vorhersagewerte f√ºr einen bestimmten Wertebereich zu erhalten, k√∂nnen wir zuerst einen Vektor x erstellen und diesen anschlie√üend zus√§tzlich zu den Gewichten in die Funktion nn √ºbergeben.\n\nx &lt;- seq(-5,5,length.out = 100)\n\ny &lt;-lapply(x, nn, w1,w2,w3) %&gt;% unlist()\n\nWir k√∂nnen das Ergebnis auch einfach grafisch darstellen:\n\ntibble(x,y) %&gt;% ggplot(aes(x=x,y=y)) +\n  geom_line(col= \"navyblue\", linewidth = 1.5)+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWie in Example¬†5.3 demonstriert, k√∂nnen wir durch neuronale Netze stark nichtlineare Funktionen modellieren.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Neuronale Netze</span>"
    ]
  },
  {
    "objectID": "05_Neuronale_Netze.html#anpassen-der-modellparameter",
    "href": "05_Neuronale_Netze.html#anpassen-der-modellparameter",
    "title": "5¬† Neuronale Netze",
    "section": "5.2 Anpassen der Modellparameter",
    "text": "5.2 Anpassen der Modellparameter\nWir k√∂nnen die Anzahl der Parameter eines Neuronalen Netzes durch die Formel\n\\[\\begin{equation*}\n(J+1)N_1\\cdot\\left(\\prod_{k=1}^{K} (N_k+1)\\cdot N_{k+1}\\right)\n\\end{equation*}\\]\nberechnen. Hierbei steht der Index \\(k = 1,\\ldots,K\\) f√ºr die verschiedenen Hidden Layer des FFNN. Somit beschreiben wir mit \\(J\\) die Anzahl der Input-Neuronen, \\(N_k\\), \\(k=1\\ldots,K\\), die Anzahl der Neuronen in den Hidden-Layern und mit \\(N_{K+1}\\) die Anzahl der Output-Neuronen. Die Addition mit 1 steht in den entsprechenden Layern f√ºr den Biasterm, da dieser ebenso mit jedem Neuron der n√§chsten Layer verbunden ist. Die Anzahl der Parameter steigt also nicht linear, sondern polynomiell.\n\n5.2.1 Gradientenabstieg\nFalls wir alle Parameter eines FFNN durch ein Objekt \\(W\\) beschreiben und das FFNN in Abh√§ngigkeit der Parameter als \\(f_W\\) definieren, dann k√∂nnen wir auch bei diesem Modell das Optimierungsproblem\n\\[\\begin{equation}\\label{eq:mse_nn}\n  \\min_{W} (f_W(x)-y)^2\n\\end{equation}\\]\nl√∂sen.\nAufgrund der Nichtlinearit√§t und der hohen Anzahl der Parameter ist eine direkte Optimierung mithilfe der partiellen Ableitungen nicht m√∂glich. Stattdessen verwenden wir beim Trainieren, also beim Anpassen der Modellparameter, verschiedene Versionen des Gradientenabstiegs.\nIn der einfachsten Form ist der Gradientenabstieg durch die Formel\n\\[\\begin{equation*}\n  W_{n+1} = W_n - \\eta \\frac{\\partial}{\\partial W} \\text{MSE}_W\n\\end{equation*}\\]\ngegeben. Die Idee hierbei ist vereinfacht gesagt, dem steilsten Abstieg zu folgen, mit dem Ziel, in einem (globalen) Minimum zu landen. Wie gro√ü die Schritte dieses Abstiegs sind, k√∂nnen wir durch den Parameter \\(\\eta\\) steuern. Der einfache Gradientenabstieg hat allerdings folgende Nachteile:\n\nBei gro√üen Datenmengen ist das Berechnen der Gradienten sehr zeitaufwendig.\nAuch in hohen Dimensionen kann es passieren, dass der MSE in ein lokales Minimum f√§llt und diesem nicht mehr entkommt.\n\nUm diese beiden Probleme zu l√∂sen, verwenden wir verschiedene Versionen des Stochastic Gradient Descent. Beim Stochastic Gradient Descent wird deshalb nur eine zuf√§llig ausgew√§hlte Teilmenge der Daten verwendet, um den MSE zu berechnen und die Parameter anzupassen. Das h√§ufigere Updaten kann dazu f√ºhren, dass aus lokalen Minima ausgebrochen wird, und zus√§tzlich, dadurch, dass nur ein Teil der Daten in jedem Schritt verwendet wird, auch der Aufwand bei der Berechnung verringert wird.\nDas Bilden der partiellen Ableitungen bez√ºglich der Netzwerkparameter kann ebenso sehr aufwendig werden, da ein tiefes neuronales Netz eine vielfach verschachtelte Funktion ist. Allerdings kann hierbei die Kettenregel verwendet werden, was den Prozess beschleunigen kann. F√ºr die tats√§chliche Berechnung der Gradienten wird der Backpropagation-Algorithmus verwendet, welcher neben der Kettenregel auch noch weitere Effizienzmechanismen verwendet. Die Idee ist hierbei, dass die Gewichte in einer Layer \\(k\\) lediglich durch den Effekt auf die Neuronen in der \\(k+1\\) Layer den MSE beeinflussen. Deshalb m√ºssen wir nur die Gradienten der Layer \\(k,k-1,\\ldots,1\\) berechnen. Dieses Vorgehen mitigiert eine ineffiziente Berechnung der Gradienten durch das Vermeiden von wiederholten Berechnungen der Gradienten in den Layern \\(k+1,\\ldots,K\\), welche nicht durch die Gewichte in Layer \\(k\\) beeinflusst werden.1\n\n\n5.2.2 Regularisierung von Neuronalen Netzen\nNeuronale Netze in einem Regressions- und Klassifikationskontext tendieren stark dazu, sich zu sehr an die Trainingsdaten anzupassen. Das ist auch nicht weiter √ºberraschend, wenn man bedenkt, wie viele Parameter letztlich in einem Neuronalen Netz enthalten sind. Zum Beispiel besteht das neuronale Netz aus Figure¬†5.4 $ (1+1) +(3+1)+ (4+1) = 27$ Parametern, w√§hrend ein einfaches lineares Modell den gleichen Zusammenhang mit nur zwei Parametern beschreiben w√ºrde.\nWir wollen deshalb zwei Methoden besprechen, welche diesen Effekt mitigieren k√∂nnen.\n\n5.2.2.1 Dropout\nDie Idee des Dropout ist simpel, aber sehr effektiv: In jeder Iteration des Trainingsprozesses deaktivieren wir die Neuronen der Hidden Layer(s) mit einer Wahrscheinlichkeit \\(p\\in(0,1)\\). Wenn ein Neuron in einer Iteration deaktiviert wurde, werden dessen Gewichte in der entsprechenden Iteration nicht aktualisiert. Wir reduzieren somit aktiv die Anzahl der Parameter im Modell beim Training und verhindern, dass sich die Parameter zu stark an die Trainingsdaten anpassen.\n\nBeispiel 5.4 Betrachte folgendes Netzwerk, bestehend aus 4 Inputneuronen, einem Hidden Layer mit acht Neuronen und einem Outputneuron.\n\n\n\n\n\nAngenommen, wir wenden Dropout mit verschiedenen Werten f√ºr \\(p\\) auf dieses neuronale Netz an. Dann k√∂nnte in einer Iteration die Architektur auch wie folgt aussehen:\n\n\n\n\n\n\nFigure¬†5.5\n\n\n\nIn Figure¬†5.5 werden in der oberen Zeile zwei potenzielle Zust√§nde des Netzwerkes f√ºr \\(p=0.25\\) angezeigt. In der unteren Zeile werden zwei potenzielle Zust√§nde des Netzwerkes f√ºr \\(p=0.5\\) angezeigt.\n\n\n\n5.2.2.2 \\(L^1\\)-Regularisierung\nW√§hrend bei der Dropout-Methode zuf√§llig Neuronen deaktiviert werden, verfolgt die Idee der \\(L^1\\)-Regularisierung eine Idee, bei welcher alle Neuronen gleicherma√üen f√ºr zu hohe Werte bestraft werden.\nBeim Trainieren des Neuronalen Netzes minimieren wir den MSE bez√ºglich der Gewichte (vgl. Gleichung \\(\\eqref{eq:mse_nn}\\)). Wir k√∂nnen diesem Optimierungsproblem einen weiteren Term \\(\\lambda\\sum_{\\omega \\in W}|\\omega|\\) (\\(\\lambda &gt; 0\\)) hinzuf√ºgen, sodass wir schlussendlich den Fehlerterm (Loss)\n\\[\\begin{equation*}\n  \\frac{1}{K}\\sum_{k=1}^K (y_k -\\hat{y}_k)^2 + \\lambda \\sum_{\\omega \\in W} |\\omega|\n\\end{equation*}\\]\nbez√ºglich der Gewichte \\(\\omega\\) minimieren.\nDer Parameter \\(\\lambda &gt; 0\\) wird hierbei als penalty bezeichnet und steuert, wie stark die Gewichte \\(\\omega\\) verringert werden.\nSowohl der Penalty-Wert \\(\\lambda\\) als auch die Dropout-Rate \\(p\\) werden f√ºr gew√∂hnlich beim sogenannten Hyperparametertuning bestimmt. Dieses w√ºrde allerdings den Rahmen der Veranstaltung sprengen und wird deshalb erst in der Masterveranstaltung Machine Learning intensiv behandelt.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Neuronale Netze</span>"
    ]
  },
  {
    "objectID": "05_Neuronale_Netze.html#sec-NNR",
    "href": "05_Neuronale_Netze.html#sec-NNR",
    "title": "5¬† Neuronale Netze",
    "section": "5.3 Neuronale Netze in R",
    "text": "5.3 Neuronale Netze in R\nDas Training von neuronalen Netzwerken innerhalb des {tidymodels}-Frameworks k√∂nnen wir durch die {brulee}-Library durchf√ºhren.\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(brulee)\n\n√Ñhnlich wie bei allen zuvor trainierten Modellen m√ºssen wir hierbei lediglich eine Architektur spezifizieren und das Modell dann mithilfe der fit-Funktion trainieren.\nInteressant im Kontext dieser √úbung sind hierbei die folgenden Parameter:\n\nepochs: Eine Ganzzahl, die die Anzahl der Trainingsdurchl√§ufe angibt.\nhidden_units: Kann entweder eine Ganzzahl oder ein Vektor aus Ganzzahlen sein. Eine einzelne Zahl bedeutet, dass nur eine versteckte Schicht verwendet wird, w√§hrend ein Vektor durch seine L√§nge die Anzahl der Schichten und durch seine Werte die Anzahl der Neuronen pro Schicht angibt.\nlearn_rate: Eine positive Zahl, die die Schrittweite f√ºr den Optimierungsalgorithmus angibt.\n\nF√ºr die Demonstration des Modells verwenden wir wieder den palmerpenguins::penguins-Datensatz. Das Ziel ist dabei wie gehabt, das Gewicht eines Pinguins durch die anderen Merkmale vorherzusagen.\n\ndata_penguin &lt;- palmerpenguins::penguins\n\nIm Vergleich zu den anderen Modellen m√ºssen wir die Daten vor dem Fitten des Modells transformieren. Die {fastDummies}-Library k√∂nnen wir verwenden, um die nominalen Features in Dummy-Variablen zu transformieren. Wir postulieren au√üerdem, dass die Variable year keinen Einfluss auf die Zielvariable body_mass_g hat. Da die fastDummies::dummy_cols()-Funktion die nominalen Features im Dataset beibeh√§lt, entfernen wir nach der Transformation alle nichtnominalen Features. Zuletzt wenden wir mithilfe der mutate_at()-Funktion die scale()-Funktion auf alle Spalten au√üer body_mass_g an. Durch die Anwendung der scale()-Funktion erreichen wir, dass jedes Feature den Mittelwert 0 und die Standardabweichung 1 besitzt.\n\nlibrary(fastDummies)\n\ndata_penguin_transformed &lt;- data_penguin %&gt;%\n  na.omit()%&gt;%\n  dummy_cols() %&gt;%\n  select(-year) %&gt;%\n  select_if(is.numeric) %&gt;%\n  mutate_at(vars(-\"body_mass_g\"),scale)\n\nNachdem wir die zugrundeliegenden Daten transformiert haben, k√∂nnen wir mithilfe der mlp()-Funktion ein DNN spezifizieren.\n\nnnet_spec &lt;- mlp(epochs = 200,\n                 hidden_units = c(64,32),\n                 learn_rate = 0.01,\n                 activation = c(\"relu\",\"relu\"),\n                 mode = \"regression\"\n                 ) %&gt;% \n  set_engine(engine = \"brulee\",\n             verbose = FALSE,\n             optimizer = \"SGD\",\n             stop_iter = 15\n             ) \n\nDie Spezifikation setzt sich hierbei aus zwei Teilen zusammen:\n\nIn den Zeilen 1-6 wird ein allgemeines MLP definiert.\n\nDas Training stoppt nach maximal 200 Epochen.\nDie Architektur besteht aus dem Input-Layer, zwei Hidden-Layern mit jeweils 64 und 32 Neuronen und dem Output-Layer.\nDie Learning Rate des Gradientenabstiegs (\\(\\eta\\)) wird auf 0.01 gesetzt.\nDie Aktrivierungsfunktionen der hidden Layer setzen wir auf \"relu\".\nDen Modus setzen wir auf \"regression\".\n\nNachdem die {brulee} unabh√§ngigen Parameter definiert wurden, passen wir in den Zeilen 7-11 noch weitere Parameter an, welche spezifisch f√ºr neuronale Netze gelten, die mithilfe der {brulee} Library trainiert werden.\n\nMithilfe von engine = \"brulee\" spezifizieren wir, dass wir ein Neuronales Netz basierend auf der brulee Library trainieren wollen.\nDas Argument verbose spezifiziert, ob wir w√§hrend dem Training den Fortschritt sehen wollen. F√ºr das Experimentieren daheim w√ºrde ich das empfehlen - Hier im Skript w√ºrde das zu viel Platz einnehmen.\nDen Optimierungsalgorithmus setzen wir auf \"SGD\" was f√ºr Stochastic Gradient Descent steht.\nDas Argument stop_iter=15 spezifiziert, dass das Training beendet wird, falls nach 15 Epochen keine Reduktion des Fehlers festgestellt wird.\n\n\nBeim Trainieren der Modellparameter ist durch das Verwenden des \"SGD\" Randomness involviert, weshalb wir ein Seed setzen sollten, um Reproduzierbarkeit zu gew√§hrleisten.\n\nset.seed(123)\nnnet_res &lt;- nnet_spec %&gt;%\n  fit(data = data_penguin_transformed,\n      formula = body_mass_g ~.\n  )\n\nWir k√∂nnen dann analog zu der MLR und den Baummodellen das neuronale Netz mithilfe der fit()-Funktion trainieren. Der fit-Funktion m√ºssen wir wie gewohnt die Argumente data und formula √ºbergeben, welche die Daten und die Formel f√ºr das Fitten spezifizieren.\nDurch das Aufrufen des trainierten Modells erhalten wir dann eine Zusammenfassung der Parameter und des Trainingsprozesses.\n\nnnet_res\n\nparsnip model object\n\nMultilayer perceptron\n\nrelu relu activation\nc(64,32) hidden units,  2,881 model parameters\n333 samples, 11 features, numeric outcome \nweight decay: 0.001 \ndropout proportion: 0 \nbatch size: 300 \nlearn rate: 0.01 \nscaled validation loss after 200 epochs: 0.121 \n\n\nNachdem wir das Neuronale Netz trainiert haben k√∂nnen wir zum Beispiel mithilfe der autoplot()-Funktion den Trainingsprozess evaluieren.\n\nnnet_res %&gt;% autoplot()+\n  theme_minimal()\n\n\n\n\n\n\n\n\nAuf der \\(y\\)-Achse wird hierbei die OOS G√ºte auf Basis eines Validierungssets berechnet. Dieses Validierungsset wurde automatisch durch die Modellspezifikation generiert. Durch das Argument validation k√∂nnten wir den Anteil der Daten, welche als Validierungsset verwendet werden sollen, in der set_engine()-Funktion manuell anpssen. Falls wir also beispielsweise\n\n  ... %&gt;%\n  set_engine(\n    ... ,\n    validation = 0.2\n  )\n\n√ºbergeben, dann werden \\(20\\%\\) der √ºbergebenen Daten als Validierungsset verwendet.\nWir sehen also, dass der Validierungsfehler zu Beginn des Trainings relativ schnell abflacht und sich nach ca. 75 Epochen stabilisiert.\nDie gr√ºne, gestrichelte Linie am rechten Ende der Grafik zeigt, f√ºr welche Iteration die Gewichte letztlich verwendet wurden. Dadurch, dass die Linie ganz am rechten Rand ist, k√∂nnen wir zum Beispiel feststellen, dass das stop_iter-Kriterium hier nicht gegriffen hat.\nEvlauieren k√∂nnen wir das FFDNN analog zu den bisherigen Modellen:\n\nnnet_res %&gt;%\n  augment(data_penguin_transformed) %&gt;%\n  rmse(.pred,body_mass_g)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        285.\n\n\nDurch das √úbergeben des trainierten Modells in die augment()-Funktion zusammen mit den transformierten Daten erhalten wir ein neues Dataset, welches neben den transformierten Trainingsdaten auch noch die Predictions .pred und Residuen .resid enth√§lt. Mithilfe der Spalten .pred und body_mass_g k√∂nnen wir dann etwa die Metrik rmse berechnen.\nWichtig: Wir berechnen hier lediglich die In-Sample-G√ºte des Modells. Es ist also zu erwarten, dass diese bereits sehr gut ist.\n\n5.3.1 Regularisierung von Neuronalen Netzen in R\nEin weiterer Vorteil des {tidymodels}-Frameworks ist, dass wir die Modellparameter eines zuvor spezifizierten Modells durch die update()-Funktion aktualisieren k√∂nnen und nicht ein neues Modell spezifizieren m√ºssen. Wir k√∂nnen der Spezifikation nnet_spec mithilfe der update()-Funktion das Argument dropout = 0.1 √ºbergeben und somit eine Dropout-Rate von \\(10\\%\\) spezifizieren.\n\nnnet_spec_do &lt;- nnet_spec %&gt;%\n  update(\n    dropout = 0.1\n  )\n\nAnschlie√üend k√∂nnten wir dieses Modell wieder wie zuvor mit der fit()-Funktion trainieren.\nEbenso k√∂nnen wir die update()-Funktion dazu verwenden, um einen Penalty-Term wie in Section 5.2.2.2 hinzuzuf√ºgen:\n\nnnet_spec_pen &lt;- nnet_spec %&gt;%\n  update(\n    penalty = 0.05\n  )\n\n\n\n5.3.2 Wichtigkeit der Varibalen\nDamit wir die Wichtigkeit der Variablen, zum Beispiel mithilfe von Permutation Feature Importance, berechnen k√∂nnen, m√ºssen wir bei Neuronalen Netzen die {iml}-Library verwenden. iml steht in diesem Kontext f√ºr Interpretable Machine Learning.\nIn einem ersten Schritt trennen wir die Features und Labels, indem wir diese separat abspeichern:\n\nfeatures &lt;- data_penguin_transformed %&gt;% select(-body_mass_g)\nlabels &lt;- data_penguin_transformed %&gt;% select(body_mass_g)\n\nAnschlie√üend extrahieren wir die Modellparameter mithilfe der extract_fit_engine()-Funktion.\n\nmodel_extract &lt;- nnet_res %&gt;% extract_fit_engine()\n\nDamit wir nun die Importance-Werte berechnen k√∂nnen, erstellen wir zuerst ein neues Predictor-Objekt, welches Teil der {iml}-Library ist. Als Argumente √ºbergeben wir die extrahierten Modellparameter model_extract, die Features und Labels. Wir k√∂nnen dann mit diesem Predictor-Objekt f√ºr die gegebenen Daten die entsprechenden Importancewerte mithilfe der FeatureImp Methode berechnen. Die Importance Werte werden standardgem√§√ü bez√ºglich einer gegebenen Metrik berechnet, welche im folgenden Beispiel durch das loss Argument auf \"rmse\" gesetzt wurde.\n\nlibrary(iml)\n\nmod &lt;- Predictor$new(model_extract, data = features, y = labels)\n\nimp &lt;- FeatureImp$new(mod, loss = \"rmse\")\n\nNachdem wir die Importance Werte berechent haben, k√∂nnen wir diese zum Beispiel mithilfe von {ggplot} grafisch darstellen:\n\nimp %&gt;%\n  pluck(\"results\") %&gt;%\n  arrange(desc(permutation.error)) %&gt;%\n  ggplot(aes(x = reorder(feature,permutation.error), y= permutation.error))+\n  geom_bar(stat = \"identity\", width = 0.5) +\n  labs(\n    y = \"RMSE after permutating the respective feature\",\n    x = \"Feature\"\n  )+\n  coord_flip()+\n  theme_minimal()\n\n\n\n\n\n\n\n\nDie Spalte permutation.error der Attributs results enth√§lt hierbei die entsprechenden Fehler erzeugt durch die Permutation. Ein hoher Fehlerwert ist ein Indikator daf√ºr, dass die Variable wichtig ist. Wir k√∂nnen also anhand der Grafik argumentieren, dass die Variabel flipper_length_mm die wichtigste ist und island_Torgensen die unwichtigste.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Neuronale Netze</span>"
    ]
  },
  {
    "objectID": "05_Neuronale_Netze.html#√ºbungsaufgaben",
    "href": "05_Neuronale_Netze.html#√ºbungsaufgaben",
    "title": "5¬† Neuronale Netze",
    "section": "5.4 √úbungsaufgaben",
    "text": "5.4 √úbungsaufgaben\n\n5.4.1 Theoretische √úbungen\n\nAufgabe 5.1 Angenommen es liegt ein FFDNN mit einem Inputneuron, zwei Hidden Layer bestehend aus zwei bzw. drei Neuronen und einem Outputneuron vor.\n\nSkizziere die die Architektur im gleichen Stil wie Figure¬†5.3.\nSchreibe Funktionsvorschrift dieses FFDNN so genau wie m√∂glich auf. Gehe dabei davon aus, dass die Aktivierungsfunktion der ersten und zweiten Hidden Layer \"ReLU\" sind. Verwende hierbei die Notation aus Gleichung \\(\\eqref{eq:nn_anal}\\) bzw. Figure¬†5.1. Setze f√ºr \\(\\sigma\\) die entsprechende Funktionsvorschrift aus Table¬†5.1.\nAus wie vielen Parametern besteht dieses Neuronale Netz?\nWelche Werte kann das spezifizierte Neuronale Netz annehmen? Wie w√ºrde sich dieser Wert ver√§ndern, wenn wir auf den Output noch eine \\(\\text{tanh}\\) Aktivierungsfunktion anwenden?\n\n\n\n\n5.4.2 R √úbungen\nWie auch in den vorherigen √úbungen verwenden wir auch in dieser √úbung den hotelrates Datensatz. Ziel wird es sein die Variable avg_price_per_room durch die verbleibenden Variablen vorherzusagen. F√ºr eine initiale Transformation verwenden wir wieder folgendes Code Snippet:\n\nhotel_rates_adj &lt;- hotel_rates %&gt;%\n mutate(\n   arrival_month = factor(month(arrival_date)),\n   num_guests = adults+children,\n   is_repeated_guest = factor(is_repeated_guest),\n   ) %&gt;%\n  dplyr::select(-c(adults,children,agent,\n                   arrival_date, arrival_date_num))\n\n\nAufgabe 5.2 ¬†\n\nVerwende die {fastDummies} Library um alle nichtnumerischen Variablen im Datensatz hotel_rates_adj in Dummy Variablen umzuwandeln.\nAus wie vielen Observationen und Features besteht der neue Datensatz?\n\n\n\nAufgabe 5.3 In dieser √úbung wollen wir die Daten in Trainings- und Testdaten aufteilen. Wie wir in Section 5.3 gelernt haben, sollten wir die Daten normalisieren, also daf√ºr sorgen, dass alle numerischen Feature Mean \\(0\\) und Standardabweichung \\(1\\) besitzen. Das einfache Anwenden der training() und testing()-Funktion ist in diesem Fall nicht mehr m√∂glich, da sonst Data Leakage auftritt.\nData Leakage beschreibt das Ph√§nomen, wenn Informationen der Trainingsdaten direkt oder indirekt in die Testdaten √ºbergehen. Da wir beim Testen der Modelle stets darauf achten sollen nur Stichproben zu verwenden, welche das Modell vorher in keinster Weise gehen hat, m√ºssen wir beim Normalisieren der Daten entsprechend aufpassen.\nEs ist also ratsam die Daten zuerst in Trainings und Testdaten einzuteilen, bevor man irgendwelche Transformationen (mit Ausnahme der Dummy-Transformation) auf die Daten anwedet.\n\nVerwende das Seed 123, die initial_split(), training() und testing()-Funktionen um ein Trainings- und Testset zu erstellen. Verwende hierbei die Default-Parameter der initial_split()-Funktion.\nBetrachte das folgende Code-Snippet.\n\nErkl√§re die Funktionsweise des gesamten Snipptes und beschreibe was in dem Objekt data_train_norm enthalten ist.\nWarum wenden wir die scale()-Funktion nur auf die Variablen des Typs &lt;dbl&gt; an?\n\n\ntrain_features &lt;- data_train %&gt;%\n  dplyr::select(-avg_price_per_room)\n\ntrain_features_dbl &lt;- train_features %&gt;%\n  select_if(is.double) %&gt;%\n  scale()\n\ntrain_features_norm &lt;- train_features %&gt;%\n  select_if(is.integer) %&gt;%\n  cbind(train_features_dbl)\n\ntrain_labels &lt;- data_train %&gt;%\n  dplyr::select(avg_price_per_room)\n\n\ndata_train_norm &lt;- cbind(\n  train_features_norm,\n  train_labels\n)\n\nDamit wir das Modell auch auf die Testdaten anwenden k√∂nnen, m√ºssen wir diese im gleichen Stil transformieren wie die Trainingsdaten.\n\nVerwende das folgende Code Snippet und die Befehle aus der letzten Teilaufgabe um die Testdaten im gleichen Stil zu transformieren.\n\ntest_features_dbl &lt;- test_features %&gt;%\n  select_if(is.double) %&gt;%\n  scale(center=attr(train_features_dbl, \"scaled:center\"),\n        scale=attr(train_features_dbl, \"scaled:scale\"))\n\nWarum verwenden wir zum standardisieren hier die Means und Standardabweichungen der Trainingsdaten?\n\n\n\n\nAufgabe 5.4 Wir wollen nun die angepassten Trainingsdaten verwenden, um ein Neuronales Netz zu trainieren.\n\nSpezifiziere hierf√ºr ein Neuronales Netz mit folgenden Parametern:\n\nepochs = 200\nhidden_units = c(64,32)\nlearn_rate = 0.025\nactivation = c(\"relu\",\"relu\")\npenalty = 1e-6\nmode = \"regression\"\n\nSetze die engine spezifischen Parameter auf\n\nengine = \"brulee\"\nverbose = false\noptimizer = \"sgd\"\nstop_iter = 15\nbatch_size = 1e3\n\n\nWelche Regularisierungsmethode wurde bei dieser Modellspezifikation verwendet?\nF√ºr wie viele Epochen soll das Neuronale Netz trainiert werden?\nunter welchen Umst√§nden kann es passieren, dass der Trainingsprozess fr√ºher abbricht?\n\nTrainiere nun das Neuronale Netz auf den Trainingsdaten mithilfe der fit()-Funktion.\n\n\n\n\n\n\nWarning\n\n\n\nDas Trainieren des Netzes kann einige minuten dauern.\n\n\nEvaluiere die Performance des Neuronalen Netzes auf den Testdaten mithilfe der Metriken \\(\\text{RMSE}\\) und \\(R^2\\). Wie ist der Wert der Metrik \\(R^2\\) zu interpretieren?\nBetrachte folgende Grafik, welche die tats√§chlichen Preise den durch das Neuronale Netz gesch√§tzten Preisen gegen√ºberstellt. Beurteile die Vorhersagekraft des Neuronalen Netzes f√ºr Zimmer, welche mehr als EUR 300 pro Nacht kosten.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Neuronale Netze</span>"
    ]
  },
  {
    "objectID": "05_Neuronale_Netze.html#l√∂sungen",
    "href": "05_Neuronale_Netze.html#l√∂sungen",
    "title": "5¬† Neuronale Netze",
    "section": "5.5 L√∂sungen",
    "text": "5.5 L√∂sungen\n\nSolution 5.1 (Aufgabe¬†5.1). \n\nDas beschriebene Neuronale Netz l√§sst sich darstellen als:\n\n\n\n\n\nDas Neuronale Netz l√§sst sich durch folgende Funktionsforschrift beschreiben:\n\\[\\begin{align*}\n  f(x) =\n  \\omega_{H_{2,1},O_1} \\cdot\n  \\max\\Big\\{\n  & \\omega_{H_{1,1},H_{2,1}} \\cdot \\max\\{\\omega_{I_1,H_{1,1}} x + \\omega_{B_1,H_{1,1}},0\\} +\\\\\n  & \\omega_{H_{1,2},H_{2,1}} \\cdot \\max\\{\\omega_{I_1,H_{1,2}} x + \\omega_{B_1,H_{1,2}},0\\} +\\\\\n  & \\omega_{B_2,H_{2,1}},\n  0 \\Big\\} + \\\\\n  +\n  \\omega_{H_{2,2},O_1} \\cdot\n  \\max\\Big\\{\n  & \\omega_{H_{1,1},H_{2,2}} \\cdot \\max\\{\\omega_{I_1,H_{1,1}} x + \\omega_{B_1,H_{1,1}},0\\} +\\\\\n  & \\omega_{H_{1,2},H_{2,2}} \\cdot \\max\\{\\omega_{I_1,H_{1,2}} x + \\omega_{B_1,H_{1,2}},0\\} +\\\\\n  & \\omega_{B_2,H_{2,2}},\n  0 \\Big\\} + \\\\\n  +\n  \\omega_{H_{2,3},O_1} \\cdot\n  \\max\\Big\\{\n  & \\omega_{H_{1,1},H_{2,3}} \\cdot \\max\\{\\omega_{I_1,H_{1,1}} x + \\omega_{B_1,H_{1,1}},0\\} +\\\\\n  & \\omega_{H_{1,2},H_{2,3}} \\cdot \\max\\{\\omega_{I_1,H_{1,2}} x + \\omega_{B_1,H_{1,2}},0\\} +\\\\\n  & \\omega_{B_2,H_{2,3}},\n  0 \\Big\\} + \\\\\n  \\omega_{B_3,O_1}\n\\end{align*}\\]\nDas Neuronale Netz besteht aus insgesamt \\((1+1)\\cdot 2 + (2+1)\\cdot 3 + (3+1)\\cdot 1 = 17\\) Parametern.\nDas spezifizierte Neuronale Netz kann Werte in ganz \\(\\mathbb{R}\\) annehmen. Falls man auf den Output noch eine \\(\\text{tanh}\\) Aktivierungsfunktion anwendet, dann wird dieser auf das Intervall \\([-1,1]\\) skaliert.\n\n\n\nSolution 5.2 (Aufgabe¬†5.2). \n\nMithilfe der dummy_cols()-Funktion k√∂nnen wir die nichtnumerischen Feature in Dummy Variablen umwandeln:\n\nhotel_rates_adj &lt;- hotel_rates_adj %&gt;%\n  dummy_cols() %&gt;%\n  select_if(is.numeric)\n\nDer neue Datensatz besteht aus 408 Variablen und 15.402 Observationen.\n\n\n\nSolution 5.3 (Aufgabe¬†5.3). \n\n\n\nset.seed(123)\nsplit_hotel &lt;- initial_split(hotel_rates_adj)\ndata_train &lt;- training(split_hotel)\ndata_test &lt;- testing(split_hotel)\n\n\n\ntrain_features &lt;- data_train %&gt;%\n  dplyr::select(-avg_price_per_room)\n\nWir verwenden zuerst die select-Funktion, um aus den Trainingsdaten die Features zu extrahieren.2 Die Feature bestehen aus allen Variablen au√üer avg_price_per_room.\n\ntrain_features_dbl &lt;- train_features %&gt;%\n  select_if(is.double) %&gt;%\n  scale()\n\nNachdem wir die Feature extrahiert haben, wenden wir auf alle Variablen vom Type &lt;dbl&gt; die scale()-Funktion an, damit lediglich die metrischen Feature Normalisiert werden. Wir wollen also verhindern, dass die Dummy Variablen, welche als &lt;int&gt; gespeichert sind auch normalisiert werden.\n\ntrain_features_norm &lt;- train_features %&gt;%\n  select_if(is.integer) %&gt;%\n  cbind(train_features_dbl)\n\nNachdem wir die Metrischen Feature normiert haben, erstellen wir einen neuen Datensatz train_features_norm in welchen wir die normierten Feature und Dummy Feature zusammenf√ºhren. Der cbind() Befehl verbindet die Spalten miteinander, so dass wir dann einen neuen, vollst√§ndigen Trainingsfeature Datensatz erstellt haben.\n\ntrain_labels &lt;- data_train %&gt;%\n  dplyr::select(avg_price_per_room)\n\ndata_train_norm &lt;- cbind(\n  train_features_norm,\n  train_labels\n)\n\nIm letzten Schritt extrahieren wir aus den alten Trainingsdaten noch die Labels in der Spalte avg_price_per_room und f√ºgen diese zu dem zuvor erstellen Datensatz train_features_norm hinzu.\nDer Datensatz data_train_norm enth√§lt also neben den Trainingslabels auch noch die normierten metrischen Feature und Dummy Variablen.\nDamit wir das Modell auch auf die Testdaten anwenden k√∂nnen, m√ºssen wir diese im gleichen Stil transformieren wie die Trainingsdaten.\n\n\n\n\ntest_features &lt;- data_test %&gt;%\n  dplyr::select(-avg_price_per_room)\n\n\ntest_features_dbl &lt;- test_features %&gt;%\n  select_if(is.double) %&gt;%\n  scale(center=attr(train_features_dbl, \"scaled:center\"),\n        scale=attr(train_features_dbl, \"scaled:scale\"))\n\ntest_features_norm &lt;- test_features %&gt;%\n  select_if(is.integer) %&gt;%\n  cbind(test_features_dbl)\n\ntest_labels &lt;- data_test %&gt;%\n  dplyr::select(avg_price_per_room)\n\ndata_test_norm &lt;- cbind(\n  test_features_norm,\n  test_labels\n)\n\n\nWenn man die Testdaten mit den eigenen Means bzw. Standardabweichungen standardisieren w√ºrde, dann verwendet man Informationen aus dem Testset. Das ist ein klassischer Fall von Data Leakage, welchen wir vermeiden m√∂chten.\n\n\n\n\nSolution 5.4 (Aufgabe¬†5.4). \n\n\nset.seed(123)\n\nnnet_spec &lt;- mlp(epochs = 200,\n                 hidden_units = c(64,32),\n                 learn_rate = 0.025,\n                 activation = c(\"relu\",\"relu\"),\n                 penalty = 1e-6,\n                 mode = \"regression\"\n                 ) %&gt;% \n  set_engine(engine = \"brulee\",\n             verbose = FALSE,\n             optimizer = \"SGD\",\n             stop_iter = 15,\n             batch_size = 1e3\n             ) \n\n\nAls Regularisierung wurde das Argument penalty auf 0.000001 gesetzt. Dies entspricht dann einer \\(L^1\\) Penalisierung wie sie in Section 5.2.2.2 eingef√ºhrt wurde.\nDas Neuronale Netz soll maximal 200 Epochen trainiert werden.\nFalls nach 15 Iterationen der Validierungsfehler nicht sinkt, dann bricht das Training auch schon vor der 200. Epoche ab.\n\n\nnnet_res &lt;- nnet_spec %&gt;%\nfit(data = data_train_norm,\n    formula = avg_price_per_room ~.)\n\n\nnnet_res %&gt;%\n    augment(data_test_norm) %&gt;%\n    yardstick::rmse(.pred,avg_price_per_room)\n  nnet_res %&gt;%\n    augment(data_test_norm) %&gt;%\n    yardstick::rsq(.pred,avg_price_per_room)\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n19.234\n\n\nrsq\nstandard\n0.915\n\n\n\n\n\n\n\nDer Wert \\(R^2 = 0.915\\) impliziert, dass wir \\(91.5\\%\\) der Varianz in den Testdaten erkl√§ren k√∂nnen.\nDa fast alle Datenpunkte mit avg_price_per_room&gt;300 √ºberhalb der Winkelhalbierenden liegen, spricht das daf√ºr, dass das Modell dazu neigt die hochpreisigen Zimmer zu untersch√§tzen. Falls Punkte √ºberhalb der Winkelhalbierenden liegen, dann gilt avg_price_per_room&gt;.pred.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Neuronale Netze</span>"
    ]
  },
  {
    "objectID": "05_Neuronale_Netze.html#footnotes",
    "href": "05_Neuronale_Netze.html#footnotes",
    "title": "5¬† Neuronale Netze",
    "section": "",
    "text": "F√ºr eine tiefgreifende Erkl√§rung kann ich Grant Sandersons Video(s) zu Backpropagation empfehlen.‚Ü©Ô∏é\nDer Zusatz dplyr:: sorgt daf√ºr, dass wir die select()-Funktion aus der {dplyr} Library verwenden. Es gibt eine Gleichnamige Funktion als Abh√§ngigkeit in der {iml} Library, welche zu Konflikten f√ºhrt.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Neuronale Netze</span>"
    ]
  },
  {
    "objectID": "06_Klassifikation.html",
    "href": "06_Klassifikation.html",
    "title": "6¬† Klassifikation",
    "section": "",
    "text": "6.1 Einf√ºhrung\nIn dieser Verasntaltung setzen wir uns vor allem mit der bin√§ren Klassifikation auseinander. Die Zielvariable \\(Y\\) nimmt also nur zwei Werte an. Normalerweise kodiert man diese Werte mit \\(0\\) und \\(1\\), so dass sich \\(Y\\) darstellen l√§sst als Vektor \\(Y\\in\\{0,1\\}^K\\). Wir sind nun daran interessiert f√ºr einen Datenpunkt \\(x\\in\\mathbb{R}^{J}\\) die Wahrscheinlichkeit \\[\\begin{equation}\n  \\mathbb{P}(Y=1|X=x)\n\\end{equation}\\] zu sch√§tzen. Falls die gesch√§tzte Wahrscheinlichkeit \\(\\hat{p}\\) gr√∂√üer als ein bestimmter, vordefinierter Schwellenwert \\(q\\in(0,1)\\) ist, dann weisen wir das Sample der Klasse \\(1\\) zu. Intuitiv macht hierbei der Wert \\(q=0.5\\) Sinn, da wir uns bei einer Wahrscheinlichkeit \\(p&gt;0.5\\) sicherer sind, dass der Datenpunkt zur Klasse \\(1\\) als zur Klasse \\(0\\) geh√∂rt.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Klassifikation</span>"
    ]
  },
  {
    "objectID": "06_Klassifikation.html#evaluation-eines-bin√§ren-klassifikationsmodells",
    "href": "06_Klassifikation.html#evaluation-eines-bin√§ren-klassifikationsmodells",
    "title": "6¬† Klassifikation",
    "section": "6.2 Evaluation eines bin√§ren Klassifikationsmodells",
    "text": "6.2 Evaluation eines bin√§ren Klassifikationsmodells\nDa eine Evaluation mithilfe von Metriken wie \\(\\text{RMSE}\\) und \\(R^2\\) bei der bin√§ren Klassifikation wenig Sinn ergeben, wollen wir in diesem Abschnitt verschiedene alternativen Metriken betrachten.\n\n6.2.1 Confusionmatrix\nAls Grundlage f√ºr die Metriken welche wir im Folgenden betrachten wollen ist die sogenannte Confusion Matrix. Diese Kreuztabelle gibt an, wie viele Datenpunkte korrekt bzw. inkorrekt klassifiziert wurden. Hierbei wird insbesondere zwischen den Klassen Negativ und Positiv Unterschieden. Im vorherigen Abschnitt wurden die Klassen mit \\(1\\) und \\(0\\) enkodiert. Meistens steht \\(1\\) im Kontext der Confusion Matrix f√ºr Positive und \\(0\\) f√ºr Negative.\n\n\n\n\n\n\nFigure¬†6.1\n\n\n\nDie Felder der Confusion Matrix k√∂nnen wir am besten anhand eines Beispiels illustrieren.\n\nBeispiel 6.1 Ein Forscherteam des Universit√§tsklinikums hat einen Test f√ºr das Feststellen einer seltenen Herzkrankheit entwickelt. Falls die √Ñrzte diese Herzkrankheit korrekt feststellen, kann diese durch einen operativen Eingriff geheilt werden. Die Wahrscheinlichkeit eines erfolgreichen Eingriffs ist hierbei gr√∂√üer als die Wahrscheinlichkeit, dass eine kranke Person den Krankheitsverlauf √ºberlebt. Ein positives Testergebnis wird mit \\(1\\) bzw. Predicted Positive enkodiert und ein negatives Ergebnis mit \\(0\\) bzw. Predicted Negative. Nun gibt es folgende M√∂glichkeiten:\n\nDer Test ist Positiv und die untersuchte Person hat die Krankheit. Dann spricht man von True Positive.\nDer Test ist Positiv und die untersuchte Person hat die Krankheit nicht. Dann spricht man von False Positive.\nDer Test ist Negativ und die untersuchte Person hat die Krankheit nicht. Dann spricht man von True Negative.\nDer Test ist Negativ und die untersuchte Person hat die Krankheit. Dann spricht man von False Negative.\n\n\nNeben einer korrekten Klassifikation (TP und TN) sollten wir uns besonders mit den beiden fehlerhaften Klassifikationsszenarien auseinandersetzen. In Example¬†6.1 ist eine f√§lschlicherweise als Negativ klassifizierte Person (FN) also ein Patient oder eine Patientin, welche die Herzkrankheit besizt, aber nicht diagnostiziert wird. Diese Art von Fehler ist im Beispiel besonders gravierend, da die Person eine h√∂here √úberlebenswahrscheinlichkeit durch den operativen Eingriff besitzt. Falls eine Person f√§lschlicherweise als Positiv klassifiziert wird (FP), dann bedeutet das, dass der Test positiv ausf√§llt, obwohl die Person nicht an der Herzkrankheit erkrankt ist. Unter gew√∂hnlichen Umst√§nden w√§re eine solche Fehlklassifikation nicht ganz so gravierend, da die Person schlie√ülich nicht an der seltenen Krankheit erkrankt ist. Falls allerdings auf Basis des Testergebnisses trotzdem ein operativer Eingriff vorgenommen wird, dann setzt man den Patient oder die Patientin einem unn√∂tigen Risiko aus.\nWie gut also so ein Test tats√§chlich funktioniert, sollte man also nicht nur an der absoluten oder relativen Anzahl der korrekt klassifizierten Datenpunkte festmachen, sondern auch daran, wie hoch die entsprechenden Fehlerquoten sind.\nPr√§ziser l√§sst sich diese Aussage mit folgenden Kennzahlen ausdr√ºcken:\n\nAccuracy: \\[\\begin{equation*}\n  \\text{Accuracy} = \\frac{TP+TN}{TP+TN+FP+FN}\n\\end{equation*}\\] Die Accuracy misst den relativen Anteil der korrekt klassifizierten Datenpunkte an der Gesamtanzahl der Datenpunkte. Sie beschreibt die Wahrscheinlichkeit f√ºr ein korrektes Testergebnis.\nSensitivit√§t: \\[\\begin{equation*}\n  \\text{Sensitivity} = \\frac{TP}{TP+FN}\n\\end{equation*}\\] Die Sensitivit√§t misst den relativen Anteil der als korrekterweise positiv klassifizierten Datenpunkte an der Gesamtanzahl der positiven Datenpunkte. Sie beschreibt die bedingte Wahrscheinlichkeit, dass ein positives Ergebnis vorhergesagt wird unter der Annahme, dass das Ergebnis tats√§chlich positiv ist.\nSpezifizit√§t: \\[\\begin{equation*}\n  \\text{Specificity} = \\frac{TN}{TN+FP}\n\\end{equation*}\\] Die Spezifizit√§t ist das Gegenst√ºck zur Sensititivit√§t. Sie misst den relativen Anteil der als korrekterweise negativ klassifizierten Datenpunkte an der Gesamtanzahl der negativen Datenpunkte. Spezifizit√§t kann auch als die bedingte Wahrscheinlichkeit interpretiert werden, dass ein negatives Ergebnis vorhergesagt wird, unter der Annahme, dass das Ergebnis tats√§chlich negativ ist.\nPr√§zision: \\[\\begin{equation*}\n  \\text{Precision} = \\frac{TP}{TP+FP}\n\\end{equation*}\\] Die Pr√§zision misst den relativen Anteil der korrekterweise als positiv klassifizierten Datenpunkte an der Gesamtzahl der als positiv klassifizierten Datenpunkte. Besonders bei unbalancierten Datens√§tzen1 ist die Pr√§zision eine wichtige Metrik, da die Accuracy bei unbalacierten Datens√§tzen zu Trugschl√ºssen f√ºhren kann.\n\n\n\n6.2.2 ROC-Kurve\nIn Section 6.1 haben wir kurz den Entscheidungsprozess bei einer Klassifikation in Abh√§ngigkeit des Schwellenwerts \\(q\\) eingef√ºhrt. Da der Schwellenwert \\(q\\) allerdings stark die Confusion Matrix beeinflussen kann, sollte man sich verschiedene Schwellenwerte in Abh√§ngigkeit ausgew√§hlter Metriken betrachten. Eine dieser Gegen√ºberstellungen ist durch die sogenannte Receiver-Operator Characteristic Kurve gegeben. Diese zeigt auf der \\(x\\)-Achhse die False-Positive Rate (\\(1-\\text{specificity}\\)) und auf der \\(y\\)-Achse die Sensitivit√§t an. Der Kurve ergibt sich dann durch das Variieren des Schwellenwerts \\(q\\):\n\n\n\n\n\n\n\n\n\nDie ROC-Kurve ist monoton steigend, d.h. je gr√∂√üer die False Positive Rate (\\(1-\\text{specificity}\\)) ist, desto gr√∂√üer wird die Sensitivit√§t. Der optimale Threshold ist dann durch jenen Punkt gegeben, welcher am am n√§hsten zum Punkt \\((1,0)\\) ist. Ein Entscheidungsschwellenwert von \\(0.5\\) ist also nicht zwingende der optimale Schwellenwert f√ºr eine Entscheidung, wenn das Ziel ist gleichzeitig die Sensitivit√§t zu maximieren und die False Positive Rate zu minimieren.\nDie Winkelhalbierende beschreibt den Fall, dass die Sensitivit√§t gleich der False Positive Rate ist. Falls diese beiden Metriken gleich sind, dann kann das unterliegende Modell nicht zwischen True Positives und False Positives unterscheiden. Dieser Fall wird dann auch gerne als M√ºnzwurf beschrieben, da die Wahrscheinlichkeit f√ºr eine korrekte, bzw. falsche Klassifizierung \\(50\\%\\) betr√§gt.\nDie ROC-Kurve kann allerdings auch irref√ºhrend sein, weshalb man neben dieser Metrik auch noch andere Metriken betrachten sollte. Besonders, falls die Klassen unbalanciert sind kann die ROC-Kurve irref√ºhrend sein, da sie keine Informationen bez√ºglich der Pr√§zision des Modells enth√§lt.\nNeben dem Ablesen der Sensitivit√§t und False Positive Rate an der Kurve selbst k√∂nnen wir auch die Fl√§che unter der Kurve berechnen. Die sogenannte Area under the Receiver Operator Characteristic (AUROC) nimmt dabei Werte zwischen \\(0\\) und \\(1\\) an, wobei \\(1\\) einem perfekten Modell entspricht und \\(0\\) einem Modell, welches alle Vorhersagen falsch trifft.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Klassifikation</span>"
    ]
  },
  {
    "objectID": "06_Klassifikation.html#die-logistische-regression",
    "href": "06_Klassifikation.html#die-logistische-regression",
    "title": "6¬† Klassifikation",
    "section": "6.3 Die Logistische Regression",
    "text": "6.3 Die Logistische Regression\nAls Erweiterung der linearen Regression erlaubt es uns die logisitische Regression ebenso eine einfache Parametrisierung der gesch√§tzten Wahrscheinlichkeit \\(\\hat{p}\\). Betrachten wir zuerst den Fall, dass unser Datensatz aus lediglich einer unabh√§ngigen Variable besteht. Im Falle der linearen Regression sch√§tzen wir dann die Parameter \\(b_0\\) und \\(b_1\\) der Gleichung (vgl. Gleichung¬†2.1)\n\\[\\begin{equation*}\nY = b_0 + b_1 X + \\varepsilon\n\\end{equation*}\\]\n\n6.3.1 Modellgleichung\nWir k√∂nnen diese Modellgleichung dann mithilfe der logistischen Funktion \\[\\begin{equation*}\n  \\sigma(x) = \\frac{1}{1+\\exp(-x)} \\in (0,1) , x\\in\\mathbb{R}\n\\end{equation*}\\] so transformieren, dass die Werte der Regressionsgleichung zwischen \\(0\\) und \\(1\\) liegen. Die Werte dieser transformierten Regressionsgleichung k√∂nnen dann als Wahrscheinlichkeiten interpretiert werden. Wir erhalten also nach dem Sch√§tzen der Parameter die Modellgleichung\n\\[\\begin{equation*}\n  \\widehat{\\mathbb{P}(Y=1|X=x)} = \\frac{1}{1+\\exp\\left(-(b_0+b_1x)\\right)}\n\\end{equation*}\\]\nDiese Sch√§tzgleichung l√§sst sich dann analog zum MLR Modell erweitern zu\n\\[\\begin{equation*}\n  \\widehat{\\mathbb{P}(Y=1|X=x_k)} = \\frac{1}{1+\\exp\\left(-(b_0+\\sum_{j=1}^Jb_jx_{k,j})\\right)}\n\\end{equation*}\\]\nDie Parameter dieser Sch√§tzgleichung werden mithilfe der Maximum Likelihood Methode durchgef√ºhrt, da eine analytische L√∂sung wie f√ºr die Parameter des MLR nur schwer zu berechnen ist.\n\nBeispiel 6.2 Betrachten wir folgendes Beispiel:\nDie Sch√§tzgleichung einer einfachen linearen Regression sei durch \\[\\begin{equation*}\n  y = \\hat{b}_0+\\hat{b}_1 x  = 4x + 1\n\\end{equation*}\\]\ngegeben:\n\n\n\n\n\n\n\n\n\nDann eignet sich diese Funktion nicht, um eine Wahrscheinlichkeit vorherzusagen, da die Funktion f√ºr \\(x &lt; -0.25\\) negative Werte annimmt und f√ºr \\(x&gt;0\\) Werte annimmt, welche gr√∂√üer als \\(1\\) sind.\nDurch eine Transformation dieser Sch√§tzgleichung zu einer logistischen Sch√§tzgleichung der Form\n\\[\\begin{equation*}\n\\widehat{\\mathbb{P}(y=1|x)} = \\frac{1}{1+\\exp(-(\\hat{b}_0 + \\hat{b}_1 x))} =\\frac{1}{1+\\exp(-(4x + 1))}\n\\end{equation*}\\]\nerreichen wir, dass die Funktionswerte nur noch im Intervall \\((0,1)\\) liegen:\n\n\n\n\n\n\n\n\n\nJe gr√∂√üer also die Werte f√ºr \\(x\\) sind, desto h√∂her ist die Wahrscheinlichkeit, dass wir die Klassifikation \\(y=1\\) annehmen.\n\n\n\n6.3.2 Interpretation der Koeffizienten\nIm Vergleich zur multiplen Linearen Regression lassen sich die Koeffizienten des logistischen Modells nicht mehr direkt interpretieren. Auch die √úberpr√ºfung der Signifikanz einzelner Variablen ist nicht mehr mithilfe des \\(t\\) Test umsetzbar.\nGrundlegend k√∂nnen wir allerdings folgende Aussagen treffen:f Das Vorzeichen eines gesch√§tzeten Parameters gibt Auskunft dar√ºber, ob die gesch√§tzte Wahrscheinlichkeit beim Erh√∂hen der Variable (um eine Einheit) steigt oder sinkt.\n\nFalls \\(b_j&gt;0,\\: j\\in\\{1,...,J\\}\\) so steigt die gesch√§tzte Wahrscheinlichkeit \\(\\widehat{\\mathbb{P}(Y=1|X)}\\).\nFalls \\(b_j&lt;0,\\: j\\in\\{1,...,J\\}\\), dann sinkt die gesch√§tzte Wahrscheinlichkeit \\(\\widehat{\\mathbb{P}(Y=1|X)}\\).\n\nUm den Einfluss einer Variable zu quantifizieren, betrachten wir die sogenannten Odds. Die Odds sind definiert als \\[\\begin{equation}\\label{eq:odds}\n\\mathrm{Odds} = \\frac{\\mathbb{P}(Y=1|X=x)}{\\mathbb{P}(Y=0|X=x)} = \\exp(b_0+\\sum_{j=1}^Jb_jx_j)\n\\end{equation}\\] Die Odds k√∂nnen Werte im Intervall \\((0,\\infty)\\) annehmen. Sie k√∂nnen interpretiert werden als die Chance dessen, dass ein Event eintritt (\\(y=1\\)) gegen√ºber dem nichteintreten des Events \\((y=0)\\).\n\nBeispiel 6.3 Bei einem Wettanbieter f√ºr Fu√üballwetten sind f√ºr ein Spiel der Mannschaft \\(A\\) gegen Mannschaft \\(B\\) die dezimalen Odds (Quoten) f√ºr einen Sieg von Mannschaft \\(A\\) mit \\(1.75\\) und f√ºr einen Sieg von Mannschaft \\(B\\) mit \\(2.10\\) gegeben. Es wird angenommen, dass kein Unentschieden m√∂glich ist. Die dezimalen Odds geben an, wie viel ein Spieler f√ºr einen eingesetzten Euro im Falle eines Gewinns zur√ºckerh√§lt (inklusive des Einsatzes). Die vom Wettanbieter implizierten Wahrscheinlichkeiten werden wie folgt berechnet:\n\nDie implizierte Wahrscheinlichkeit f√ºr einen Sieg von \\(A\\) ist:\n\\[\\begin{equation*}\n  \\mathbb{P}_{\\text{impl}}(A) = \\frac{1}{\\text{Odds}(A)} = \\frac{1}{1.75} \\approx 0.5714\n\\end{equation*}\\]\nDie implizierte Wahrscheinlichkeit f√ºr einen Sieg von \\(B\\) ist:\n\\[\\begin{equation*}\n  \\mathbb{P}_{\\text{impl}}(B) = \\frac{1}{\\text{Odds}(B)} = \\frac{1}{2.10} \\approx 0.4762\n\\end{equation*}\\]\n\nDie Summe der implizierten Wahrscheinlichkeiten ist: \\[\\begin{equation*}\n  \\mathbb{P}_{\\text{impl}}(A) + \\mathbb{P}_{\\text{impl}}(B) \\approx 0.5714 + 0.4762 = 1.0476\n\\end{equation*}\\] Diese Summe ist gr√∂√üer als 1, was die Wettmarge (Overround) des Anbieters widerspiegelt. Die tats√§chlichen Wahrscheinlichkeiten, die der Wettanbieter zugrunde legt (nach Bereinigung der Marge), erh√§lt man durch Normalisierung: \\[\\begin{equation*}\n  \\mathbb{P}(A) = \\frac{\\mathbb{P}_{\\text{impl}}(A)}{\\mathbb{P}_{\\text{impl}}(A) + \\mathbb{P}_{\\text{impl}}(B)} = \\frac{0.5714}{1.0476} \\approx 0.5455\n\\end{equation*}\\] \\[\\begin{equation*}\n  \\mathbb{P}(B) = \\frac{\\mathbb{P}_{\\text{impl}}(B)}{\\mathbb{P}_{\\text{impl}}(A) + \\mathbb{P}_{\\text{impl}}(B)} = \\frac{0.4762}{1.0476} \\approx 0.4545\n\\end{equation*}\\]\nSomit rechnet der Wettanbieter mit einer Wahrscheinlichkeit von ca. \\(54.55\\%\\), dass Mannschaft \\(A\\) gewinnt, und mit ca. \\(45.45\\%\\), dass Mannschaft \\(B\\) gewinnt.\n\nFalls \\(\\mathrm{Odds}=1\\), dann sind beide Events gleich wahrscheinlich, w√§hrend Odds \\(&gt;1\\) bzw. \\(&lt;1\\) implizieren, dass die Wahrscheinlichkeit f√ºr das Eintreten des untersuchten Events \\(&gt;50\\%\\) bzw. \\(&lt;50\\%\\) ist.\nDer Einfluss der Modellparameter auf die Odds kann ebenso interpretiert werden:\n\nSteigt eine Variable \\(X_j\\) um eine Einheit, so erh√∂hen, bzw. sinken die Odds um \\(\\exp(b_j)\\).\n\nFalls zum Beispiel \\(\\exp(\\hat{b}_j)=2\\), so verdoppeln sich die Odds beim Erh√∂hen der Variable \\(X_j\\) um eine Einheit.\n\n\n6.3.3 Statistische Signifikanz\nIm Vergleich zur linearen Regression k√∂nnen bei der logistischen Regression den \\(t\\)-Test bzw. \\(F\\)-Test nicht auf ein gesch√§tztes Modell anwenden.\nF√ºr die gsch√§tzten Koeffizienten verwendet man den Wald-Test. Hierbei werden die Hypothesen \\[\\begin{equation*}\nH_0: b_j = 0\\quad\\text{vs.}\\quad H_1:b_j\\neq 0\n\\end{equation*}\\] gepr√ºft. Die Test Statistik ist durch \\(W = \\frac{\\hat{b_j}}{\\hat{\\text{sd}}(\\hat{b_j})}\\) gegeben und zum Berechnen des \\(p\\)-Werts verwenden wir die Standardnormalverteilung.\nDie statistische Signifikanz des Gesamtmodells k√∂nnen wir zum Beispiel mithilfe des Likelihood Ratio Tests evaluieren. Das Hypothesenpaar ist durch\n\\[\\begin{equation*}\nH_0: b_j = 0\\, \\forall j=1,...,J\\quad\\text{vs.}\\quad H_1:b_j\\neq 0 \\text{ f√ºr mind. ein } j.\n\\end{equation*}\\]\ngegeben.\nDie Teststatistik wird durch den Term\n\\[\\begin{equation*}\n  \\nu = -LL_v\n\\end{equation*}\\]\nwobei \\(-LL_v\\) die Log-Likelihood des Vollmodells ist, also\n\\[\\begin{equation*}\n  LL_v = \\sum_{k=1}^{K}\\hat{y}_k\\log(\\widehat{\\mathbb{P}(Y=y_k|X=x_k)})+(1-\\hat{y}_k)\\log(1-\\widehat{\\mathbb{P}(Y=y_k|X=x_k)})\n\\end{equation*}\\]\nUnter der Annahme, dass \\(\\nu\\sim \\chi^2_{n-k}\\) ist, kann dann der entsprechende \\(p\\)-Wert berechnet werden.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Klassifikation</span>"
    ]
  },
  {
    "objectID": "06_Klassifikation.html#klassifikationsb√§ume",
    "href": "06_Klassifikation.html#klassifikationsb√§ume",
    "title": "6¬† Klassifikation",
    "section": "6.4 Klassifikationsb√§ume",
    "text": "6.4 Klassifikationsb√§ume\nIn Chapter 4 haben wir uns bereits intensiv mit Regressionsb√§umen auseinandergesetzt. Der √úbergang von Regressions zu klassifikationsb√§umen ist letztendlich auch nicht mehr komplex! An Stelle einer Reelwertigen Zielvariable betrachten wir in den Blattknoten nun gesch√§tzte Klassen, bzw. Klassenwahrscheinlichkeitn. Der Algorithmus zum sch√§tzen eines Klassifikationsbaums im Verlgeich zu Regressionsb√§umen ist hierbei fast unver√§ndert. Lediglich der Improvement Wert, welcher bei Regressionsb√§umen mithilfe des \\(\\text{MSE}\\) berechnet wird, muss durch eine passende Metrik im Klassifikationskontext ersetzt werden. Diese Metrik ist durch die Gini Impurity gegeben. F√ºr einen Knoten \\(K_n\\) ist diese definiert als \\[\\begin{equation*}\n  1-(p_1^2+p_0^2)\n\\end{equation*}\\]\nwobei \\(p_1\\) die relative H√§ufigkeit der Klasse \\(1\\) im Knoten ist und \\(p_0\\) die relative H√§ufigkeit der Klasse \\(0\\) im Knoten. Es wird dann getestet, f√ºr welches \\(p_i,\\: i=0,1\\) das entfernen zu einem gr√∂√üeren Impurity Index f√ºhrt. D.h., falls \\(p_1\\geq p_0\\), dann wird der Knoten als Klassifikationswert Klasse \\(1\\) zur√ºckgeben. Die Entscheidung bez√ºglich den Splittingvariablen funktioniert gleich: Teste, f√ºr welche Variable sich die gr√∂√üte Reduktion der Gini Impurity ergibt und verwende diese f√ºr einen weiteren Split.\nDie Wichtigkeit der Variablen kann dann ebenso bez√ºglich dieses Impurity Wertes gemessen werden, so dass wir effektiv die gleiche Permutation Feature Importance Methode verwenden k√∂nnen.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Klassifikation</span>"
    ]
  },
  {
    "objectID": "06_Klassifikation.html#neuronale-netze",
    "href": "06_Klassifikation.html#neuronale-netze",
    "title": "6¬† Klassifikation",
    "section": "6.5 Neuronale Netze",
    "text": "6.5 Neuronale Netze\n√Ñhnlich wie bei Klassifikationsb√§umen m√ºssen wir auch bei Neuronalen Netzen lediglich die Verlustfunktion anpassen, um nominale Zielvariablen zu ber√ºcksichtigen. Der R√ºckgabewert eines Neuronalen Netzes bei der bin√§ren Klassifikation ist dabei nicht das Klassenlabel selbst, sondern die gesch√§tzte Wahrscheinlichkeit \\(\\widehat{\\mathbb{P}(Y=1|X=x)}\\). Die Verlustfunktion ist durch die sogenannte Binary Cross Entropy\n\\[\\begin{equation*}\nL(y,\\hat{y}) -\\left(y\\log(\\hat{y})+(1-y)\\log(1-\\hat{y})\\right)\n\\end{equation*}\\]\ngegeben, wobei \\(\\hat{y} = \\widehat{\\mathbb{P}(Y=1|X=x)}\\) und \\(y\\) das entsprechende Klassenlabel \\(1\\) oder \\(0\\) ist. Wichtig ist, dass die finale Aktiverungsfunktion den Output des Neuronalen Netzes also auf das Intervall \\([0,1]\\) skaliert, damit die Verlustfunktion auch berechnet werden kann (vgl. Aufgabe¬†5.1 Teilaufgabe 4)!",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Klassifikation</span>"
    ]
  },
  {
    "objectID": "06_Klassifikation.html#klassifikation-in-r",
    "href": "06_Klassifikation.html#klassifikation-in-r",
    "title": "6¬† Klassifikation",
    "section": "6.6 Klassifikation in R",
    "text": "6.6 Klassifikation in R\nIn diesem Abschnitt wollen wir die verschiedenen Klassifizierungsalgorithmen und deren Evaluation in R betrachten. Als Grundlage f√ºr diese Einf√ºhrung dient der penguins Datensatz, welchen wir auch schon in vorherigen √úbungen behandelt haben. Ziel wird es sein, die Variable Geschlecht vorherzusagen, da diese im Datensatz bin√§r mit \"male\" und `‚Äúfemale\n\ndata_penguin &lt;- palmerpenguins::penguins %&gt;%\n  select(-year) %&gt;%\n  na.omit() %&gt;%\n  mutate(sex = factor(\n    if_else(sex==\"male\",1,0)\n    )\n  )\n\nNachdem wir den Datensatz als data_penguin gespeichert haben, entfernen wir alle NA Werte und wandeln mithilfe der mutate() Funktion die Variable sex in eine Faktor Variable um. Die Auspr√§gungen setzen hierbei auf \\(1\\), falls ein Pinguin m√§nnlich ist und \\(0\\), falls ein Pinguin weiblich ist.\n\n6.6.1 Logistische Regression\nEin logistisches Modell k√∂nnen wir mithilfe der logistic_reg() Funktion spezifizieren. Nach dem Spezifizieren k√∂nnen wir dann die Modellparameter einfach wieder mithilfe der fit() Funktion sch√§tzen. Als Argumente √ºbergeben wir der fit() Funktion die Daten selbst und die Formel, bez√ºglich welcher die Parameter gesch√§tzt werden sollen. Da wir die Variable sex prognostizieren wollen, verwenden wir deshalb die Formel sex~..\n\nlog_spec &lt;- logistic_reg()\n\nlog_fit &lt;- log_spec %&gt;%\n  fit(data = data_penguin,\n      formula = sex ~.)\n\nNach dem Sch√§tzen der Parameter k√∂nnen wir wieder mithilfe der extract_fit_engine() Funktion die Parameter extrahieren und die Modellzusammenfassung durch die summary() Funktion erzeugen.\n\nlog_fit %&gt;% extract_fit_engine() %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = sex ~ ., family = stats::binomial, data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.4128  -0.2000   0.0022   0.1441   2.8235  \n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -80.376672  12.329735  -6.519 7.08e-11 ***\nspeciesChinstrap   -7.402697   1.662534  -4.453 8.48e-06 ***\nspeciesGentoo      -8.427611   2.597027  -3.245  0.00117 ** \nislandDream         0.324158   0.809135   0.401  0.68870    \nislandTorgersen    -0.507858   0.855746  -0.593  0.55287    \nbill_length_mm      0.614436   0.131968   4.656 3.22e-06 ***\nbill_depth_mm       1.646446   0.335798   4.903 9.43e-07 ***\nflipper_length_mm   0.026654   0.048307   0.552  0.58111    \nbody_mass_g         0.005819   0.001087   5.352 8.71e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 461.61  on 332  degrees of freedom\nResidual deviance: 126.05  on 324  degrees of freedom\nAIC: 144.05\n\nNumber of Fisher Scoring iterations: 7\n\n\nDie Variable island scheintnicht statistisch signifikant zum Niveau \\(\\alpha = 0.1\\) zu sein, was auch den Erwartungen entspricht, da auf jeder Insel sowohl m√§nnliche als auch weibliche Pinguine leben. Allerdings stellt sich dann nat√ºrlich wieder die Frage, warum die Variable species statistisch signifikant ist! Eine m√∂gliche Ursache w√§re Multikollinearit√§t, wobei wir das an dieser Stelle nicht weiter untersuchen wollen. Au√üerdem ist die Variable flipper_length_mm nicht statistisch signifikant zum Niveau \\(\\alpha = 0.1\\). Ein Grund hierf√ºr k√∂nnte sein, dass die durchschnittliche L√§nge der Flossen bei m√§nnlichen und weiblichen Pinguinen gleichgro√ü ist.\nUm das Modell zu evaluieren, sollten wir zun√§chst eine Confusion Matrix erzeugen. Daf√ºr m√ºssen wir zun√§chst Vorhersagen generieren. Wir daf√ºr die augment() Funktion direkt auf das log_fit Objekt unter Verwendung des Arguments data_penguin anewenden. Der R√ºckgabewert ist ein Tibble welches neben dem √ºbergebenen Datensatz noch die Spalten .pred_class, .pred_0 und pred_1 enth√§lt.\n\nDie Spalte .pred_class enth√§lt die vorhergesagte Klasse ermittelt durch den Schwellenwert \\(q=0.5\\).\nDie Spalte .pred_0 enth√§lt die gesch√§tzte Wahrscheinlichkeit, dass der Datenpunkt zur Klasse \\(0\\) (sex = \"female\") geh√∂rt.\nDie Spalte .pred_1 enth√§lt die gesch√§tzte Wahrscheinlichkeit, dass der Datenpunkt zur Klasse \\(1\\) (sex = \"male\") geh√∂rt.\n\n\nlog_fit %&gt;%\n  augment(data_penguin) %&gt;%\n  glimpse()\n\nRows: 333\nColumns: 10\n$ .pred_class       &lt;fct&gt; 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1‚Ä¶\n$ .pred_0           &lt;dbl&gt; 3.604544e-01, 7.103332e-01, 9.151390e-01, 7.923568e-‚Ä¶\n$ .pred_1           &lt;dbl&gt; 0.639545554, 0.289666770, 0.084861038, 0.207643155, ‚Ä¶\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse‚Ä¶\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6‚Ä¶\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2‚Ä¶\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18‚Ä¶\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800‚Ä¶\n$ sex               &lt;fct&gt; 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0‚Ä¶\n\n\nUm nun eine Confusion Matrix zu erzeugen, selektieren wir zuerst die relevanten Spalten .pred_class und sex und √ºbergeben diese der table() Funktion. Die table() Funktion erzeugt eine Kreuztabelle welche einer Confusion Matrix entspricht.\n\nlog_fit %&gt;%\n  augment(data_penguin) %&gt;%\n  select(.pred_class,sex) %&gt;%\n  table()\n\n           sex\n.pred_class   0   1\n          0 153  13\n          1  12 155\n\n\nNun haben wir allerdings noch folgendes Problem: In Figure¬†6.1 stehen die Positives in der ersten Zeile bzw. der ersten Spalte. Die resultierende Confusion Matrix zeigt die Werte Allerdings sortiert nach Zeilen- bzw. Spaltennamen an. Wir k√∂nnen diese aber einfach neu anordnen indem wir den Befehl auf die zuvor selektierten Spalten anwenden:\n\nmutate(across(c(.pred_class, sex), ~ factor(., levels = c(\"1\", \"0\"))))\n\nDa die Spalten .pred_class und sex Faktorvariablen sind, k√∂nnen wir die Werte mithilfe des levels Argument neu sortieren. Durch das Spezifizieren von levels = c(\"1\",\"0\") erreichen wir, dass die Reihenfolge der Zeilen und Spalten getauscht werden. Es gilt also nach dieser Transformation \"1\"&lt;\"0\".\n\ncm_lr &lt;- log_fit %&gt;%\n  augment(data_penguin) %&gt;%\n  select(.pred_class,sex) %&gt;%\n  mutate(across(c(.pred_class, sex), ~ factor(., levels = c(\"1\", \"0\")))) %&gt;%\n  table()\n\ncm_lr\n\n           sex\n.pred_class   1   0\n          1 155  12\n          0  13 153\n\n\nNun haben wir zwar erreicht, dass die Eintr√§ge richtig sortiert sind, allerdings spezifizieren die Spalten in Figure¬†6.1 die Predictions und die Zeilen die tats√§chlichen Werte.\nUm die Zeilen und Spalten zu vertauschen, k√∂nnen wir die resultierende Tabelle einfach mithilfe der t() Funktion transponieren:\n\ncm_lr &lt;- cm_lr %&gt;% t()\n\ncm_lr\n\n   .pred_class\nsex   1   0\n  1 155  13\n  0  12 153\n\n\nAuf Basis dieser Confusion Matrix ergeben sich nun folgende Metriken:\n\n\n\nMetric\nValue\n\n\n\n\nAccuracy\n0.924\n\n\nSensitivity\n0.923\n\n\nSpecificity\n0.927\n\n\nPrecision\n0.928\n\n\n\nDie oben aufgef√ºhrten Metriken k√∂nnen wir nat√ºrlich auch mithilfe von R berechnen. Damit wir nicht jede Metrik einzeln berechnen m√ºssen, erstellen wir ein metric_set Objekt mithilfe der metric_set() Funktion, welcher wir die entsprechenden Metriken als Argumente √ºbergeben. Um das Modell dann auf Basis dieser Menge an Metriken zu evaluieren, √ºbergeben wir dem Objekt die Argumente truth = sex und estimate = .pred_class. Wichtig ist, dass wir wieder die Reihenfolge der Auspr√§gungen anpassen, um die Darstellung der Confusion Matrix aus Figure¬†6.1 zu erhalten:\n\nmulti_metric &lt;- metric_set(accuracy,sensitivity,specificity,precision)\n\nlr_metrics &lt;- log_fit %&gt;%\n  augment(data_penguin) %&gt;%\n  mutate(across(c(.pred_class, sex), ~ factor(., levels = c(\"1\", \"0\")))) %&gt;%\n  multi_metric(truth = sex,\n               estimate = .pred_class)\n\nlr_metrics\n\n# A tibble: 4 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.925\n2 sensitivity binary         0.923\n3 specificity binary         0.927\n4 precision   binary         0.928\n\n\nDie eben berechneten Metriken basieren alle auf dem Schwellenwert \\(q = 0.5\\). Wir sollten deshalb auch die ROC Kurve betrachten, um ein Gef√ºhl f√ºr verschiedene Schwellenwerte zu bekommen. Die Werte der ROC-Kurve k√∂nnen wir mithilfe der roc_curve() Funktion berechnen: Als Argument √ºbergeben wir der Funktion die gesch√§tzten Wahrscheinlichkeiten, dass ein Sample zur Klasse \\(1\\) geh√∂rt und die Variable sex, welche die tats√§chlichen Werte enth√§lt.\nDer R√ºckgabewert ist dann ein Tibble, welcher neben dem Schwellenwert \\(q\\) gegeben durch die Spalte .threshold die beiden Spalten specificity und sensitivity enth√§lt.\n\nlr_roc &lt;- log_fit %&gt;%\n  augment(data_penguin) %&gt;%\n  mutate(across(c(.pred_class, sex), ~ factor(., levels = c(\"1\", \"0\")))) %&gt;%\n  roc_curve(.pred_1,truth = sex)\n\nlr_roc %&gt;% glimpse()\n\nRows: 335\nColumns: 3\n$ .threshold  &lt;dbl&gt; -Inf, 5.839409e-06, 8.115530e-06, 9.277271e-06, 1.030156e-‚Ä¶\n$ specificity &lt;dbl&gt; 0.000000000, 0.000000000, 0.006060606, 0.012121212, 0.0181‚Ä¶\n$ sensitivity &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n\n\nNun k√∂nnen wir die Kurve mithilfe von ggplot erzeugen:\n\np1 &lt;- lr_roc %&gt;%\n  ggplot(aes(x=1-specificity,y=sensitivity))+\n  geom_path()+\n  theme_minimal()\n\np1\n\n\n\n\n\n\n\n\nDie geom_path() Funktion verbindet die Punkte in der √ºbergebenen Reihenfolge. Wir k√∂nnen diese Grafik nun beliebig erweitern, um z.B. die Sensitivit√§t und Spezifizit√§t f√ºr den Schwellenwert \\(q=0.5\\) hervorzuheben.\n\nthreshold_50 &lt;- lr_roc %&gt;%\n  filter(.threshold == median(.threshold)) \n\np1 +\n  annotate(\n    geom = \"point\",\n    x = 1-threshold_50$specificity,\n    y = threshold_50$sensitivity,\n    color = \"red\",\n    size = 3\n  )\n\n\n\n\n\n\n\n\nDer Schwellenwert \\(q=0.5\\) ist wahrscheinlich maximiert wahrscheinlich nicht gleichzeitig die Sensitivit√§t und Spezifizit√§t, weshalb wir diesen optimalen Wert auch berechnen und einzeichnen k√∂nnen. Der Punkt, welcher beide Werte simultan maximiert kann mithilfe des lr_roc Objektes einfach berechnet werden: Indem wir die beiden Spalten specificity und sensitivity aufsummieren und das Maximium dieser Summe berechnen, ermitteln wir den optimalen Schwellenwert.\n\nthreshold_opt &lt;- lr_roc %&gt;%\n  mutate(sssum = sensitivity+specificity) %&gt;%\n  filter(sssum == max(sssum))\n\nthreshold_opt\n\n# A tibble: 1 √ó 4\n  .threshold specificity sensitivity sssum\n       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1      0.442       0.921       0.946  1.87\n\n\nDa der optimale Schwellenwert \\(q=0.442\\) betr√§gt, wollen wir unsere Ergebnisse ensrsprechend anpassen:\n\nlr_metrics_new &lt;- log_fit %&gt;%\n  augment(data_penguin) %&gt;%\n  mutate(\n    .pred_class = if_else(.pred_1 &gt;= threshold_opt$.threshold,\"1\",\"0\"),\n    across(c(.pred_class, sex),\n           ~ factor(., levels = c(\"1\", \"0\")))) %&gt;%\n  multi_metric(truth = sex,\n               estimate = .pred_class)\n\nWir k√∂nnen dann die Ergebnisse auf Basis dieses neuen Schwellenwerts mit den Ergebnissen auf Basis des alten Schwellenwerts grafisch vergleichen:\n\n\n\n\n\n\n\n\n\nDer Grafik k√∂nnen wir entnehmen, dass durch das Anpassen des Schwellenwerts zwar die Sensitivit√§t stark gestiegen ist, allerdings die Spezifizit√§t leicht gesunken ist. Ebenso hat sich die Accuracy verbessert, aber die Pr√§zision leicht verschlechtert.\n\n\n6.6.2 Klassifikationsb√§ume\nDas Fitten von Klassifikationsb√§umen im {tidymodels} Framework funktioniert analog zum Fitten von Regressionsb√§umen. Lediglich das Argument mode muss auf \"classification\" statt \"regression\" gestetzt werden.\nWir trainieren zuerst einen Baum ohne die Hyperparameter gesondert zu spezifizieren:\n\ntree_spec_vanilla &lt;- decision_tree(mode = \"classification\")\n\ntree_fit_vanilla &lt;- tree_spec_vanilla %&gt;% fit(\n  formula = sex~.,\n  data = data_penguin\n)\n\nDas Trainierte Baummodell k√∂nnen wir dann wider mithilfe der fancyRpartplot() Funktion darstellen:\n\ntree_fit_vanilla %&gt;%\n  extract_fit_engine() %&gt;%\n  rattle::fancyRpartPlot(caption = \"\")\n\n\n\n\n\n\n\n\nDie Knoten des dargestellten Baums sehen etwas anders aus im Vergleich zu einem Regressionsbaum: Zwar stehen der Vorhersagewert (\\(0\\) bzw. \\(1\\)) und die relative Anzahl der Sample im Knoten weiterhin im Knoten, allerdings wird jetzt auch noch zus√§tzlich die Impurity angezeigt. Die Terme in der Mitte der Knoten geben an wie gro√ü der Anteil der jeweiligen Klassen in den verschiednen Knoten sind. So sind zum Beipiel im Knoten mit der ID 4 (linker unterer Blattknoten) \\(98\\%\\) der Samples im Knoten aus der Klasse \\(0\\) und nur \\(2\\%\\) aus der Klasse \\(1\\). Umso ausgeglichener diese beiden Werte sind, desto weniger rein ist der Knoten.\nUm eine noch bessere Performance zu erzielen, sollten die Hyperparameter min_n, tree_depth und cost_complexity angepasst werden. F√ºr den folgenden Baum setzen wir deshalb die Baumtiefe auf 13, die Mindestanzahl der Samples in einem Knoten ben√∂tigt f√ºr einen weiteren Split auf 3 und den Cost-Complexity Parameter auf 3.67*10^-7.\n\ntree_spec &lt;- decision_tree(mode = \"classification\",\n                           tree_depth = 13,\n                           min_n = 3,\n                           cost_complexity = 3.67e-7)\n\ntree_fit &lt;- tree_spec %&gt;% fit(\n  formula = sex~.,\n  data = data_penguin\n)\n\nF√ºr den resultierenden Baum ergeben sich dann die Metriken\n\ntree_metrics &lt;- tree_fit %&gt;%\n  augment(data_penguin) %&gt;%\n  mutate(across(c(.pred_class, sex), ~ factor(., levels = c(\"1\", \"0\")))) %&gt;%\n  multi_metric(truth = sex,\n               estimate = .pred_class)\n\ntree_metrics\n\n# A tibble: 4 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.988\n2 sensitivity binary         0.994\n3 specificity binary         0.982\n4 precision   binary         0.982\n\n\nWir k√∂nnen diese Metriken grafisch mit denen der logistischen Regression vergleichen:\n\n\n\n\n\n\n\n\n\nDer Klassifikationsbaum eignet sich also entsprechend der Metriken besser f√ºr die Vorhersage des Geschlechts eines Pinguins.\n\n\n6.6.3 Neuronale Netze\nBevor wir ein Neuronales Netz trainieren k√∂nnen m√ºssen wir die numerischen Feature wieder normalisieren und nominalen Feature in Dummy Variablen umwanderln. Hierf√ºr verwenden wir wieder die dummy_cols() Funktion. Damit wir aber nicht die Zielvariable \"sex\" in eine Dummy Variable umwandeln, spezifizieren wir in dem Argument select_columns der Funktion dass nur \"island\" und \"spezies\" umgewandelt werden sollen. Zus√§tzlich spezifizieren wir wieder in der mutate_at() Funktion, dass au√üer der Zielvariable \"sex\" alle weiteren Variablen normalisiert werden sollen.\n\nlibrary(fastDummies)\n\ndata_penguin_transformed &lt;- data_penguin %&gt;%\n  na.omit()%&gt;%\n  dummy_cols(select_columns = c(\"island\",\"species\")) %&gt;%\n  select(-c(species,island))%&gt;%\n  mutate_at(vars(-\"sex\"),scale)\n\nDie Spezifikation eines Neuronalen Netz Klassifizierers funktioniert dann analog wie im Regressionsfall. Nur das Argument mode muss auf \"classification\" gesetzt werden, damit die Fehlerfunktion entsprechend angepasst wird.\n\nnnet_spec &lt;- mlp(epochs = 100,\n                 hidden_units = c(64,16),\n                 learn_rate = 0.075,\n                 activation = c(\"relu\",\"relu\"),\n                 mode = \"classification\"\n                 ) %&gt;% \n  set_engine(engine = \"brulee\",\n             verbose = FALSE,\n             optimizer = \"SGD\",\n             stop_iter = 15\n             ) \n\nDas Fitten der Modellparameter funktioniert ebenso analog mithilfe der fit Funktion:\n\nset.seed(123)\nnnet_res &lt;- nnet_spec %&gt;%\n  fit(data = data_penguin_transformed,\n      formula = sex ~.\n  )\n\nNach dem Fitten k√∂nnen wir dann das gesch√§tzte Modell wieder anhand unseres multi_metric Objekts evaluieren:\n\nnnet_res %&gt;%\n  augment(data_penguin_transformed) %&gt;%\n  mutate(across(c(.pred_class, sex), ~ factor(., levels = c(\"1\", \"0\")))) %&gt;%\n  multi_metric(truth = sex,\n               estimate = .pred_class)\n\n# A tibble: 4 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.919\n2 sensitivity binary         0.929\n3 specificity binary         0.909\n4 precision   binary         0.912\n\n\nDie Ergebnisse des Neuronalen Netzes sind etwas schlechter als die des Klassifikationsbaumes, wobei man bei diesem Vergleich auch vorsichtig sein muss, da man hier die Performance auf den Trainingsdaten vergleich. Da der Penguins Datensatz allerdings nur 344 Sample umfasst, kann es sein, dass die Datenmenge f√ºr das erfolgreiche Trainieren eines Neuronalen Netz Klassifizierer zu klein ist.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Klassifikation</span>"
    ]
  },
  {
    "objectID": "06_Klassifikation.html#√ºbungsaufgaben",
    "href": "06_Klassifikation.html#√ºbungsaufgaben",
    "title": "6¬† Klassifikation",
    "section": "6.7 √úbungsaufgaben",
    "text": "6.7 √úbungsaufgaben\n\nAufgabe 6.1 ¬†\n\nIm Kontext der logistischen Regression ist die Wahrscheinlichkeit, dass ein Datenpunkt zur Klasse \\(1\\) geh√∂rt definiert als \\[\\begin{equation*}\n\\mathbb{P}(Y=1|X=x) = \\frac{1}{1+\\exp\\left(-(b_0+\\sum_{j=1}^J b_j x_j)\\right)}\n\\end{equation*}\\] Zeige nun , dass \\[\\begin{equation*}\n\\mathbb{P}(Y=0|X=x) = \\frac{\\exp\\left(-\\left(b_0+\\sum_{j=1}^J b_j x_j\\right)\\right)}{1+\\exp\\left(-\\left(b_0+\\sum_{j=1}^J b_j x_j\\right)\\right)}\n\\end{equation*}\\]\nIn Section 6.3.2 haben wir in Gleichung \\(\\eqref{eq:odds}\\) die Odds definiert. Zeige, dass der Quotient der Wahrscheinlichkeiten tats√§chlich \\(\\exp(b_0+\\sum_{j=1}^J b_jx_j)\\) ergibt.\n\n\n\nAufgabe 6.2 Gegeben sei folgende Confusion Matrix:\n\n\n\n\n\n\n\n\n\nBerechne f√ºr diese Confusion Matrix die Metriken\n\nAccuracy\nSensitivit√§t\nSpezifizit√§t\nPrecision\n\nRunde die Ergebnisse auf drei Nachkommastellen.\n\nF√ºr die folgenden Aufgaben wollen wir ein Datensatz untersuchen, welcher anonymisierte Patientendaten von \\(300\\) Personen enth√§lt. Der Datensatz Heart Disease Prediction Dataset kann auf der Plattform Kaggle, oder √ºber diesen Link2 heruntergeladen werden.\nDer Datensatz enth√§lt folgende Variablen:\n\n\n\n\n\n\n\n\nVariable\nBeschreibung\nAuspr√§gung\n\n\n\n\nage\nAlter des Patienten (in Jahren)\nZahl\n\n\nsex\nGeschlecht des Patienten\n1 = m√§nnlich, 0 = weiblich\n\n\ncp\nTyp der Brustschmerzen\n1‚Äì4\n\n\ntrestbps\nBlutdruck in Ruhe bei Aufnahme ins Krankenhaus (in mm Hg)\nZahl\n\n\nchol\nSerum-Cholesterin in mg/dl\nZahl\n\n\nfbs\nN√ºchternblutzucker &gt; 120 mg/dl\n1 = ja, 0 = nein\n\n\nrestecg\nRuhe-EKG-Ergebnisse\n0‚Äì2\n\n\nthalach\nMaximale erreichte Herzfrequenz\nZahl\n\n\nexang\nBelastungsinduzierte Angina\n1 = ja, 0 = nein\n\n\noldpeak\nST-Streckensenkung durch Belastung im Vergleich zur Ruhe\nZahl\n\n\n\nZus√§tzlich enth√§lt der Datensatz noch die Variable target, welche beschreibt, ob eine Person an einer Herzkrankheit leidet. Die Variable target hat die Auspr√§gungen 1 und 0, wobei 1 f√ºr ein positives Testergebnis bzw. Krankheitsbild steht und 0 f√ºr ein negatives.3\n\nAufgabe 6.3 ¬†\n\nLese den Datensatz ein und wandle die Zielvarioble target in eine Faktorvariable um.\nFinde heraus, wie viele Patienten mit bzw. ohne Herzkrankheit im Datensatz vorhanden sind. Ist der Datensatz balanciert?\nSind in dem gegebenen Datensatz mehr M√§nner oder Frauen von der Herzkrankheit betroffen? Betrachte nicht nur die absolute Anzahl, sondern auch die relative Anzahl in den beiden Geschlechtsgruppen.\nErstelle jeweils einen Boxplot f√ºr das Alter jener Personen welche erkrankt, bzw. nicht erkankt sind. L√§sst sich ein Zusammenhang zwischen dem Alter der Personen und der Krankheit anhand der Boxplots erkennen?\n\n\n\nAufgabe 6.4 In dieser √úbung wollen wir ein logistisches Regressionsmodell sch√§tzen, die gesch√§tzten Parameter untersuchen und verschiedene Metriken auf Test Daten berechnen.\n\nTeile den Datensatz in Trainings- und Testdaten ein. Verwende das Seed 123 und die Defaultparameter der initial_split Funktion bez√ºglich des Anteils der Trainings- und Testaden.\n\nSch√§tze ein logistisches Regressionmodell auf Basis der Trainingsdaten.\nErstelle eine Summary des gesch√§tzeten Modells und beurteile, welche Variablen statistisch signifikant sind zum Mindestniveau \\(\\alpha = 0.1\\).\nAngenommen die Variable cp erh√∂ht sich um eine Einheit, w√§hrend alle anderen Variablen gleich bleiben. Wie wirkt sich das auf die Odds aus?\n\nNun wollen wir das Modell auf den Testdaten evaluieren.\n\nErstelle eine Confusion Matrix wie in Abschnitt Section 6.2.1 auf Basis der Testdaten.\nVerwende das multi_metric Objekt aus Section 6.6.1 um eine √ºbersicht der verschiedenen Metriken zu erstellen.\nHat das Modell gr√∂√üere Schwierigkeiten Personen als krank zu identifizieren, welche tats√§chlich krank sind, oder Personen als nicht-erkrankt zu identifizieren, welche tats√§chlich nicht krank sind?\n\n\n\n\nAufgabe 6.5 ¬†\n\n\nTrainiere nun einen Klassifikationsbaum mit folgenden Parametern auf den Trainingsdaten.\n\ncost_complexity = 6.43e-4,\ntree_depth = 14,\nmin_n = 12\n\nBerechne nun die gleichen Metriken wie in Teilaufgabe 3. ii. aus Aufgabe¬†6.4 auf Basis der Testdaten.\nWelches der beiden Modelle (logistische Regression vs.¬†Entscheidungsbaum) eignet sich in diesem Kontext besser um eine Erkrankung vorherzusagen?\n\nDa beim Erstellen der Vorhersagen der Schwellenwert \\(q = 0.5\\) verwendet wurde, sollen wir in dieser Teilaufgabe die ROC Kurve untersuchen.\n\nErstelle eine ROC-Kurve auf Basis der Testdaten wie in Section 6.6.1.\nWarum sind in der dargestellten ROC Kurve nur sehr wenige Spezifizit√§ts und Sensitivit√§ts Kombinationen dargestellt?\nBestimme anhand der folgenden Tabelle den optimalen Schwellenwert \\(q\\) (welcher simultan die Spezifizit√§t und Sensitivit√§t maximiert) und stelle diesen in der ROC-Kurve der vorherigen Teilaufgabe dar.\n\n\n\n\n\n\n\n\n.threshold\nspecificity\nsensitivity\n\n\n\n\n‚àíInf\n0.000\n1.000\n\n\n0.082\n0.000\n1.000\n\n\n0.143\n0.429\n0.951\n\n\n0.235\n0.457\n0.951\n\n\n0.417\n0.543\n0.927\n\n\n0.686\n0.571\n0.854\n\n\n0.892\n0.657\n0.610\n\n\n0.917\n0.943\n0.024\n\n\nInf\n1.000\n0.000\n\n\n\n\n\n\n\nIst der optimale Schwellenwert gr√∂√üer oder kleiner als \\(0.5\\)?",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Klassifikation</span>"
    ]
  },
  {
    "objectID": "06_Klassifikation.html#l√∂sungen",
    "href": "06_Klassifikation.html#l√∂sungen",
    "title": "6¬† Klassifikation",
    "section": "6.8 L√∂sungen",
    "text": "6.8 L√∂sungen\n\nSolution 6.1 (Aufgabe¬†6.1). \n\nGegeben ist die Wahrscheinlichkeit f√ºr die Klasse \\(1\\) in einem logistischen Regressionsmodell: \\[\\begin{equation*}\n\\mathbb{P}(Y=1|X=x) = \\frac{1}{1+\\exp\\left(-(b_0+\\sum_{j=1}^J b_j x_j)\\right)}\n\\end{equation*}\\] Da es sich um ein bin√§res Klassifikationsproblem handelt, gilt: \\[\\begin{equation*}\n\\mathbb{P}(Y=0|X=x) = 1 - \\mathbb{P}(Y=1|X=x)\n\\end{equation*}\\] Setze die gegebene Formel ein: \\[\\begin{align*}\n\\mathbb{P}(Y=0|X=x)\n&= 1 - \\frac{1}{1 + \\exp\\left(-(b_0 + \\sum_{j=1}^J b_j x_j)\\right)} \\\\\n&= \\frac{\\left(1 + \\exp\\left(-(b_0 + \\sum_{j=1}^J b_j x_j)\\right)\\right) - 1}{1 + \\exp\\left(-(b_0 + \\sum_{j=1}^J b_j x_j)\\right)} \\\\\n&= \\frac{\\exp\\left(-(b_0 + \\sum_{j=1}^J b_j x_j)\\right)}{1 + \\exp\\left(-(b_0 + \\sum_{j=1}^J b_j x_j)\\right)}\n\\end{align*}\\]\nEs gilt: \\[\\begin{align*}\n\\frac{\\mathbb{P}(Y=1|X=x)}{\\mathbb{P}(Y=0|X=x)}\n&= \\frac{\n   \\dfrac{1}{1 + \\exp\\left(-(b_0 + \\sum_{j=1}^J b_j x_j)\\right)}\n}{\n   \\dfrac{\\exp\\left(-\\left(b_0 + \\sum_{j=1}^J b_j x_j\\right)\\right)}{1 + \\exp\\left(-\\left(b_0 + \\sum_{j=1}^J b_j x_j\\right)\\right)}\n} \\\\\n&= \\frac{1 + \\exp\\left(-\\left(b_0 + \\sum_{j=1}^J b_j x_j\\right)\\right)}{\\exp\\left(-\\left(b_0 + \\sum_{j=1}^J b_j x_j\\right)\\right)} \\cdot \\frac{1}{1 + \\exp\\left(-(b_0 + \\sum_{j=1}^J b_j x_j)\\right)} \\\\\n&= \\frac{1}{\\exp\\left(-(b_0 + \\sum_{j=1}^J b_j x_j)\\right)} \\\\\n&= \\exp\\left(b_0 + \\sum_{j=1}^J b_j x_j\\right)\n\\end{align*}\\]\n\n\n\nSolution 6.2 (Aufgabe¬†6.2). \n\n\n\n\n\n\n\n\nMetrik\nWert\n\n\n\n\naccuracy\n0.838\n\n\nsensitivity\n0.880\n\n\nspecificity\n0.793\n\n\nprecision\n0.819\n\n\n\n\n\n\n\n\n\nSolution 6.3 (Aufgabe¬†6.3). \n\n\n\ndata_heart &lt;- read.csv(file = \"data/heart-disease.csv\") %&gt;%\n  mutate(target = factor(target))\n\n\n\ndata_heart %&gt;%\n  group_by(target) %&gt;%\n  summarise(n=n())\n\n# A tibble: 2 √ó 2\n  target     n\n  &lt;fct&gt;  &lt;int&gt;\n1 0        138\n2 1        165\n\n\nIn dem Datensatz sind mehr Personen ohne der Krankheit als Personen mit der Krankheit.\n\n\ndata_heart %&gt;%\n  group_by(sex,target) %&gt;%\n  summarise(n=n())\n\n# A tibble: 4 √ó 3\n# Groups:   sex [2]\n    sex target     n\n  &lt;int&gt; &lt;fct&gt;  &lt;int&gt;\n1     0 0         24\n2     0 1         72\n3     1 0        114\n4     1 1         93\n\n\nIm Datensatz leiden \\(93\\) M√§nner an der Herzkrankheit, w√§hrend bei den Frauen nur \\(72\\) Personen an der Krankheit leiden.\nUm die relativen Anteile zu bestimmen, m√ºssen wir die obige Syntax etwas anpassen. In der summarise() Funktion k√∂nnen wir mithilfe des .groups = \"drop_last\" Arguments die Gruppierung bez√ºglich der Variable target aufl√∂sen (da diese die letzte Gruppe ist). Der Summary Datensatz enth√§lt nach dieser Operation also nur noch die Gruppe bez√ºglich der Variable sex. Wenn wir dann auf diesen Datensatz die mutate() Funktion anwenden, dann werden die in der mutate() spezifizierten Funktionen Gruppenweise angewendet.\n\ndata_heart %&gt;%\n  group_by(sex, target) %&gt;%\n  summarise(n = n(), .groups = \"drop_last\") %&gt;%\n  mutate(freq = n / sum(n))\n\n# A tibble: 4 √ó 4\n# Groups:   sex [2]\n    sex target     n  freq\n  &lt;int&gt; &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt;\n1     0 0         24 0.25 \n2     0 1         72 0.75 \n3     1 0        114 0.551\n4     1 1         93 0.449\n\n\nObwohl also absolut gesehen die Anzahl der erkrankten Personen in der Gruppe der M√§nner gr√∂√üer ist, (\\(93\\) vs.¬†\\(72\\)) ist der relative Anteil unter den Frauen viel h√∂her (\\(75\\%\\) vs.¬†\\(44.9\\%\\)).\n\n\ndata_heart %&gt;%\n  ggplot(aes(x=target,y=age))+\n    geom_boxplot()+\n    theme_minimal()\n\n\n\n\n\n\n\n\nDer Altersmedian in der Gruppe der erkankten Person ist niedriger als in der Gruppe der Personen welche nicht an der Krankheit leiden. Auch die anderen beiden Quartile der Gruppe der erkrankten Personen sind im gegebenen Datensatz niedriger als die der Gruppe der nicht erkrankten Personen. Man k√∂nnte also vermuten, dass die Krankheit vermehrt bei j√ºngern Leuten auftritt.\n\n\n\nSolution 6.4 (Aufgabe¬†6.4). \n\n\n\nset.seed(123)\n\nsplit_heart &lt;- initial_split(data_heart)\ndata_train &lt;- training(split_heart)\ndata_test &lt;- testing(split_heart)\n\n\n\n\nlog_spec &lt;- logistic_reg()\n\nlog_fit &lt;- log_spec %&gt;%\n  fit(data = data_train,\n      formula = target ~.)\n\n\n\nlog_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  summary()\n\n\nCall:\nstats::glm(formula = target ~ ., family = stats::binomial, data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3143  -0.3945   0.1470   0.5640   2.5546  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  4.870921   2.965683   1.642 0.100501    \nage         -0.013524   0.027092  -0.499 0.617656    \nsex         -1.517896   0.551637  -2.752 0.005930 ** \ncp           0.800679   0.205740   3.892 9.95e-05 ***\ntrestbps    -0.024524   0.011616  -2.111 0.034745 *  \nchol        -0.002865   0.004507  -0.636 0.525027    \nfbs          0.092637   0.595299   0.156 0.876337    \nrestecg      0.743640   0.416802   1.784 0.074398 .  \nthalach      0.025976   0.012428   2.090 0.036603 *  \nexang       -1.178701   0.478019  -2.466 0.013671 *  \noldpeak     -0.719277   0.252574  -2.848 0.004402 ** \nslope        0.219195   0.429168   0.511 0.609530    \nca          -0.623691   0.214508  -2.908 0.003643 ** \nthal        -1.187637   0.354814  -3.347 0.000816 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 312.74  on 226  degrees of freedom\nResidual deviance: 155.52  on 213  degrees of freedom\nAIC: 183.52\n\nNumber of Fisher Scoring iterations: 6\n\n\nLediglich die Variablen age, chol, fbs und slope sind nicht zu einem Singifikanzniveau \\(\\alpha&lt;0.1\\) statistisch signifikant.\n\nEs gilt \\(\\hat{b}_{\\verb|cp|} \\approx 0.801\\) und deshalb \\(\\exp(\\hat{b}_{\\verb|cp|}) \\approx 2.43\\). Die Odds steigen also um \\(2.43\\).\n\n\n\n\nlog_fit %&gt;%\n  augment(data_test) %&gt;%\n  select(c(.pred_class,target))%&gt;%\n  mutate(across(c(.pred_class, target), ~ factor(., levels = c(\"1\", \"0\"))))%&gt;%\n  table()\n\n           target\n.pred_class  1  0\n          1 37 10\n          0  4 25\n\n\n\n\nlog_fit %&gt;%\n  augment(data_test) %&gt;%\n  mutate(across(c(.pred_class, target), ~ factor(., levels = c(\"1\", \"0\")))) %&gt;%\n  multi_metric(truth = target,\n               estimate = .pred_class) \n\n# A tibble: 4 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.816\n2 sensitivity binary         0.902\n3 specificity binary         0.714\n4 precision   binary         0.787\n\n\nDa Sensitivit√§t \\(&gt;\\) Spezifizit√§t ist das Modell besser darin die True Positives korrekt zu erkennen. In einem Medizinischen Kontext ist das w√ºnschenswert, da so weniger tats√§chlich erkrankte Personen als nicht erkrankt identifiziert werden.\n\n\n\n\nSolution 6.5 (Aufgabe¬†6.5). \n\n\n\n\ntree_spec &lt;- decision_tree(mode = \"classification\",\n                           cost_complexity = 6.43e-4,\n                           tree_depth = 14,\n                           min_n = 22\n)\n\ntree_res &lt;- tree_spec %&gt;%\n  fit(data = data_train,\n      formula = target~.)\n\n\n\ntree_res %&gt;%\n  augment(data_test) %&gt;%\n          mutate(across(c(.pred_class, target), ~ factor(., levels = c(\"1\", \"0\")))) %&gt;%\n          multi_metric(truth = target,\n                       estimate = .pred_class) \n\n# A tibble: 4 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.724\n2 sensitivity binary         0.854\n3 specificity binary         0.571\n4 precision   binary         0.7  \n\n\nDas logistische Regressionsmodell eigenet sich besser, da alle Metriken auf den Testdaten besser sind als die des Klassifikationsbaums.\n\n\n\n\ntree_roc &lt;- tree_res %&gt;%\n  augment(data_test) %&gt;%\n  mutate(across(c(.pred_class, target), ~ factor(., levels = c(\"1\", \"0\")))) %&gt;%\n  roc_curve(.pred_1,truth = target)\n\np1 &lt;- tree_roc %&gt;%\n  ggplot(aes(x=1-specificity,y=sensitivity))+\n  geom_path()+\n  theme_minimal()\n\np1\n\n\n\n\n\n\n\n\nIn der ROC-Kurve sind nur wenige Kombinationen dargestellt, da der trainierte Baum nur wenige Sch√§tzwerte f√ºr die Wahrscheinlichkeiten enth√§lt. Die Anzahl der Sch√§tzwerte ist durch die Anzahl der Blattknoten gegeben. Im Vergleich zur logistischen Regression ergeben sich also eine geringere Anzahl an Schwellenwerten die zu einer Ver√§nderung der Spezifizit√§t und Sensitivit√§t f√ºhren.\nDen optimalen Schwellenwert kann man ermitteln indem man die Summe von Spezifizit√§t und Sensitivit√§t betrachtet. Beim gr√∂√üten Summenwert kann dann der entsprechende Schwellenwert abgelesen werden. Dieser maximale Summenwert ist durch \\(1.47\\) gegeben, was dem Schwellenwert \\(q=0.417\\) entspricht.\n\np1 +\n  annotate(\n    geom = \"point\",\n    x = 1-0.543,\n    y = 0.927,\n    color = \"red\",\n    size = 3\n  )\n\n\n\n\n\n\n\n\nIst der optimale Schwellenwert gr√∂√üer oder kleiner als \\(0.5\\)?\nDer optimale Schwellenwert \\(q_{\\verb|opt|}=0.417\\) ist kleiner als \\(0.5\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Klassifikation</span>"
    ]
  },
  {
    "objectID": "06_Klassifikation.html#footnotes",
    "href": "06_Klassifikation.html#footnotes",
    "title": "6¬† Klassifikation",
    "section": "",
    "text": "Datens√§tze bei welchen weitaus mehr Negatives als Positives vertreten sind‚Ü©Ô∏é\nFunktioniert nur in der Web Version des Skripts‚Ü©Ô∏é\npositives Krankheitsbild hei√üt hier, dass eine Person erkrankt ist.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Klassifikation</span>"
    ]
  },
  {
    "objectID": "07_Clustering.html",
    "href": "07_Clustering.html",
    "title": "7¬† Clustering",
    "section": "",
    "text": "7.1 Einf√ºhrung\nEin wichtiges Teilgebiet des unsupervised learning ist die Clusteranalyse. Ziel der Clusternaalyse ist es einen gegebenen Datensatz in Untergruppen einzuteilen, so dass diese Untergruppen innerhalb m√∂glichst homogen sind und zu anderen Gruppen heterogen. Unter homogen verstehen wir, dass sich die Objekte in einer Untergruppe sehr √§hneln. Unter heterogen verstehen wir auf der anderen Seite, dass sich die Cluster gegenseitig kaum √§hneln sollen. Was wir unter √Ñhnlichkeit verstehen, wollen wir im n√§chsten Abschnitt etwas genauer untersuchen.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "07_Clustering.html#√§hnlichkeitsma√üe",
    "href": "07_Clustering.html#√§hnlichkeitsma√üe",
    "title": "7¬† Clustering",
    "section": "7.2 √Ñhnlichkeitsma√üe",
    "text": "7.2 √Ñhnlichkeitsma√üe\nIm plamerpenguins::penguins Datensatz befinden sich verschiedene Datentypen in den Variablen welche die Pinguine beschreiben. Da dieser Datensatz sowohl aus metrischen, als auch aus nominalen Features besteht, wollen wir im folgenden Absatz f√ºr beide Arten von Feature √§hnlichkeits- bzw. Distanzma√üe betrachten.\n\n7.2.1 Metrische Features\nSo gibt zum Beipsiel die Variable body_mass_g an, wie schwer ein Pinguin ist. Intuitiv scheint es also sinnvoll zu sagen, dass sich zwei Pinguine √§hnlich sind, wenn diese ungef√§hr gleichviel wiegen. Den Gewichtsunterschied k√∂nnen wir hierbei relativ leicht bez√ºglich verschiedener Distanzma√üe berechnen:\nDie \\(L^q\\) Metrik ist eine Veralgemeinerung der euklidischen Distanz welche wir f√ºr die Berechnung der quadratischen Fehler verwendet haben. Diese ist definiert als \\[\\begin{equation}\n  d_q(x,y) = \\left(\\sum_{j=1}^J |x_j-y_j|^q\\right)^{\\frac{1}{q}},\\qquad q\\geq1\n\\end{equation}\\]\n\nF√ºr \\(q=1\\) erhalten wir die Manhattan-Metrik.\nF√ºr \\(q=2\\) spricht man von der euklidischen Metrik.\n\nWelche Metrik f√ºr eine gegebene Variable sinnvoll ist h√§ngt dabei stark von den Daten selbst ab.\nNeben diesem absoluten Distanzma√ü k√∂nnen wir die √Ñhnlichkeit bei metrischen Merkmalen auch durch Korrelation messen.\nDer Person Korrelationskoeffizient f√ºr zwei Datenpunkte \\(x,y\\in\\mathbb{R}^J\\) ist definiert als\n\\[\\begin{equation*}\ns(x,y)={\\frac {\\sum _{j=1}^{J}(x_{j}-{\\bar {x}})(y_{j}-{\\bar {y}})}{{\\sqrt {\\sum _{j=1}^{J}(x_{i}-{\\bar {x}})^{2}}}{\\sqrt {\\sum _{j=1}^{n}(y_{j}-{\\bar {y}})^{2}}}}},\n\\end{equation*}\\]\nwobei wir hier annehmen, dass alle Feature metrisch sind.\n\\(s(x,y)\\) kann Werte zwischen \\(-1\\) und \\(1\\) annehmen, ist allerdings beschr√§nkt auf lineare Korrelation. Falls \\(s(x,y)=1\\), so sprechen wir von einer perfekten positiven Korrlation, w√§hrend \\(s(x,y)=1\\) f√ºr eine perfekte negative Korrelation steht. Falls \\(s(x,y)=0\\), dann sagen wir, dass die beiden Datenpunkte unkorreliert sind.\nDie Distanzen oder Korrelationen zwischen den einzelnen Datenpunkten lassen sich dann in einer Distanzmatrix \\(D\\) zusammenfassen. Komponentenweise k√∂nnen wir diese wie folgt definieren:\n\\[\\begin{equation*}\n  D = (d_{i,j})^{j=1,...,K}_{i=1,...,K},\\qquad \\text{mit}\\qquad d_{i,j} = d(x_i,x_k)\n\\end{equation*}\\]\n\\(D\\) ist also eine quadratische Matrix, wobei ein Eintrag \\(d_{i,j}\\) der Distanz zwischen den Punkten \\(x_i\\) und \\(x_j\\) entspricht.\n√Ñhnlich wie bei dem Vergleich der gesch√§tzten Koeffizienten im Zuge der linearen Regression oder dem skalieren der Daten bei Neuronalen Netzen, sollten wir auch beim Clustering die Daten normalisieren, damit die verscheidenen Skalen der metrischen Feature keine Verzerrung herbeif√ºhren.\n\n\n7.2.2 Nominale Features\nWir haben bisher angenommen, dass die Feature im Datensatz metrisch sind, da sich die √Ñhnlichkeitsma√üe in diesem Fall relativ leicht berechnen lassen. Allerdings sind in den meisten Datens√§tzen auch nominale Feature vertreten, weshalb wir auch diese ber√ºcksichtigen sollten.\nSo sind sich im Beispiel der Pinguine etwa zwei Pinguine auch gewisserma√üen √§hnlich, falls diese der gleichen Spezies angeh√∂ren. Allerdings ist nicht klar, wie wir den Abstand zwischen zwei verschiedenen Spezies messen!\n√Ñhnlich wie beim Umgang mit nominalen Features im supervised Learning k√∂nnen wir die Nominalen Feature in Dummy Variablen umwandeln. Ausgehend von dieser Dummy Transformation k√∂nnen wir dann eine Kontingenztabelle erstellen, welche die Grundlage f√ºr Distanzma√üe im Kontext nominaler Features bildet.\nBetrachten wir den Teildatensatz des Pinguin Datensatzes, welcher lediglich die nominalen bereits so transformiert Feature enth√§lt, dass alle nominalen Feature zu Dummy Variablen transformiert wurden.\nF√ºr die ersten zwei Pinguine ergibt sich dann folgende Darstellung:\n\n\n                  Penguin1 Penguin2\nspecies_Adelie           1        1\nspecies_Chinstrap        0        0\nspecies_Gentoo           0        0\nisland_Biscoe            0        0\nisland_Dream             0        0\nisland_Torgersen         1        1\nsex_female               0        1\nsex_male                 1        0\nsex_NA                   0        0\n\n\nDie daraus resultierende Kontingenztabelle ist\n\n\n        Penguin2\nPenguin1 1 0\n       1 2 1\n       0 1 5\n\n\nIn der Kontingenztabelle sehen wir, dass bei f√ºnf Variablen simultan Penguin1 und Penguin2 den Wert \\(0\\) annehmen. Lediglich bei zwei Variablen (species_Adelie und island_Torgersen) besitzen beide Pinguine den Wert 1. Wir sehen also, dass die Pinguine der gleichen Spezies angeh√∂ren und auf der gleichen Insel leben. Da die betrachteten Pinguine allerdings unterschiedliche Geschlechter haben, sind die Werte auf der Nebendiagonale \\(1\\).\nIm Vergleich zu metrischen Features erhalten wir somit vorerst nicht einen Wert, sondern eine Tabelle welche die Unterschiede beschreibt. Wir k√∂nnen diese Matrix dann unter Verwendung aller Eintr√§ge in einen einzelnen Wert umwandeln. H√§ufig wird hierf√ºr folgende allgemeine Formel verwendet:\n\\[\\begin{equation*}\ns(x_m,x_l) = \\frac{a+\\delta d}{a+\\delta d+ \\lambda (b+c)},\n\\end{equation*}\\]\nwobei \\(a,b,c,d\\) die Eintr√§ge der Tabelle\n\n\n\n\n\\(x_l = 1\\)\n\\(x_l = 0\\)\n\n\n\n\n\\(x_m = 1\\)\n\\(a\\)\n\\(b\\)\n\n\n\\(x_m = 0\\)\n\\(c\\)\n\\(d\\)\n\n\n\nsind und \\(\\lambda,\\delta\\in\\{0,1\\}\\).\n\nF√ºr \\(\\delta = 0\\) und \\(\\lambda =1\\) erhalten wir die Jaccard-Distanz.\nF√ºr \\(\\delta = 1\\) und \\(\\lambda =1\\) ergibt sich der M-Koeffizient.\nFalls man nur den Term \\(\\frac{a}{a+b+c+d}\\) betrachtet, so erh√§lt man die Russel and Rao Distance.\n\nSo ergibt sich f√ºr das obige Beispiel folgende Jaccard-Distanz:\n\\[\\begin{equation*}\n  s(\\verb|Penguin1|,\\verb|Penguin2|) = \\frac{a}{a+\\lambda (b+c)} = \\frac{2}{2+1+1} = 0.5.\n\\end{equation*}\\]\nWir k√∂nnen also auch durch diese Transformation eine gewisse √Ñhnlichkeit zwischen den Koeffizienten quantifizieren.\n\n\n7.2.3 Gemischte Features\nWir haben nun separat sowohl metrische als auch nominale Feature untersucht. Da in den meisten Datens√§tzen aber sowohl metrische, als auch nominale Feature vorhanden sind, stellt sich berechtigterweise die Frage, wie man mit Datenpunkten umgeht, welche sowohl metrische als auch nominale Feature enthalten.\nDer Gower-Coefficient kombiniert verschiedene Merkmalstypen und fasst diese in einer Distanz zusammen. Dieser ist definiert als\n\\[\\begin{equation*}\n  d_G(x_m,x_l) = \\frac{\\sum_{j=1}^J \\delta(x_{m,j},x_{l,j})d(x_{m,j},x_{l,j})}{\\sum_{j=1}^J\\delta(x_{m,j},x_{l,j})}.\n\\end{equation*}\\]\nHierbei gilt\n\\[\\begin{equation*}\n  d(x_{m,j},x_{l,j}) =\n  \\begin{cases}\n   \\mathbb{I}_{(x_{m,j}\\neq x_{l,j})},\\qquad &j \\text{ nominales Feature },\\\\\n   \\frac{|x_{m,j}- x_{l,j}|}{\\max_{m = 1,...,K}x_{m,j}-\\min_{m = 1,...,K}x_{m,j}},\\qquad &j \\text{ metrisches Feature }\n  \\end{cases}\n\\end{equation*}\\] und \\(\\delta(x_{m,j} x_{l,j}):\\mathbb{R}\\to[0,\\infty)\\) eine nichtnegative Funktion. \\(\\mathbb{I}_{(x_{m,j}\\neq x_{l,j})}\\) steht hierbei f√ºr die Indikatorfunktion. Sie nimmt den Wert \\(1\\) an, falls \\((x_{m,j}\\neq x_{l,j})\\) und sonst \\(0\\).\nDer Gowerkoeffizient skaliert au√üerdem die metrischen Feature auf das Interval \\([0,1]\\), weshalb hier eine Normalisierung nicht weiter notwendig ist.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "07_Clustering.html#clustering",
    "href": "07_Clustering.html#clustering",
    "title": "7¬† Clustering",
    "section": "7.3 Clustering",
    "text": "7.3 Clustering\nNachdem wir nun f√ºr verschiedene Auspr√§gungsarten Distanzma√üe definiert haben, k√∂nnen wir auf verschiedenste Art und Weise Datenpunkte mit niedrigen Distanzma√üen bzw. hoher Korrelation zu Untergruppen zusammenfassen. Man unterscheidet hierbei zwischen verschiedenen Verfahrenstypen, n√§mlich den\n\nHierarchischen Verfahren\nPartitionierungsbasierten Verfahren\nDichtebasierten Verfahren\n\nIm Kontext dieser Veranstaltung setzen wir uns vor allem mit den Hierarchischen und Partitionierungsbasierten Varfahren auseinander. F√ºr Dichtebasierte Verfahren, siehe zum Beispiel DBSCAN und h-DBSCAN\n\n7.3.1 Hierarchische Verfahren\nBei den Hierarchischen Verfahren unterscheidet man zwischen agglomorativen und divisiven Verfahren. Die grunds√§tzliche Idee ist, iterativ Gruppen an Punkten (Cluster) zu vergr√∂√üern oder zu verkleinern indem man eine Anforderung an die Homogenit√§t der Cluster schrittweise erh√∂ht oder verringert. Hierbei versteht man unter agglomorativen Verfahren, dass zu Beginn jeder Datenpunkt ein eigenes Cluster bildet und diese Cluster schrittweise durch das Zusammenf√ºhren vergr√∂√üert werden.\n\nBei einem divisiven Verfahren starten wir mit einem gro√üen Cluster welches alle Datenpunkte beeinhaltet und verkleinern dieses iterativ durch das Aufteilen in kleinere Cluster.\n\n7.3.1.1 Linkage Methoden\nUnabh√§ngig davon, ob es sich um ein agglomoratives oder divisives Verfahren handelt, m√ºssen wir spezifizieren, wie die Distanzen zwischen verschiedenen Clustern gemessen werden. Nur wenn klar ist wie gro√ü die Distanzen innerhalb und zwischen verschiedenen Clustern sind kann bestimmt werden wie diese aufgeteilt bzw. zusammengefasst werden sollen.\nWir unterscheiden beim Messen dieser Distanzen zwischen der Single-, Complete- und Average-Linkage Distanz:\n\nSingle-Linkage: \\[\\begin{equation*}\n   D(C_i,C_j) = \\min_{l\\in C_i, m\\in C_j} d_{l,m}\n\\end{equation*}\\]\nComplete-Linkage: \\[\\begin{equation*}\n  D(C_i,C_j) = \\max_{l\\in C_i, m\\in C_j} d_{l,m}\n\\end{equation*}\\]\nAverage-Linkage: \\[\\begin{equation*}\n  D(C_i,C_j) =\\frac{1}{n_in_j} \\sum_{l\\in C_i} \\sum_{m\\in C_j} d_{l,m}\n\\end{equation*}\\]\n\nwobei \\(C_i\\) und \\(C_j\\) verschiedene Cluster sind und \\(d_{l,m}\\) ein Distanz-, bzw. √Ñhnlichkeitsma√ü.\n\nBeispiel 7.1 Betrachte folgenden einfachen Datensatz bestehend aus zwei metrischen Variablen \\(x\\) und \\(y\\), welcher bereits in drei Cluster eingeteilt ist:\n\n\n\n\n\n\n\n\n\nDann k√∂nnen wir die Distanzen bez√ºglich der dem Single-Linkage Verfahren wie folgt grafisch darstellen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Clusterdistanzen bez√ºglich der dem Complete-Linkage Verfahren lassen sich ebenso grafisch darstellen:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.3.1.2 Agglomorative- und Divisive Verfahren\nWir k√∂nnen nun mithilfe der zuvor definierten Distanzma√üe zwischen Punkten und Clustern das agglomorative und divisive Clusterverfahren genauer betrachten.\nDen Ablauf des agglomorativen Verfahrens k√∂nnen wir in den folgenden Schritten zusammenfassen:\n\nWeise jeden Datenpunkt einem eigenen Cluster zu.\nWiederhole die folgenden Schritte, bis eine gew√ºnschte Anzahl an Cluster gebildet wurde:\n\nBerechne die Distanzen zwischen allen Paaren von Clustern gem√§√ü dem Verkn√ºpfungskriterium (Single-, Complete-, Average-Linkage).\n\nFinde das Paar \\((C_i,C_j)\\) mit der minimalen Distanz.\n\nF√ºhre \\(C_i\\) und \\(C_j\\) zu einem neuen Cluster \\(C_{ij} = C_i \\cup C_j\\) zusammen.\n\n\nDer Ablauf des divisiven Verfahrens k√∂nnen wir ebenso wie folgt zusammenfassen:\n\nBeginne mit einem einzelnen Cluster bestehend aus allen Datenpunkten.\nZerlege jedes Cluster in zwei Untercluster gem√§√ü den Spaltungsskriterien (Single-, Complete-, Average-Linkage).\nWiederhole Schritt 2 bis eine vorher festgelegte Zahl an Cluster erreicht wurde.\n\n\n\n7.3.1.3 Dendrogramme\nUnabh√§ngig von der Dimensionalit√§t des Datensatzes ist es w√ºnschenswert die Clusterbildung zu visualisieren, damit wir ein Gef√ºhl daf√ºr bekommen k√∂nnen, wie\n\nweit die verschiedenen Cluster voneinander entfernt sind.\nsich die Distanzen der Cluster durch weiteres Aufteilen bzw. Zusammenf√ºgen ver√§ndert.\n\nHierf√ºr betrachten wir die sogenannten Dendrogramme, welche anhand einer Baumstruktur die Anzahl der Cluster und deren Clusterdistanz visualisieren. Dieses wird als Bin√§rbaum dargestellt, wobei die die Blattknoten den einzelnen Datenpunkten entsprechen (jeder Datenpunkt ist ein Cluster).\nIn den h√∂hreren Ebenen des Baums liegen die zusammengef√ºhrten Cluster der vorherigen Ebenen. Ob der Baum allerdings hierbei von unten nach oben oder verkehrt konstruiert wird h√§ngt davon ab, ob es sich um ein agglomoratives oder divisives Verfahren handelt. Auf der \\(y\\)-Achse bilden wir die Distanz zwischen Cluster ab und auf Baisis dieser Entscheiden wie viele Cluster wir bilden wollen.\nEine horizontale Linie im Digaramm beschreibt also, wie gro√ü der Abstand zwischen den zwei darauffolgenden Clustern ist. Die vertikale Position der Knoten zeigt au√üerdem, in welcher Reihenfolge die Knoten zu einem Cluster aggregiert wurden. Durch das Hinzuf√ºgen einer horizontalen Geraden an einem beliebigen Punkt, k√∂nnen wir anhand der Anzahl der (potenziellen) Schnittpunkte der Geraden mit dem Dendrogramm die Anzahl der Cluster in der entsprechenden Schicht bestimmen.\nDie folgende Grafik zeigt das Dendrogramm erstellt auf Basis des Datensatzes in Example¬†7.1:\n\n\n\n\n\n\n\n\n\nWie in der Grafik zu erkennen ist, ist der Abstand zwischen den beiden Clustern, welche beim Trennen des gesamten Datensatzes entstehen die Distanz bei ca. 1.5. Durch das Hinzuf√ºgen einer horizontalen Geraden im Punkt \\(y=0.25\\) erhalten wir dann, dass zu diesem Heterogenit√§tsniveau bzw. Dissimilarity Level \\(4\\) Cluster gebildet wurden:\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.3.2 Bestimmung der optimalen Clusteranzahl\nBei hierarchischen Algorithmen werden im Dendrogram f√ºr verschiedene Clusteranazahlen die Dissimilarity Werte abgebildet, allerdings ist sowohl in diesem Kontext als auch beim \\(k\\)-Means algorithmus nicht klar, wie der Wert \\(k\\) gew√§hlt werden soll. Nat√ºrlich k√∂nnten wir jeden Punkt sein eigenes Cluster zuweisen, allerdings ist es dann nicht sinnvoll die Cluster anschlie√üend zu untersuchen. Genausowenig sollten wir die Anzahl der Cluster auf \\(1\\) setzen, da schlie√ülich das Ziel ist m√∂glichst homogene Cluster zu finden. Eine Antwort auf die Frage, wie viele Cluster man w√§hlen sollte liefert der Scree-Plot. Dieser zeigt auf der \\(x\\)-Achse die Anzahl der Cluster und auf der \\(y\\)-Achse das Dissimilarity Level. Diese Kurve ist monoton fallend in der Anzahl der Cluster, da mit jedem weiteren Cluster die Homogenit√§t innerhalb der Cluster steigt.\n\n\n\n\n\n\n\n\n\nEine Entscheidungsregel f√ºr die Anzahl der Cluster lautet dann:\n\nW√§hle jene Anzahl an Cluster, f√ºr welche der Winkel eingeschlossen zwischen den beiden benachbarten Punkten am geringsten ist.\n\nHeuristisch wird diese Regel auch oft Ellenbogenkriterium genannt, da man jenen Punkt auf der Kurve ausw√§hlt, welcher dem Knick eines Ellenbogens am √§hnlichsten ist.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "07_Clustering.html#√§hnlichkeitsma√üe-und-clustering-in-r",
    "href": "07_Clustering.html#√§hnlichkeitsma√üe-und-clustering-in-r",
    "title": "7¬† Clustering",
    "section": "7.4 √Ñhnlichkeitsma√üe und Clustering in R",
    "text": "7.4 √Ñhnlichkeitsma√üe und Clustering in R\nIn diesem Abschnitt wollen wir verschiedene √Ñhnlichkeits- bzw. Distanzma√üe, Clusteralgorithmen und Clusteranalysen in R betrachten. Hierf√ºr verwenden wir vor allem die folgenden Pakete:1\n\nlibrary(tidyverse)\nlibrary(tidyclust)\nlibrary(ggdendro)\nlibrary(cluster)\n\nWir betrachten in diesem Abschnitt wieder den palmerpenguins::penguins Datensatz.\n\n7.4.1 √Ñhnlichkeitsma√üe\nDie Funktion cluster::daisy() berechnet f√ºr einen √ºbergebenen Datensatz verschiedene Distanzmatrizen. Als Argumente √ºbergibt man hier neben dem Datensatz auch das Argument metric, welches die Metrik, bzw. Methode spezifiziert bez√ºglich welcher die √Ñhnlichkeitsma√üe berechnet werden sollen. Zul√§ssige Metriken sind hierbei \"euclidean\", \"manhattan\" und \"gower\". Lediglich die Gower Metrik erlaubt es uns mit dieser Funktion auch gemischte Datens√§tze zu untersuchen.\n\nBeispiel 7.2 Betrachte folgenden Teildatensatz des palmerpenguins::penguins Datensatzes, bestehend aus dem ersten, 50. und 250. Pinguin (ohne die Variable year):\n\ndata_pen_bottom &lt;- palmerpenguins::penguins[c(1,50,250),] %&gt;% \n  select(-year)\n\nDann k√∂nnen wir f√ºr die metrischen Feature mithilfe der daisy() Funktion sowohl die \\(L^2\\) als auch die Manhattan Metrik berechnen:\n\ndata_pen_bottom %&gt;%\n  select_if(is.numeric) %&gt;%\n  daisy(metric = \"euclidean\")\n\nDissimilarities :\n          1         2\n2  400.1456          \n3 1125.7814  725.7070\n\nMetric :  euclidean \nNumber of objects : 3\n\n\nWir k√∂nnen dieses Ergebnis auch selbst verifizieren:\n\\[\\begin{align*}\n  d(x_1,x_2) &= \\sqrt{(39.1-42.3)^2+(18.7-21.2)^2+(181-191)^2+(3750-4150)^2} = 400.1456\\\\\n  d(x_1,x_3) &= \\sqrt{(39.1-46.9)^2+(18.7-14.6)^2+(181-222)^2+(3750-4875)^2} = 1125.781\\\\\n  d(x_2,x_3) &= \\sqrt{(42.3-46.9)^2+(21.2-14.6)^2+(191-222)^2+(4150-4875)^2} = 725.7070\n\\end{align*}\\]\nDie Manhattan-Metrik zwischen den Datenpunkten k√∂nnen wir einfach berechnen, indem wir das Argument \"euclidean\" durch \"manhattan\" ersetzen.\nBeachte: Wir haben hier die Feature noch nicht normalisiert! \nDa der Datensatz aber nicht nur aus metrischen Feature besteht, wollen wir die Gower-Distanz auf die Punkte anwenden:\n\ndata_pen_bottom %&gt;%\n  daisy(metric = \"gower\")\n\nDissimilarities :\n          1         2\n2 0.3412146          \n3 0.9458874 0.8557551\n\nMetric :  mixed ;  Types = N, N, I, I, I, I, N \nNumber of objects : 3\n\n\nNeben der Distanzmatrix erhalten wir noch Informationen √ºber die verschiedenen Datentypen. N steht in diesem Kontext f√ºr Nominal und I f√ºr Interval. Auf Basis dieses Ergebnisses k√∂nnen wir feststellen, dass sich der erste und zweite Pinguin am √§hnlichsten sind und der erste und dritte Pinguin am un√§hnlichsten.\n\nUm eine Korrelationsmatrix f√ºr metrischen Feature zu erzeugen, k√∂nnen wir die corrr::correlate Funktion verwenden. Dieser Funktion m√ºssen wir das Argument method = \"pearson\" √ºbergeben, um eine Korrelationsmatrix bez√ºglich dem Pearson-Korrelationskoeffizienten zu erhalten.\n\nBeispiel 7.3 Damit wir die Korrelationsmatrix f√ºr die metrischen Feature des Pinguin Datensatzes berechnen k√∂nnen, m√ºssen wir diese zuerst wieder extrahieren und dann die correlate() Funktion darauf anwenden:\n\ndata_pen_bottom %&gt;%\n  select_if(is.numeric) %&gt;%\n  corrr::correlate(method = \"pearson\")\n\n# A tibble: 4 √ó 5\n  term              bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;                      &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 bill_length_mm            NA            -0.693             0.983       0.998\n2 bill_depth_mm             -0.693        NA                -0.814      -0.737\n3 flipper_length_mm          0.983        -0.814            NA           0.993\n4 body_mass_g                0.998        -0.737             0.993      NA    \n\n\nAnhand dieser Korrelationsmatrix kann man ablesen, dass die Variablen body_mass_g und bill_length_mm sehr stark positiv korrelieren und die Variablen flipper_length_mm sowie bill_depth_mm sehr stark negativ korrelieren.\n\n\n\n7.4.2 Clustering\nIn diesem Abschnitt betrachten wir hierarchische Verfahren.\n\n7.4.2.1 Hierarchische Verfahren\nZum Durchf√ºhren einer hierarchischen Clustermethode k√∂nnen wir die stats::hclust() Funktion verwenden. Die hclust() Funktion verwendet hierbei allerdings lediglich ein agglomoratives Verfahren. F√ºr divisive Verfahren k√∂nnen wir die dclust::dclust() Funktion verwenden.2. Das allgemeine Vorgehen hierbei ist wie folgt:\n\nBerechne f√ºr die zu untersuchenden Daten eine Distanz- oder √Ñhnlichkeitsmatrix, z.B. mithilfe der daisy() Funktion.\nVerwende diese Distanz- oder √Ñhnlichkeitsmatrix zum berechnen der Cluster.\n\nDa im Pinguin Datensatz sowohl metrische als auch nominale Feature vorhanden sind, verwenden wir auch hier wieder die Gower Distanz. In Kombination mit der hclust Funktion k√∂nnen wir dann ein hclust Objekt erstellen.\n\nhclust_pen &lt;- data_penguin %&gt;%\n  daisy(metric = \"gower\") %&gt;%\n  hclust(method = \"complete\")\n\nDie Variable hclust_pen enth√§lt also ein hclust Objekt, welches unter anderem folgende Attribute besitzt:\n\n\n\n\n\n\n\nFeld\nBeschreibung\n\n\n\n\nmerge\nEine (n-1) √ó 2 Matrix. Zeile i von merge beschreibt das Zusammenf√ºhren von Clustern im Schritt i der Clusteranalyse. Wenn ein Eintrag j negativ ist, bedeutet das, dass die Beobachtung -j in diesem Schritt zusammengef√ºhrt wurde. Ist j positiv, wurde ein bereits zuvor gebildeter Cluster (aus Schritt j) zusammengef√ºhrt. Negative Eintr√§ge stehen also f√ºr das Zusammenf√ºhren einzelner Beobachtungen, positive f√ºr das Zusammenf√ºhren bestehender Cluster.\n\n\nheight\nEine Menge von n-1 reellen Werten. Gibt die H√∂he des Clusterings an, also den Wert des Kriteriums, das f√ºr die Zusammenf√ºhrung verwendet wurde.\n\n\norder\nEin Vektor, der eine Permutation der urspr√ºnglichen Beobachtungen angibt, geeignet zur Darstellung des Dendrogramms, sodass sich die Zweige nicht kreuzen.\n\n\nlabels\nBeschriftungen f√ºr jedes der zu clusternden Objekte.\n\n\ncall\nDer Funktionsaufruf, der das Ergebnis erzeugt hat.\n\n\nmethod\nDie verwendete Clustermethode.\n\n\ndist.method\nDas verwendete Distanzma√ü zur Erstellung von d (nur vorhanden, wenn das Distanzobjekt ein \"method\"-Attribut besitzt).\n\n\n\nDas Attribut merge enth√§lt also Informationen dar√ºber wie die verschiedenen Cluster konstruiert werden. Mithilfe der head() Funktion k√∂nnen wir dann zum Beispiel die ersten 11 Schritte betrachten:\n\nhclust_pen %&gt;%\n  pluck(\"merge\") %&gt;%\n  head(11)\n\n      [,1] [,2]\n [1,]   -1   -4\n [2,] -233 -272\n [3,] -306 -316\n [4,] -199 -213\n [5,] -191 -209\n [6,]  -51  -57\n [7,] -223 -227\n [8,] -212 -222\n [9,]  -64  -66\n[10,] -220 -228\n[11,] -195    4\n\n\nIn den ersten zehn Schritten wurden immer einzelne Observationen zusammengef√ºgt. Erst in Schritt 11 wurde die 195. Observation dem Cluster aus Zeile 4 hinzugef√ºgt, welches nach dem hinzuf√ºgen also aus den Observationen \\(\\{199,213,195\\}\\) besteht.\nNach der Berechnung der Cluster k√∂nnen wir dann einen Scree-Plot erstellen um eine optimale Anzahl der Cluster zu bestimmen. Hierf√ºr verwenden wir folgendes Code-Snippets:\n\ncomplete_clust &lt;- hclust_pen %&gt;%\n  pluck(\"height\")%&gt;%\n  as_tibble() %&gt;%\n  mutate(num_clust = (nrow(data_penguin)-1):1) %&gt;%\n  rename(height=value)\n\nZuerst extrahieren wir aus dem Objekt hclust_pen das Attribut height. Dieses enth√§lt einen monoton fallenden Vektor mit Werten welche sich durch das verwendete Linkage Verfahren ergeben haben. So ist der erste Wert jene Distanz, welche sich beim Zusammenf√ºhren der ersten beiden Einzelpunkte zu einem Cluster ergeben. Da der R√ºckgabewert ein Vektor ist, wandeln wir diesen mithilfe der as_tibble() Funktion in ein Tibble um, damit wir im darauffolgenden Schritt die Anzahl der Cluster hinzuf√ºgen k√∂nnen. Mithilfe der rename() Funktion wandeln wir zum Schluss den Name der Spalte value in height abzu√§ndern.\nNachdem wir die Daten aufbereitet haben, wollen wir diese nun als Scree-Plot darstellen. Zuerst filtern wieder die zuvor erstellten Daten so, dass nur Ergebnisse f√ºr die Einteilung in \\(1\\),‚Ä¶,\\(9\\) Cluster angezeigt werden. Durch das Filtern erreichen wir, dass der Plot letztendlich √ºbersichtlicher ist. Wir k√∂nnen dann den Scree-Plot einfach als Kombination einer Punktewolke und eines Line-Charts darstellen:\n\ncomplete_clust %&gt;% \n  filter(num_clust &lt;= 9) %&gt;%\n  ggplot(aes(x=num_clust, y=height)) +\n     geom_point() +\n     geom_line()\n\n\n\n\n\n\n\n\nDamit der Plot leichter zu lesen ist, passen wir im folgenden Code-Snippet noch die Achsen an:\n\ncomplete_clust %&gt;% \n  filter(num_clust &lt;= 9) %&gt;%\n  ggplot(aes(x=num_clust, y=height)) +\n     geom_point() +\n     geom_line()+\n     scale_x_discrete(\n       labels = seq.int(1,9),\n       limits = factor(seq.int(1,9))\n     )+\n  labs(\n    x = \"Number of Clusters\",\n    y = \"Dissimilarity Level\"\n    )+\n  theme_minimal()\n\n\n\n\n\n\n\n\nAnhand der gegebenen Grafik l√§sst sich nicht ganz eindeutig sagen, welche Anzahl der Cluster optimal ist. Man k√∂nnte aber argumentieren, dass der Winkel um Punkt 3 oder 5 am kleinsten aussieht, so dass 3 oder 5 die optimale Anzahl an Cluster ist.\nAngenommen wir entscheiden uns den Datensatz in 5 Cluster aufzuteilen, dann k√∂nnen wir die cutree() Funktion verwenden, um Cluster-Label zum Datensatz hinzuzuf√ºgen:\n\nclustered_pen &lt;- data_penguin %&gt;%\n  cbind(hclust_pen %&gt;%\n          cutree(k=5) %&gt;%\n          factor()\n  ) %&gt;%\n  rename(c(\"cluster_label\" = 9))\n\nDer n√§chste Schritt ist nun die einzelnen Cluster zu analysieren. So k√∂nnte man zum Beispiel f√ºr jedes Cluster das durchschnittliche Gewicht der Pinguine berechnen und die Gr√∂√üe des Clusters angeben:\n\nclustered_pen %&gt;%\n  na.omit() %&gt;%\n  group_by(cluster_label) %&gt;%\n  summarise(\n    clust_size = n(),\n    avg_weight = mean(body_mass_g)\n    )\n\n# A tibble: 5 √ó 3\n  cluster_label clust_size avg_weight\n  &lt;fct&gt;              &lt;int&gt;      &lt;dbl&gt;\n1 1                     23      4035.\n2 2                     73      3369.\n3 3                     50      4048.\n4 4                    119      5092.\n5 5                     68      3733.\n\n\nIm dritten Cluster befinden sich also nicht nur die meisten Pinguine sondern im Schnitt auch die schwersten!\nEin Dendrogramm k√∂nnen wir mithilfe von verschiedenen Funktionen erzeugen.\nDie R interne base::plot() Funktion kann direkt auf das Objekt hclust_pen angewendet werden:\n\nhclust_pen %&gt;% plot()\n\n\n\n\n\n\n\n\nDer Unterschied zu den in Section 7.3.1.3 erzeugten Dendrogrammen liegt in der Darstellung der Blattknoten.\nDiese unterscheiden sich bei dem durch die base::plot() erzeugte Funktion vor allem durch die vertikale Position. Die vertikale Position gibt Auskunft dar√ºber, in welcher Reihenfolge die Cluster gebildet wurden. Dadurch, dass das Dendrogramm in diesem Fall aber nicht wirklich √ºbersichtlich ist, wollen wir im folgenden ein Beispiel betrachten.\n\nBeispiel 7.4 Betrachte den synthetischen, normalisierten Datensatz erzeugt durch das Code-Snippet:\n\nset.seed(123)\n\ndata_exm &lt;- tibble(\n  x = c(\n    rnorm(5,1,0.1),\n    rnorm(5,0.5,0.1)\n  ),\n  y = c(\n    rnorm(5,0.5,0.1),\n    rnorm(5,1.0,0.1)\n  )\n) \n\nWir k√∂nnen diesen als Punktewolke darstellen:\n\n\n\n\n\n\n\n\n\nDurch das Anwenden der hclust() Funktion erhalten wir dann wieder ein Objekt, welches wir direkt mithile der plot() Funktion darstellen k√∂nnen:\n\ndata_exm %&gt;%\n  daisy(metric = \"euclidean\") %&gt;%\n  hclust(method = \"single\") %&gt;%\n  plot()\n\n\n\n\n\n\n\n\nAnhand dieses Plots k√∂nnen wir nun die Reihenfolge der Clusterbildung ablesen:\nDa die Punkte 2 und 4 am tiefsten abgebildet sind, wurden diese Punkte zuerst zu einem Cluster \\(C_{2,4}\\) zusammengef√ºhrt. Darauffolgend wird der Punkt 5 dem Cluster \\(C_{2,4,5}\\) hinzugef√ºgt. Im letzten Schritt werden dann die beiden Cluster mit den Punkten \\(\\{3,1,5,2,4\\}\\) bzw. \\(\\{6,8,19,7,9\\}\\) zusammengef√ºhrt, da diese Cluster am weitesten entfernt voneinander liegen.\nWir k√∂nnen durch die merge Tabelle diese Beobachtungen auch direkt verifizieren:\n\n\ndata_exm %&gt;%\n  daisy(metric = \"euclidean\") %&gt;%\n  hclust(method = \"single\") %&gt;%\n  pluck(\"merge\")\n\n      [,1] [,2]\n [1,]   -2   -4\n [2,]   -5    1\n [3,]   -1    2\n [4,]   -7   -9\n [5,]  -10    4\n [6,]   -3    3\n [7,]   -8    5\n [8,]   -6    7\n [9,]    6    8",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "07_Clustering.html#√ºbungsaufgaben",
    "href": "07_Clustering.html#√ºbungsaufgaben",
    "title": "7¬† Clustering",
    "section": "7.5 √úbungsaufgaben",
    "text": "7.5 √úbungsaufgaben\n\nAufgabe 7.1 Gegeben sei folgender Ausschnitt aus dem penguins Datensatz:\n\n\n# A tibble: 5 √ó 6\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           42            20.2               190        4250\n2 Adelie  Dream               39.8          19.1               184        4650\n3 Adelie  Biscoe              35            17.9               192        3725\n4 Adelie  Dream               40.2          17.1               193        3400\n5 Gentoo  Biscoe              50.5          15.9               225        5400\n\n\n\nBerechne f√ºr den ersten und vierten Pinguins die Euklidische- und Manhattan-Distanz auf Basis der metrischen Features.\nBerechne f√ºr den zweiten und f√ºnften Pinguin die Gower-Distanz, falls\n\nalle Variablen gleich gewichtet werden.\ndie nominalen Feature doppelt gewichtet werden.\n\n\n\n\nAufgabe 7.2 Beim Durchf√ºhren eines Hierarchischen Clusteringalgorithmus ergibt sich folgender Ausschnitt aus dem merge Attribut bestehend aus den ersten 5 Schritten:\n\n\n     [,1] [,2]\n[1,]   -1   -4\n[2,]   -2    1\n[3,]   -3    2\n[4,]   -5    3\n[5,]   -6    4\n\n\n\nBeschreibe die ersten f√ºnf Schritte. Gehe hierbei auf die Clusterbildung ein und gebe die resultierenden Cluster an.\nUm welches der beiden Verfahren (Single-Linkage, Complete-Linkage) handelt es sich vermutlich hier?\n\n\n\nAufgabe 7.3 Es sei nun folgende Distanzmatrix gegeben:\n\n\nDissimilarities :\n         1        2        3        4\n2 1.595799                           \n3 3.895069 3.349281                  \n4 3.478931 3.632839 3.368725         \n5 2.537908 3.097825 3.325886 2.704879\n\nMetric :  euclidean \nNumber of objects : 5\n\n\nF√ºhre f√ºr diese Matrix die ersten zwei Schritte des Complete-Linkage Verfahrens durch. Gebe hierbei die kleinsten Distanzen zwischen den Clustern und die Distanzmatrizen nach jedem Schritt an. Gebe au√üerdem nach dem zweiten Schritt die finalen Cluster an.\n\nWir wollen f√ºr die R √úbungen ein letztes Mal den modeldata::hotel_rates Datensatz untersuchen.\nVerwende das folgende Code-Snippet f√ºr das Preprocessing und bearbeite die darauffolgenden Aufgaben mithilfe des data_hotel_filtered Datensatzes.\n\ndata_hotel &lt;- modeldata::hotel_rates\n\ncntry_list&lt;-c(\"and\", \"aut\", \"aze\",\n              \"che\", \"chl\", \"cze\",\n              \"esp\", \"geo\", \"hun\", \"ita\", \n              \"lux\", \"mar\", \"mwi\", \"nga\", \"prt\", \n              \"rus\", \"swe\", \"tur\", \"twn\", \n              \"usa\")\n\ndata_hotel_filtered &lt;- data_hotel %&gt;%\n  filter(country %in% cntry_list) %&gt;%\n  mutate(\n   arrival_month = factor(month(arrival_date)),\n   num_guests = adults+children,\n   is_repeated_guest = factor(is_repeated_guest),\n   near_christmas = factor(near_christmas),\n   near_new_years = factor(near_new_years)\n   ) %&gt;%\n  select(-c(adults,children,agent,arrival_date,arrival_date_num))\n\n\nAufgabe 7.4 ¬†\n\nSchreibe eine Funktion scale2, welche als Input eine Variable x erh√§lt und diese entsprechend der Formel \\[\\begin{equation*}\n  \\tilde{x} = \\frac{x-\\hat{\\mu}_x}{\\hat{\\text{sd}}_x}\n\\end{equation*}\\] transformiert. Die Variable \\(\\tilde{x}\\) soll anschlie√üend der R√ºckagebwert der Funktion scale2 sein.\nVerwende die in Teilaufgabe 1. erstellte Funktion scale2 um alle Feature des Typ &lt;dbl&gt; zu normieren. Speichere hierbei die normalisierten Daten in einem neuen Datensatz data_hotel_filtered_scaled.\n\n\n\nAufgabe 7.5 Verwende den in Aufgabe¬†7.4 erstellten Datensatz data_hotel_filtered_scaled um das Complete-Linkage Verfahren auf Basis einer Gower-Distanzmatrix anzuwenden und speichere das hclust Objekt in einer neuen Variable data_clust.\n\n\nAufgabe 7.6 Wir wollen nun die Ergebnisse des hierarchischen Clusterns untersuchen. Erstelle hierf√ºr wie einen Scree-Plot und identifiziere anhand der ‚ÄúEllenbogen‚Äù-Methode die optimale Anzahl an Clusterpunkten.\n\n\nAufgabe 7.7 Verwende die in der vorherigen Aufgabe bestimmte optimale Clusteranzahl um einen neuen Datensatz data_hotel_clustered zu erstellen, welcher zum einen den Datensatz data_hotel_filtered enth√§lt und zum anderen eine neue Spalte cluster_label, welche die entsprechenden Cluster Label f√ºr jeden Datenpunkt enth√§lt.\n\n\nAufgabe 7.8 ¬†\n\nFinde heraus, wie viele Datenpunkte in den jeweiligen Clustern vertreten sind und was der Durchschnittliche Wert der Variable avg_price_per_room in jedem Cluster ist.\nErstelle einen Plot, welcher f√ºr jedes Cluster einen Boxplot f√ºr die Variable lead_time enth√§lt. Gibt es Auff√§lligkeiten bez√Øuglich der Quantile?",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "07_Clustering.html#l√∂sungen",
    "href": "07_Clustering.html#l√∂sungen",
    "title": "7¬† Clustering",
    "section": "7.6 L√∂sungen",
    "text": "7.6 L√∂sungen\n\nSolution 7.1 (Aufgabe¬†7.1). \n\n\nDie euklidische Distanz ist gegeben durch \\[\\begin{equation*}\n   \\sqrt{(42-40.2)^2+(20.2-17.1)^2+(190-193)^2+(4250-3400)^2} = 850.0129     \n\\end{equation*}\\]\nDie Manhattan Distanz inst gegeben durch \\[\\begin{equation*}\n  |42-40.2|+\n  |20.2-17.1|+\n  |190-193|+\n  |4250-3400| = 857.9\n\\end{equation*}\\]\n\n\nFalls alle Variablen gleich gewichtet sind, dann gilt \\[\\begin{align*}\n  d_G(x_m,x_l) &=\n  \\frac{\\sum_{j=1}^J \\delta(x_{m,j},x_{l,j})d(x_{m,j},x_{l,j})}{\\sum_{j=1}^J\\delta(x_{m,j},x_{l,j})}\\\\\n  &=\\frac{\\sum_{j=1}^J c\\cdot d(x_{m,j},x_{l,j})}{\\sum_{j=1}^Jc}\\\\\n  &=\\frac{c\\cdot \\sum_{j=1}^J  d(x_{m,j},x_{l,j})}{J\\cdot c}\\\\\n  &=\\frac{\\sum_{j=1}^J  d(x_{m,j},x_{l,j})}{J}\n\\end{align*}\\] Es ergibt sich dann: \\[\\begin{align*}\n  d(x_{2,1},x_{5,1}) &= 1\\\\\n  d(x_{2,2},x_{5,2}) &= 1\\\\\n  d(x_{2,3},x_{5,3}) &= \\frac{|39.8-50.5|}{50.5-35}=0.69\\\\\n  d(x_{2,4},x_{5,4}) &= \\frac{|19.1-15.9|}{20.2-15.9}=0.74\\\\\n  d(x_{2,5},x_{5,5}) &=\\frac{|184-225|}{225-184} = 1\\\\\n  d(x_{2,6},x_{5,6}) &=\\frac{|4650-5400|}{5400-3400} = 0.375\n\\end{align*}\\] Das Aufsummiern und Teilen durch \\(6\\) ergibt dann \\[\\begin{equation*}\n    d_G(x_2,x_4) = \\frac{1+1+0.69+0.74+1+0.375}{6} = 0.8\n\\end{equation*}\\]\nFalls die nominalen Variablen doppelt gewichtet werden, dann gilt: \\[\\begin{equation}\n  d_G(x_m,x_l) =\n  \\frac{\\sum_{j=1}^J w_j \\cdot d(x_{m,j},x_{l,j})}{\\sum_{j=1}^J w_j}\n\\end{equation}\\] Dabei setzen wir: \\[\\begin{equation}\nw_1 = w_2 = 2, \\quad w_3 = w_4 = w_5 = w_6 = 1\n\\end{equation}\\] Es ergibt sich dann: \\[\\begin{align}\nd(x_{2,1},x_{5,1}) &= 1 \\notag \\\\\nd(x_{2,2},x_{5,2}) &= 1 \\notag \\\\\nd(x_{2,3},x_{5,3}) &= \\frac{|39.8 - 50.5|}{50.5 - 35} = \\frac{10.7}{15.5} \\approx 0.69 \\notag \\\\\nd(x_{2,4},x_{5,4}) &= \\frac{|19.1 - 15.9|}{20.2 - 15.9} = \\frac{3.2}{4.3} \\approx 0.744 \\notag \\\\\nd(x_{2,5},x_{5,5}) &= \\frac{|184 - 225|}{225 - 184} = \\frac{41}{41} = 1 \\notag \\\\\nd(x_{2,6},x_{5,6}) &= \\frac{|4650 - 5400|}{5400 - 3400} = \\frac{750}{2000} = 0.375 \\notag\n\\end{align}\\] Die gewichtete Summe der Distanzen im Z√§hler: \\[\\begin{equation}\n2 \\cdot 1 + 2 \\cdot 1 + 1 \\cdot 0.69 + 1 \\cdot 0.744 + 1 \\cdot 1 + 1 \\cdot 0.375 = 6.809\n\\end{equation}\\] Die Summe der Gewichte im Nenner: \\[\\begin{equation}\n2 + 2 + 1 + 1 + 1 + 1 = 8\n\\end{equation}\\] Insgesamt ergibt sich dann \\[\\begin{equation}\nd_G(x_2, x_5) = \\frac{6.809}{8} \\approx 0.851\n\\end{equation}\\]\n\n\n\n\nSolution 7.2 (Aufgabe¬†7.2). \n\n\nMerge -1 und -4 \\(\\implies\\) Cluster \\(C1 = \\{1, 4\\}\\)\n\nMerge -2 und 1 \\(\\implies\\) Cluster \\(C2 = \\{2, 1, 4\\}\\)\n\nMerge -3 und 2 \\(\\implies\\) Cluster \\(C3 = \\{3, 2, 1, 4\\}\\)\n\nMerge -5 und 3 \\(\\implies\\) Cluster \\(C4 = \\{5, 3, 2, 1, 4\\}\\)\n\nMerge -6 und 4 \\(\\implies\\) Cluster \\(C5 = \\{6, 5, 3, 2, 1, 4\\}\\)\n\nEs handelt sich hierbei vermutlich um das Single-Linkage Verfahren, da dieses dazu tendiert Ketten zu bilden.\n\n\n\nSolution 7.3 (Aufgabe¬†7.3). Zuerst m√ºssen wir das Paar mit minimalem Abstand identifizieren:\n\nDer kleinste Wert ist durch \\(1.5958\\) zwischen Objekt 1 und 2 gegeben.\nWir k√∂nnen diese also zu einem Cluster \\(C_{1,2} = \\{1,2\\}\\) zusammenfassen.\n\nDurch das Anwenden der Complete-Linkage Methode ergibt sich dann folgende Distanzmatrix:\n\n\n\n\n\n\n\n\n\n\n3\n4\n5\n\n\n\n\n\\(C_{1,2}\\)\n\\(\\max\\{3.9, 3.35\\} = 3.9\\)\n\\(\\max\\{3.48, 3.63\\} = 3.63\\)\n\\(\\max\\{2.54, 3.1\\} = 3.1\\)\n\n\n3\n\n3.37\n3.33\n\n\n4\n\n\n2.7\n\n\n\nWir k√∂nnen in dieser neuen Distanzmatrix nun wieder nach der kleinsten Distanz zwischen den Clustern suchen und diese zum n√§chsten Cluster hinzuf√ºgen:\n\nDer kleinste Wert ist durch \\(2.7049\\) zwischen Objekt 4 und 5 gegeben.\nEs ergibt sich dadurch das Cluster \\(C_{4,5} = \\{4,5\\}\\)\n\nWir erhalten daraufhin die Distanzmatrix\n\n\n\n\n3\n\\(C_{4,5}\\)\n\n\n\n\n\\(C_{1,2}\\)\n\\(3.90\\)\n\\(\\max\\{3.48, 3.63, 2.54, 3.1\\} = 3.63\\)\n\n\n3\n‚Äî\n\\(\\max\\{3.37, 3.33\\} = 3.37\\)\n\n\n\nDie finalen Cluster nach zwei Schritten sind dann durch\n\\[\\begin{align*}\nC_{1,2} &= \\{1,2\\}\\\\\nC_{3}   &= \\{3\\}\\\\\nC_{4,5} &= \\{4,5\\}\n\\end{align*}\\]\ngegeben.\n\n\nSolution 7.4 (Aufgabe¬†7.4). \n\n\n\nscale2 &lt;- function(x){\n  x_scaled &lt;- (x-mean(x)/sd(x))\n  return(x_scaled)\n}\n\n\n\ndata_hotel_filtered_scaled &lt;- data_hotel_filtered %&gt;%\n  mutate_if(is_double , scale2 )\n\n\n\n\nSolution 7.5 (Aufgabe¬†7.5). \n\ndata_clust &lt;- data_hotel_filtered_scaled %&gt;%\n  daisy(metric=\"gower\") %&gt;%\n  hclust(method=\"complete\")\n\n\n\nSolution 7.6 (Aufgabe¬†7.6). \n\ncomplete_clust &lt;- data_clust %&gt;%\n  pluck(\"height\") %&gt;%\n  as_tibble() %&gt;% \n  mutate(num_clust =(nrow(data_hotel_filtered)-1):1) %&gt;% \n  rename(height=value)\n\ncomplete_clust %&gt;%\n  filter(num_clust&lt;=9) %&gt;%\n  ggplot(aes(x=num_clust,y=height))+\n  geom_point()+\n  geom_line()+\n  scale_x_discrete(\n    labels = seq.int(1,9),\n    limits = factor(seq.int(1,9))\n    )+\n  labs(x =\"Number of Clusters\",y =\"Dissimilarity Level\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nMithilfe der Ellenbogen Methode k√∂nnen wir \\(k=6\\) als optimale Clusteranzahl identifizieren, da dieser Punkt den kleinsten Winkel im Vergleich zu den anderen Punkten einschlie√üt.\n\n\nSolution 7.7 (Aufgabe¬†7.7). Aus der vorherigen Aufgabe ergibt sich, dass die optimale Anzahl an Cluster durch \\(k=6\\) gegeben ist.\n\ndata_hotel_clustered &lt;- data_clust %&gt;%\n  cutree(k=6) %&gt;%\n  factor() %&gt;%\n  cbind(data_hotel_filtered) %&gt;%\n  rename(\"cluster_label\" = 1)\n\n\n\nSolution 7.8 (Aufgabe¬†7.8). \n\nMithilfe der group_by() und summary() Funktionen k√∂nnen wir direkt berechnen, wie viele Datenpunkte in den jeweiligen Clustern sind und wie hoch die durchschnittlichen √úbernachtungskosten in den jeweiligen Clustern sind:\n\ndata_hotel_clustered %&gt;%\n  group_by(cluster_label) %&gt;%\n  summarise(n=n(),\n            mean_price = mean(avg_price_per_room)\n  )\n\n# A tibble: 6 √ó 3\n  cluster_label     n mean_price\n  &lt;fct&gt;         &lt;int&gt;      &lt;dbl&gt;\n1 1              3785      134. \n2 2              1880      140. \n3 3               440       59.8\n4 4                53      102. \n5 5              1297       57.4\n6 6                83      169. \n\n\n\n\ndata_hotel_clustered %&gt;%\n  ggplot(aes(x=cluster_label,y=lead_time))+\n    geom_boxplot()\n\n\n\n\n\n\n\n\nDie Boxplots zeigen, dass die verschiedenen Cluster teilweise sehr verschiedene Quartile enthalten. Wenig √ºberraschend ist, dass ziemlich alle untere Quartile um die \\(0\\) ligen, da es viele G√§ste geben wird welche nur kurzfristig ein Zimmer buchen. Insbersondere im 6. Cluster scheint es allerdings eine Tendenz zu geben, die Hotezimmer bereits im Voraus zu buchen, was dieses Cluster von den anderen unterscheidet.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "07_Clustering.html#footnotes",
    "href": "07_Clustering.html#footnotes",
    "title": "7¬† Clustering",
    "section": "",
    "text": "Diese m√ºssen eventuell noch installiert werden‚Ü©Ô∏é\nDas {dclust} Package muss eventuell zuvor installiert werden‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  }
]