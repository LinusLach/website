<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Linus Lach">
<meta name="dcterms.date" content="2023-05-05">

<title>A Gentle (Mathematicians) Introduction to PyTorch and Neural Networks Part 01 – Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">A Gentle (Mathematicians) Introduction to PyTorch and Neural Networks Part 01</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Python</div>
                <div class="quarto-category">PyTorch</div>
                <div class="quarto-category">Machine Learning</div>
                <div class="quarto-category">Logistic Regression</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Linus Lach </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 5, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="a-gentle-mathematicians-introduction-to-pytorch-and-neural-networks-part-01" class="level1">
<h1>A Gentle (Mathematicians) Introduction to PyTorch and Neural Networks Part 01</h1>
<p>In this series of blog posts, I’d like to introduce some fundamentals of the <a href="https://pytorch.org/">PyTorch</a> framework and neural networks. If you haven’t lived under a rock like me for the past few years, you have probably already heard some things about those magical ✨neural networks✨ and their applications like <a href="https://openai.com/blog/chatgpt">ChatGPT</a>.</p>
<p>The structure of this post is as follows: First, we familiarize ourselves a bit with the <code>PyTorch</code> framework. To build on this newly gained knowledge, we build a logistic model with <code>PyTorch</code> and gain some understanding about the binary cross-entropy loss in a theoretical setting. In the final part we will learn how to train our logistic regression model to achieve a good fit on given data.</p>
<section id="pytorch-and-tensors" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-and-tensors">PyTorch and Tensors</h2>
<div id="cell-2" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:53:30.127135Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:53:30.144095Z&quot;}" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Some packages needed throughout the article</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn, optim</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>One of the most basic objects within the <code>PyTorch</code> framework is a tensor, which can be thought of as a scalar, vector, or matrix depending on how it is used. The basic syntax is as follows. The datatype can also be specified which will be important later on for classification tasks.</p>
<div id="08f75b2b" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:53:30.755829Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:53:30.781759Z&quot;}" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="dv">0</span>])</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor([<span class="fl">0.0</span>,<span class="fl">1.0</span>,<span class="fl">2.0</span>])</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.tensor([[<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>],[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>],[<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>]])</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'x = '</span>,x,<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>,<span class="st">"y = "</span>,y,<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>,<span class="st">"z = "</span>,z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x =  tensor([0]) 
 y =  tensor([0., 1., 2.]) 
 z =  tensor([[ 1, -1,  1],
        [ 0,  1,  0],
        [ 0,  0,  1]])</code></pre>
</div>
</div>
<p>In the code above, <code>x</code> represents a scalar, <code>y</code> a vector, and <code>z</code> a matrix. Note, that in the vector <code>y</code> we did not use the default datatype which is <code>long</code> or more specifically <code>torch.LongTensor</code> rather than <code>float</code> or <code>torch.FloatTensor</code> which is automatically assigned when using floating point notation. The data type can also be specified manually by setting the <code>dtype</code> argument to the desired type and can be checked with the <code>.type()</code> method or <code>dtype</code> attribute.</p>
<div id="927cec4b" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:53:31.299891Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:53:31.322830Z&quot;}" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"x.type() = "</span>,x.<span class="bu">type</span>(),<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>,<span class="st">"x.dtype = "</span>,x.dtype)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x.type() =  torch.LongTensor 
 x.dtype =  torch.int64</code></pre>
</div>
</div>
<p>Note that <code>torch.LongTensor</code> and <code>torch.int64</code> are synonyms and both refer to the 64-bit signed integer type. Similar to lists in the <code>numpy</code> or base <code>python</code> framework, elements of a tensor can be accessed via their respective indexes.</p>
<div id="fff74799" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:53:31.799575Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:53:31.856422Z&quot;}" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The first element of x:"</span>, x[<span class="dv">0</span>])</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The second column of the matrix z:"</span>, z[:,<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The first element of x: tensor(0)
The second column of the matrix z: tensor([-1,  1,  0])</code></pre>
</div>
</div>
<p>We can also work with tensors in functions which will be of more importance in the later sections. Furthermore, without additional settings, tensors can be plotted by using the familiar <code>pyplot</code> package. See the example below.</p>
<div id="7218ddb6" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:53:32.298285Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:53:32.657829Z&quot;}" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="fl">0.1</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(t):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>torch.exp(<span class="op">-</span><span class="dv">2</span><span class="op">*</span>t))</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>plt.plot(x,f(x))<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Post_01_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Note, that syntax in <code>PyTorch</code> is quite similar to the syntax of the <code>Numpy</code> package (e.g.&nbsp;<code>torch.arange</code> has the same functionality as <code>np.arange</code> and accessing elements in a tensor works analogously to simple Python lists). Thus, it is obvious to ask why we should use the <code>PyTorch</code> framework to begin with. The answer to that question is, that we can conveniently calculate derivatives within the <code>PyTorch</code> framework. To do so, we need to set the <code>requires_grad</code> parameter for a given tensor to <code>True</code>. Check out the following example:</p>
<div id="4a88385f" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:53:32.807430Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:53:32.844330Z&quot;}" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="fl">1.</span>], requires_grad<span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"f(x) = f(1) ="</span>, f(x))</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>f(x).backward()</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>f(x) = f(1) = tensor([0.8808], grad_fn=&lt;MulBackward0&gt;)
tensor([0.2100])</code></pre>
</div>
</div>
<p>We first define a tensor <code>x</code> with the <code>requires_grad</code> parameter set to <code>True</code> which allows us to calculate the derivative of any function that takes <code>x</code> as an input. Without getting into too much detail, note that the return value of <code>f(x)</code> is now a tensor with the additional <code>grad_fn</code> attribute, which is a pointer into a graph storing the data created by the operations on the tensor. For more details see <a href="https://pytorch.org/docs/stable/notes/autograd.html">Autograd Mechanics</a>. By plugging <code>x</code> into the previously defined function <code>f</code> and calling the method <code>backward()</code> we calculate the derivative of <code>f</code> in the point <code>x</code>. The result can then be viewed by calling the <code>x.grad</code> attribute. If we call <code>x.grad</code> without previously plugging <code>x</code> into a function the return value will be <code>None</code> as there is no gradient to be calculated.</p>
<p>Calculating the derivative over a range of values requires a small adjustment to the code. The <code>backward()</code> method stores the values of the gradient of any function in <code>x.grad</code> as we have seen in the example above. Thus, <code>x.grad</code> needs to have the same shape as <code>x</code>. However, if <code>x</code> is not a scalar, i.e., a list of values, <code>f(x).backward()</code> would try to calculate the derivative of each element in <code>f(x)</code> with respect to every element in the list <code>x</code>. This dimension mismatch can be solved by setting the gradient to <code>torch.ones_like(x)</code> which calculates the gradient of each element in <code>f(x)</code> with respect to the corresponding element in <code>x</code>.</p>
<p>To plot the results with <code>pyplot</code>, we have to detach the gradients (i.e., the pointer into the graph) from the vectors to handle tensors like <code>numpy</code> lists.</p>
<div id="330b435c" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:53:33.966885Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:53:34.301206Z&quot;}" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="fl">0.1</span>, requires_grad <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>f(x).backward(gradient<span class="op">=</span>torch.ones_like(x))</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>plt.plot(x.detach(),f(x).detach(), label <span class="op">=</span> <span class="st">"$f(x)$"</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>plt.plot(x.detach(),x.grad.detach(), label <span class="op">=</span> <span class="st">" $\dfrac{\partial}{\partial x} f(x)$"</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Post_01_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="logistic-regression-with-two-class-datasets" class="level2">
<h2 class="anchored" data-anchor-id="logistic-regression-with-two-class-datasets">Logistic regression with two class datasets</h2>
<p>Now, that we have established a very basic understanding of the <code>PyTorch</code> module, we can move on to some basic applied statistics aka ✨machine learning✨. A brief recap of logistic regression and linear models serves as a great introduction to classification problems in a mathematical way. Let us assume that we have a one-dimensional input, i.e., <span class="math inline">\(x\in\mathbb{R}\)</span>, and two classes labeled <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, i.e., <span class="math inline">\(y\in\{0,1\}\)</span>. The main idea is to tackle a binary classification problem (e.g.&nbsp;will a loan default? Does the picture show a cat?). In theory, a simple linear model is applied to the data and then plugged into the sigmoid function, i.e., let the linear model be given by <span class="math display">\[\begin{equation}
\hat y = b+w* x
\end{equation}\]</span> where <span class="math inline">\(\hat y\)</span> is the estimate of <span class="math inline">\(y\)</span> given the parameters <span class="math inline">\(b\)</span> and <span class="math inline">\(w\)</span> and the input <span class="math inline">\(x\)</span>. Then, the value of the logistic model is given by <span class="math display">\[\begin{equation}
\sigma(\hat y) = \sigma(b+w*x) = \frac{1}{1+\exp(-1*(b+w*x))}.
\end{equation}\]</span> The output of the sigmoid function <span class="math inline">\(\sigma\)</span> can be interpreted as a probability of a feature <span class="math inline">\(x\)</span> belonging either to class <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>. If for a given feature <span class="math inline">\(x\)</span> the value of <span class="math inline">\(\sigma(\hat y)\)</span> is closer to <span class="math inline">\(1\)</span> we can say that it belongs to class <span class="math inline">\(1\)</span> with probability <span class="math inline">\(\sigma(\hat y)\)</span>, whereas a small value of <span class="math inline">\(\sigma(\hat y)\)</span> can be interpreted as <span class="math inline">\(x\)</span> belonging to class <span class="math inline">\(0\)</span> with probability <span class="math inline">\(1-\sigma(\hat y)\)</span>.</p>
<div id="af26ba6a" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:53:35.200971Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:53:35.253161Z&quot;}" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a dataset class that produces our example data</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Data_2(Dataset):</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, soft_max<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.x <span class="op">=</span> torch.arange(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="fl">0.1</span>).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> soft_max:</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.y <span class="op">=</span> torch.zeros(<span class="va">self</span>.x.shape[<span class="dv">0</span>])</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.y <span class="op">=</span> torch.zeros(<span class="va">self</span>.x.shape[<span class="dv">0</span>], <span class="dv">1</span>)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y[<span class="va">self</span>.x[:, <span class="dv">0</span>] <span class="op">&gt;</span> <span class="fl">0.5</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> soft_max:</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.y <span class="op">=</span> <span class="va">self</span>.y.<span class="bu">type</span>(torch.LongTensor)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">len</span> <span class="op">=</span> <span class="va">self</span>.x.shape[<span class="dv">0</span>]</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.x[index], <span class="va">self</span>.y[index]</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.<span class="bu">len</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="co"># A function for plotting the data and model results</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_data_2(data, model <span class="op">=</span> <span class="va">None</span>, est_line<span class="op">=</span><span class="va">False</span>, soft_max<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> est_line:</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>        plt.plot(data.x,</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>                 torch.sigmoid(<span class="bu">list</span>(model.parameters())[<span class="dv">0</span>].item() <span class="op">*</span> data.x <span class="op">+</span> <span class="bu">list</span>(model.parameters())[<span class="dv">1</span>].item()),</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>                 color<span class="op">=</span><span class="st">'black'</span>,</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>                 label<span class="op">=</span><span class="st">'estimated decision boundary'</span>)</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> soft_max:</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> data[:][<span class="dv">0</span>]</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>        y_label <span class="op">=</span> [<span class="st">'yhat=0'</span>, <span class="st">'yhat=1'</span>]</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>        y_color <span class="op">=</span> [<span class="st">'r'</span>, <span class="st">'b'</span>]</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>        Y <span class="op">=</span> []</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> w, b, y_l, y_c <span class="kw">in</span> <span class="bu">zip</span>(model.state_dict()[<span class="st">'0.weight'</span>], model.state_dict()[<span class="st">'0.bias'</span>], y_label, y_color):</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>            Y.append((w <span class="op">*</span> X <span class="op">+</span> b).numpy())</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>            plt.plot(X.numpy(), (w <span class="op">*</span> X <span class="op">+</span> b).numpy(), y_c, label<span class="op">=</span>y_l)</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>    plt.plot(data.x.numpy()[(data.y <span class="op">==</span> <span class="dv">0</span>)], data.y.numpy()[(data.y <span class="op">==</span> <span class="dv">0</span>)], <span class="st">'ro'</span>, label<span class="op">=</span><span class="st">"class 0"</span>)</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>    plt.plot(data.x.numpy()[(data.y <span class="op">==</span> <span class="dv">1</span>)], data.y.numpy()[(data.y <span class="op">==</span> <span class="dv">1</span>)], <span class="st">'bo'</span>, label<span class="op">=</span><span class="st">"class 1"</span>)</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>    plt.ylim(<span class="op">-</span><span class="fl">0.5</span>, <span class="dv">3</span>)</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Consider the following dataset consisting of two classes labeled <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. Then, the goal is to apply the theoretic results from above in order to find the optimal parameters <span class="math inline">\(b\)</span> and <span class="math inline">\(w\)</span>.</p>
<div id="886cb4e2" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:53:36.526772Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:53:36.944684Z&quot;}" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>data_2 <span class="op">=</span> Data_2()</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>plot_data_2(data_2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Post_01_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The parameters of the best fitting line can be either be calculated explicitly in the case of a standard linear model, or by minimizing some error term numerically. We will focus on the latter, as our models and the underlying data will be increasingly more complicated. We can build a linear model in <code>PyTorch</code> as we would do in base Python.</p>
<div id="579d2589" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:53:37.800109Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:53:37.861942Z&quot;}" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(x,b <span class="op">=</span> <span class="dv">0</span>, w <span class="op">=</span> <span class="dv">2</span>):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> b <span class="op">+</span> w <span class="op">*</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then, by applying the sigmoid function we obtain the logistic model.</p>
<div id="d1f087b2" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:53:38.511158Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:53:38.553075Z&quot;}" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(x, b<span class="op">=</span> <span class="dv">0</span>, w <span class="op">=</span> <span class="dv">2</span>):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>torch.exp(<span class="op">-</span><span class="dv">1</span><span class="op">*</span>(b <span class="op">+</span> w <span class="op">*</span> x)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="435199a0" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:53:39.196465Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:53:39.451298Z&quot;}" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>,<span class="fl">0.1</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>plt.plot(x,forward(x))<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Post_01_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Instead of using our custom function, we can also use the <code>nn.Linear</code> submodule. The <code>nn.Linear</code> submodule applies a linear transformation to the incoming data in the same way as the <code>forward</code> function. However, we can specify the input and output dimensions directly and do not have to bother with that any further. Check out the example below.</p>
<div id="3239b9ec" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:53:40.405268Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:53:40.464114Z&quot;}" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>dim_in <span class="op">=</span> <span class="dv">2</span><span class="op">;</span> dim_out <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> nn.Linear(dim_in,dim_out)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="fl">0.0</span>,<span class="fl">1.0</span>])</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(f(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([-0.5982], grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
<p>The weight and bias of the module instance <code>f</code> can be accessed via the <code>state_dict</code> method, which stores the weights and biases of the model:</p>
<div id="c239cf2b" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:53:41.313609Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:53:41.359487Z&quot;}" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"weights: "</span>,f.state_dict()[<span class="st">'weight'</span>],<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>, <span class="st">"bias: "</span>, f.state_dict()[<span class="st">'bias'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>weights:  tensor([[-0.6328, -0.1698]]) 
 bias:  tensor([-0.4284])</code></pre>
</div>
</div>
<p>Note that the weights are assigned randomly. The reason is that randomized initial weights generally lead to a better performance of the model during the training phase. We can now define our first model:</p>
<div id="bb5f038b" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:53:42.371602Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:53:42.401522Z&quot;}" data-execution_count="15">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Logistic_regression(nn.Module):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,n_inputs):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Logistic_regression, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> nn.Linear(n_inputs,<span class="dv">1</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>,x):</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(<span class="va">self</span>.linear(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Instead of defining a simple logistic model in the form of a function as above, we define the model as a class. Defining the model as a class allows us to work with several instances/objects of the class at the same time without a need for code duplication. The class structure can be explained as follows. The <code>nn.Module</code> class is passed as a superclass, meaning that our class can inherit all the submodules, to all the models we will be building. Those pre-programmed submodules include the <code>nn.Linear</code> model which we will be using a lot. If <code>nn.Module</code> is passed as a superclass , the <code>__init__</code> and <code>forward</code> functions have to be specified manually. Considering the <code>__init__</code> method, it is directly inherited from the <code>nn.Module</code> class and a linear variable is defined. The <code>forward</code> function is the key to our logistic model as it takes a tensor <code>x</code> as an input and returns the sigmoid function applied to the output of the linear model. Let us create an instance of the logistic model defined above and see what it looks like.</p>
<div id="c10f1ffe" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:53:43.514683Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:53:43.755040Z&quot;}" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Logistic_regression(<span class="dv">1</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>,<span class="fl">0.1</span>).view(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>plot_data_2(data_2, model <span class="op">=</span> model, est_line<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Post_01_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Needless to say, the model above does a bad job of classifying the data correctly. Thus, let’s train our model, i.e., find a good fit for the parameters. Before doing that, let’s talk briefly about loss functions, in particular, the binary cross entropy loss and how it’s derived. In case you’re not interested in the mathematical nitty-gritty details, you can skip the following paragraph.</p>
</section>
<section id="derivation-of-the-cross-entropy-error-in-a-two-class-setting" class="level2">
<h2 class="anchored" data-anchor-id="derivation-of-the-cross-entropy-error-in-a-two-class-setting">Derivation of the cross entropy error in a two class setting</h2>
<p>Assume we have a two-class classification problem with class labels <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, and a labeled data-sample <span class="math inline">\((x_1,y_1),...,(x_N,y_N),, N\in\mathbb{N}\)</span> with <span class="math inline">\(x_i \in\mathbb{R}\)</span> for all <span class="math inline">\(i\in\{1,...,N\}\)</span>. Let’s say there is some <span class="math inline">\(n \in\mathbb{N}\)</span> with <span class="math inline">\(1&lt;n&lt;N\)</span> such that <span class="math inline">\(y_i = 0\)</span> for all <span class="math inline">\(i&lt;n\)</span> and <span class="math inline">\(y_i = 1\)</span> for all <span class="math inline">\(i\geq n\)</span>, i.e.&nbsp;the data is <a href="https://en.wikipedia.org/wiki/Linear_separability">linearly separable</a>. Then, we are interested in maximizing the probability <span class="math display">\[\begin{equation}
    \mathbb{P}(Y|wX+b):=\prod_{i=1}^{n-1}\mathbb{P}(y_i= 0|wx_i+b)\prod_{i=n}^{N}\mathbb{P}(y_i= 1|wx_i+b),
\end{equation}\]</span> where <span class="math inline">\(Y=(y_1,...,y_N)\)</span>, <span class="math inline">\(X = (x_1,...,x_N)\)</span>, and <span class="math inline">\(w,b \in\mathbb{R}\)</span>, as this yields the optimal estimation for the parameters <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span>.</p>
<p>Note, that <span class="math display">\[\begin{align}
   \mathbb{P}(y_i= 0|wx_i+b) &amp;= 1-\sigma(wx_i+b),\\
   \mathbb{P}(y_i= 1|wx_i+b) &amp;= \sigma(wx_i+b),
\end{align}\]</span> where <span class="math inline">\(\sigma\)</span> denotes the sigmoid function, i.e., <span class="math inline">\(\sigma:\mathbb{R}\to [0,1],\,\sigma(x) = \frac{e^x}{e^x+1}\)</span>. Then, <span class="math display">\[\begin{align}
    \mathbb{P}(Y|wX+b) = \prod_{i=1}^{n-1}1-\sigma(wx_i+b)\prod_{i=n}^{N}\sigma(wx_i+b).
\end{align}\]</span> Since <span class="math inline">\(y_i \in\{0,1\}\)</span> for all <span class="math inline">\(i\in\{1,...,N\}\)</span>, the expression above is equivalent to <span class="math display">\[\begin{equation}
    \mathbb{P}(Y|wX+b) = \prod_{i=1}^{N}(1-\sigma(wx_i+b))^{1-y_i}\sigma(wx_i+b)^{y_i}
\end{equation}\]</span> which can be maximized by applying maximum likelihood estimation.</p>
<p>Recall, that <span class="math display">\[\begin{equation}
    \underset{w,b}{\text{argmax }}\mathbb{P}(Y|wX+b) = \underset{w,b}{\text{argmax }}\log\left(\mathbb{P}(Y|wX+b)\right),
\end{equation}\]</span> since the logarithm is monotonically increasing. Thus, we can apply the logarithm to <span class="math inline">\(\mathbb{P}(Y|wX+b)\)</span> which yields <span class="math display">\[\begin{align}
    \log\left(\mathbb{P}(Y|wX+b)\right) &amp;= \sum_{n=1}^{N}\log\left((1-\sigma(wx_i+b))^{1-y_i}\cdot\sigma(wx_i+b)^{y_i}\right)\\
                                        &amp;= \sum_{n=1}^{N}(1-y_i)\log(1-\sigma(wx_i+b))\cdot y_i\log(\sigma(wx_i+b))
\end{align}\]</span> Instead of maximizing <span class="math inline">\(\log\left(\mathbb{P}(Y|wX+b)\right)\)</span> we can equivalently minimize <span class="math inline">\((-1)\log\left(\mathbb{P}(Y|wX+b)\right)\)</span>. We can also average the latter which yields <span class="math display">\[\begin{align}
     -\frac{1}{N}\log\left(\mathbb{P}(Y|wX+b)\right) &amp;= -\frac{1}{N}\sum_{n=1}^{N}(1-y_i)\log(1-\sigma(wx_i+b))\cdot y_i\log(\sigma(wx_i+b))\\
     &amp;=-\frac{1}{N}\sum_{n=1}^{N}(1-y_i)\log(1-\hat y_i)\cdot y_i\log(\hat y_i) =: \text{CEL}(\hat Y, Y),
\end{align}\]</span> where <span class="math inline">\(\hat y = \sigma(wx+b)\)</span>. <span class="math inline">\(\text{CEL}(\hat Y, Y)\)</span> is nothing but the binary cross-entropy loss. The binary cross-entropy loss can then be minimized by applying an appropriate algorithm like (stochastic) gradient descent, which will be covered in the next post. Implementing the binary cross-entropy loss in <code>PyTorch</code> works as follows. Note, that by adding the parameter <span class="math inline">\(\varepsilon\)</span> we avoid having to deal with singularities of the logarithm.</p>
<div id="55b639e3" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:53:45.508465Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:53:45.528415Z&quot;}" data-execution_count="17">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> BinaryCELoss(yhat,y):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    eps <span class="op">=</span> <span class="fl">9e-10</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span><span class="dv">1</span><span class="op">*</span>torch.mean(y<span class="op">*</span>torch.log(yhat<span class="op">+</span>eps)<span class="op">+</span>(<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span>torch.log(<span class="dv">1</span><span class="op">-</span>yhat<span class="op">+</span>eps))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="training-a-simple-model" class="level2">
<h2 class="anchored" data-anchor-id="training-a-simple-model">Training a simple model</h2>
<p>In order to train the model, we have to take care of some initial steps:</p>
<ol type="1">
<li>Define a model instance (that will be trained) of the previously built class.</li>
<li>Set a learning rate for the optimization algorithm (this will also be elaborated on in the next post).</li>
<li>Set a loss function (like the <code>BinaryCELoss</code> we defined above) and an optimizer (like <code>PyTorchs</code> stochastic gradient descent optimizer).</li>
</ol>
<div id="76c82b1c" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:54:11.903021Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:54:11.910005Z&quot;}" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Logistic_regression(<span class="dv">1</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(),lr<span class="op">=</span>lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Training the model usually works as follows:</p>
<ol type="1">
<li>We define a <code>train_model</code> method which takes the number of training steps (or formally <code>epochs</code>) as an input.</li>
<li>The <code>LOSS_Custom</code> and <code>LOSS</code> lists are used to store the loss of the model depending on the loss function.</li>
<li>For each epoch we perform the actual training:
<ol type="1">
<li>First, we calculate the estimate <span class="math inline">\(\hat y\)</span>.</li>
<li>Then, we calculate the loss of our estimate and the true class labels and add them to the <code>LOSS</code> List</li>
<li>After calculating the loss, we calculate the gradient by using the <code>backward</code>.</li>
</ol></li>
</ol>
<div id="b85be979" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:54:13.066719Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:54:13.091675Z&quot;}" data-execution_count="19">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(epochs):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    LOSS <span class="op">=</span> []</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>        yhat <span class="op">=</span> model(data_2.x)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> BinaryCELoss(yhat,data_2.y)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>        LOSS.append(loss.data)</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    plot_data_2(data_2, model, est_line <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    plt.plot(np.arange(<span class="dv">0</span>,epochs,<span class="dv">1</span>),LOSS)</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">"Loss during different epochs"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="0104b7d2" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:54:13.814027Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:54:14.394993Z&quot;}" data-execution_count="20">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>est <span class="op">=</span> train_model(<span class="dv">200</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Post_01_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Post_01_files/figure-html/cell-21-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The accuracy of the model can be calculated as follows:</p>
<ol type="1">
<li>Set the estimates <span class="math inline">\(\hat y\)</span> to <code>True</code> (corresponding to class <span class="math inline">\(1\)</span>) if <span class="math inline">\(\hat y\)</span> is bigger than <span class="math inline">\(0.5\)</span> and to <code>False</code> if <span class="math inline">\(\hat y \leq 0.5\)</span> (corresponding to class <span class="math inline">\(0\)</span>) and save them in a list called <code>label</code>.</li>
<li>Convert the true labels <span class="math inline">\(y\)</span> to a <code>ByteTensor</code> which takes the values <code>0</code> or <code>1</code> and compares them to the <code>label</code> list.</li>
<li>Taking the mean returns a value between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, where <span class="math inline">\(1\)</span> stands for a perfect accuracy.</li>
</ol>
<div id="729aea08" class="cell" data-executetime="{&quot;start_time&quot;:&quot;2023-05-15T12:54:26.640913Z&quot;,&quot;end_time&quot;:&quot;2023-05-15T12:54:26.669836Z&quot;}" data-execution_count="21">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> model(data_2.x)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>label <span class="op">=</span> yhat <span class="op">&gt;</span> <span class="fl">0.5</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy: "</span>,torch.mean((label<span class="op">==</span>data_2.y.<span class="bu">type</span>(torch.ByteTensor)).<span class="bu">type</span>(torch.<span class="bu">float</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy:  tensor(1.)</code></pre>
</div>
</div>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>In this short introduction, we reviewed logistic regression and learned how to implement it using the <code>PyTorch</code> framework. Additionally, we familiarised ourselves with the binary cross-entropy loss and learned how to train a simple model using (stochastic) gradient descent. In the next post, we’ll take a closer look at gradient descent in one dimension.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="LinusLach/blogComments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->




</body></html>